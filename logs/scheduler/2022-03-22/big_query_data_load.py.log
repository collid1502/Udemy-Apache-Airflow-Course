[2022-03-22 17:34:46,011] {scheduler_job.py:153} INFO - Started process (PID=793) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:34:46,018] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:34:46,020] {logging_mixin.py:112} INFO - [2022-03-22 17:34:46,019] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:34:46,089] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:34:46,141] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:34:46,370] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:34:46,834] {scheduler_job.py:1294} INFO - Created <DagRun bigquery_data_load @ 2022-03-22T16:00:00+00:00: scheduled__2022-03-22T16:00:00+00:00, externally triggered: False>
[2022-03-22 17:34:46,855] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 16:00:00+00:00: scheduled__2022-03-22T16:00:00+00:00, externally triggered: False>
[2022-03-22 17:34:46,935] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:34:46,954] {scheduler_job.py:1640} INFO - Creating / updating <TaskInstance: bigquery_data_load.load_data 2022-03-22 16:00:00+00:00 [scheduled]> in ORM
[2022-03-22 17:34:47,083] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.072 seconds
[2022-03-22 17:35:10,821] {scheduler_job.py:153} INFO - Started process (PID=814) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:10,826] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:10,828] {logging_mixin.py:112} INFO - [2022-03-22 17:35:10,827] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:10,861] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:10,871] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:11,103] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:11,178] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 16:00:00+00:00: scheduled__2022-03-22T16:00:00+00:00, externally triggered: False>
[2022-03-22 17:35:11,408] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:11,433] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.612 seconds
[2022-03-22 17:35:14,811] {scheduler_job.py:153} INFO - Started process (PID=816) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:14,821] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:14,823] {logging_mixin.py:112} INFO - [2022-03-22 17:35:14,822] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:14,862] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:14,875] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:15,158] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:15,250] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 16:00:00+00:00: scheduled__2022-03-22T16:00:00+00:00, externally triggered: False>
[2022-03-22 17:35:15,294] {logging_mixin.py:112} INFO - [2022-03-22 17:35:15,294] {dagrun.py:309} INFO - Marking run <DagRun bigquery_data_load @ 2022-03-22 16:00:00+00:00: scheduled__2022-03-22T16:00:00+00:00, externally triggered: False> failed
[2022-03-22 17:35:15,383] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:15,405] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.594 seconds
[2022-03-22 17:35:18,807] {scheduler_job.py:153} INFO - Started process (PID=823) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:18,816] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:18,818] {logging_mixin.py:112} INFO - [2022-03-22 17:35:18,817] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:18,867] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:18,877] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:19,244] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:19,339] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:19,370] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.563 seconds
[2022-03-22 17:35:22,904] {scheduler_job.py:153} INFO - Started process (PID=828) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:22,913] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:22,923] {logging_mixin.py:112} INFO - [2022-03-22 17:35:22,923] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:22,995] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:23,019] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:23,397] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:23,542] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:23,574] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.671 seconds
[2022-03-22 17:35:26,898] {scheduler_job.py:153} INFO - Started process (PID=836) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:26,954] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:26,973] {logging_mixin.py:112} INFO - [2022-03-22 17:35:26,973] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:27,065] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:27,107] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:28,125] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:28,371] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:28,464] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.566 seconds
[2022-03-22 17:35:30,811] {scheduler_job.py:153} INFO - Started process (PID=838) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:30,818] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:30,820] {logging_mixin.py:112} INFO - [2022-03-22 17:35:30,819] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:30,856] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:30,866] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:31,207] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:31,306] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:31,329] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.518 seconds
[2022-03-22 17:35:34,835] {scheduler_job.py:153} INFO - Started process (PID=840) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:34,842] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:34,844] {logging_mixin.py:112} INFO - [2022-03-22 17:35:34,844] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:34,886] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:34,897] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:35,301] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:35,399] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:35,422] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.587 seconds
[2022-03-22 17:35:38,838] {scheduler_job.py:153} INFO - Started process (PID=842) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:38,844] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:38,850] {logging_mixin.py:112} INFO - [2022-03-22 17:35:38,849] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:38,892] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:38,903] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:39,244] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:39,332] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:39,358] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.520 seconds
[2022-03-22 17:35:42,818] {scheduler_job.py:153} INFO - Started process (PID=844) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:42,823] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:42,825] {logging_mixin.py:112} INFO - [2022-03-22 17:35:42,825] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:42,865] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:42,874] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:43,254] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:43,346] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:43,370] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.553 seconds
[2022-03-22 17:35:46,847] {scheduler_job.py:153} INFO - Started process (PID=846) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:46,852] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:46,854] {logging_mixin.py:112} INFO - [2022-03-22 17:35:46,854] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:46,891] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:46,901] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:47,189] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:47,286] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:47,308] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.461 seconds
[2022-03-22 17:35:50,831] {scheduler_job.py:153} INFO - Started process (PID=848) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:50,855] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:50,858] {logging_mixin.py:112} INFO - [2022-03-22 17:35:50,857] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:50,889] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:50,897] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:51,269] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:51,353] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:51,379] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.549 seconds
[2022-03-22 17:35:54,887] {scheduler_job.py:153} INFO - Started process (PID=850) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:54,894] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:54,897] {logging_mixin.py:112} INFO - [2022-03-22 17:35:54,897] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:54,964] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:54,973] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:55,290] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:55,385] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:55,410] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.523 seconds
[2022-03-22 17:35:58,875] {scheduler_job.py:153} INFO - Started process (PID=852) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:58,881] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:35:58,883] {logging_mixin.py:112} INFO - [2022-03-22 17:35:58,882] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:58,911] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:35:58,919] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:35:59,144] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:35:59,231] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:35:59,254] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.379 seconds
[2022-03-22 17:36:02,854] {scheduler_job.py:153} INFO - Started process (PID=855) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:02,863] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:02,865] {logging_mixin.py:112} INFO - [2022-03-22 17:36:02,865] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:02,918] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:02,931] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:03,264] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:03,414] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:03,442] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.588 seconds
[2022-03-22 17:36:06,861] {scheduler_job.py:153} INFO - Started process (PID=857) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:06,868] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:06,870] {logging_mixin.py:112} INFO - [2022-03-22 17:36:06,870] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:06,913] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:06,924] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:07,129] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:07,213] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:07,237] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.377 seconds
[2022-03-22 17:36:10,877] {scheduler_job.py:153} INFO - Started process (PID=859) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:10,885] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:10,886] {logging_mixin.py:112} INFO - [2022-03-22 17:36:10,886] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:10,922] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:10,932] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:11,147] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:11,223] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:11,245] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.368 seconds
[2022-03-22 17:36:14,872] {scheduler_job.py:153} INFO - Started process (PID=861) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:14,883] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:14,885] {logging_mixin.py:112} INFO - [2022-03-22 17:36:14,885] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:14,926] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:14,937] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:15,184] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:15,262] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:15,283] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.411 seconds
[2022-03-22 17:36:18,872] {scheduler_job.py:153} INFO - Started process (PID=863) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:18,879] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:18,880] {logging_mixin.py:112} INFO - [2022-03-22 17:36:18,880] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:18,920] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:18,930] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:19,163] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:19,248] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:19,270] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.398 seconds
[2022-03-22 17:36:22,909] {scheduler_job.py:153} INFO - Started process (PID=865) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:22,915] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:22,916] {logging_mixin.py:112} INFO - [2022-03-22 17:36:22,916] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:22,959] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:22,969] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:23,195] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:23,277] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:23,302] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.392 seconds
[2022-03-22 17:36:26,889] {scheduler_job.py:153} INFO - Started process (PID=867) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:26,902] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:26,904] {logging_mixin.py:112} INFO - [2022-03-22 17:36:26,903] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:26,949] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:26,960] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:27,194] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:27,275] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:27,300] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.411 seconds
[2022-03-22 17:36:30,893] {scheduler_job.py:153} INFO - Started process (PID=869) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:30,908] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:30,910] {logging_mixin.py:112} INFO - [2022-03-22 17:36:30,910] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:30,961] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:30,986] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:31,557] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:31,641] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:31,662] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.770 seconds
[2022-03-22 17:36:34,938] {scheduler_job.py:153} INFO - Started process (PID=872) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:34,944] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:34,946] {logging_mixin.py:112} INFO - [2022-03-22 17:36:34,946] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:34,982] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:34,991] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:35,306] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:35,383] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:35,404] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.466 seconds
[2022-03-22 17:36:38,920] {scheduler_job.py:153} INFO - Started process (PID=874) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:38,926] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:38,928] {logging_mixin.py:112} INFO - [2022-03-22 17:36:38,928] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:38,959] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:38,968] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:39,207] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:39,286] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:39,306] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.387 seconds
[2022-03-22 17:36:42,931] {scheduler_job.py:153} INFO - Started process (PID=876) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:42,939] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:42,940] {logging_mixin.py:112} INFO - [2022-03-22 17:36:42,940] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:42,972] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:42,981] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:43,193] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:43,284] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:43,307] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.376 seconds
[2022-03-22 17:36:46,958] {scheduler_job.py:153} INFO - Started process (PID=878) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:46,967] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:46,968] {logging_mixin.py:112} INFO - [2022-03-22 17:36:46,968] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:47,015] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:47,026] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:47,344] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:47,461] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:47,492] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.534 seconds
[2022-03-22 17:36:50,939] {scheduler_job.py:153} INFO - Started process (PID=880) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:50,950] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:50,953] {logging_mixin.py:112} INFO - [2022-03-22 17:36:50,952] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:51,017] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:51,031] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:51,323] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:51,419] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:51,449] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.511 seconds
[2022-03-22 17:36:54,929] {scheduler_job.py:153} INFO - Started process (PID=882) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:54,934] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:54,936] {logging_mixin.py:112} INFO - [2022-03-22 17:36:54,936] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:54,975] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:54,985] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:55,210] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:55,290] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:55,313] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.384 seconds
[2022-03-22 17:36:58,970] {scheduler_job.py:153} INFO - Started process (PID=884) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:58,975] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:36:58,977] {logging_mixin.py:112} INFO - [2022-03-22 17:36:58,976] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:59,007] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:36:59,016] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:36:59,199] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:36:59,278] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:36:59,299] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.330 seconds
[2022-03-22 17:37:02,950] {scheduler_job.py:153} INFO - Started process (PID=886) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:02,957] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:02,958] {logging_mixin.py:112} INFO - [2022-03-22 17:37:02,958] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:03,007] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:03,018] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:03,238] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:03,320] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:03,343] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.393 seconds
[2022-03-22 17:37:06,949] {scheduler_job.py:153} INFO - Started process (PID=889) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:06,969] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:06,971] {logging_mixin.py:112} INFO - [2022-03-22 17:37:06,971] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:07,031] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:07,043] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:07,362] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:07,494] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:07,525] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.576 seconds
[2022-03-22 17:37:10,964] {scheduler_job.py:153} INFO - Started process (PID=891) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:10,970] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:10,972] {logging_mixin.py:112} INFO - [2022-03-22 17:37:10,972] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:11,007] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:11,015] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:11,224] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:11,303] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:11,323] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.359 seconds
[2022-03-22 17:37:14,960] {scheduler_job.py:153} INFO - Started process (PID=893) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:14,966] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:14,968] {logging_mixin.py:112} INFO - [2022-03-22 17:37:14,967] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:15,014] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:15,023] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:15,285] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:15,385] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:15,407] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.447 seconds
[2022-03-22 17:37:18,961] {scheduler_job.py:153} INFO - Started process (PID=895) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:18,967] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:18,969] {logging_mixin.py:112} INFO - [2022-03-22 17:37:18,969] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:19,000] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:19,008] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:19,236] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:19,317] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:19,338] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.377 seconds
[2022-03-22 17:37:22,969] {scheduler_job.py:153} INFO - Started process (PID=897) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:22,974] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:22,975] {logging_mixin.py:112} INFO - [2022-03-22 17:37:22,975] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:23,006] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:23,015] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:23,197] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:23,273] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:23,294] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.326 seconds
[2022-03-22 17:37:26,972] {scheduler_job.py:153} INFO - Started process (PID=899) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:26,979] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:26,980] {logging_mixin.py:112} INFO - [2022-03-22 17:37:26,980] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:27,018] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:27,026] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:27,305] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:27,393] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:27,416] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.445 seconds
[2022-03-22 17:37:30,976] {scheduler_job.py:153} INFO - Started process (PID=901) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:30,984] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:30,985] {logging_mixin.py:112} INFO - [2022-03-22 17:37:30,985] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:31,018] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:31,026] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:31,233] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:31,307] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:31,331] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.355 seconds
[2022-03-22 17:37:34,999] {scheduler_job.py:153} INFO - Started process (PID=903) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:35,004] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:35,005] {logging_mixin.py:112} INFO - [2022-03-22 17:37:35,005] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:35,036] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:35,053] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:35,251] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:35,324] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:35,347] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.348 seconds
[2022-03-22 17:37:39,048] {scheduler_job.py:153} INFO - Started process (PID=906) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:39,056] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:39,058] {logging_mixin.py:112} INFO - [2022-03-22 17:37:39,058] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:39,139] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:39,175] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:39,506] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:39,634] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:39,669] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.621 seconds
[2022-03-22 17:37:42,987] {scheduler_job.py:153} INFO - Started process (PID=908) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:42,993] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:42,995] {logging_mixin.py:112} INFO - [2022-03-22 17:37:42,994] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:43,032] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:43,045] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:43,276] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:43,372] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:43,401] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.414 seconds
[2022-03-22 17:37:47,011] {scheduler_job.py:153} INFO - Started process (PID=910) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:47,016] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:47,018] {logging_mixin.py:112} INFO - [2022-03-22 17:37:47,017] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:47,049] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:47,063] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:47,274] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:47,355] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:47,376] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.365 seconds
[2022-03-22 17:37:51,016] {scheduler_job.py:153} INFO - Started process (PID=912) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:51,024] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:51,026] {logging_mixin.py:112} INFO - [2022-03-22 17:37:51,026] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:51,059] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:51,071] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:51,311] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:51,391] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:51,412] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.399 seconds
[2022-03-22 17:37:55,011] {scheduler_job.py:153} INFO - Started process (PID=914) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:55,018] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:55,019] {logging_mixin.py:112} INFO - [2022-03-22 17:37:55,019] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:55,054] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:55,067] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:55,267] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:55,349] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:55,372] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.362 seconds
[2022-03-22 17:37:59,032] {scheduler_job.py:153} INFO - Started process (PID=916) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:59,037] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:37:59,038] {logging_mixin.py:112} INFO - [2022-03-22 17:37:59,038] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:59,069] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:37:59,082] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:37:59,298] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:37:59,379] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:37:59,401] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.369 seconds
[2022-03-22 17:38:03,038] {scheduler_job.py:153} INFO - Started process (PID=918) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:03,044] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:03,054] {logging_mixin.py:112} INFO - [2022-03-22 17:38:03,054] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:03,100] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:03,116] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:03,357] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:03,437] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:03,459] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.422 seconds
[2022-03-22 17:38:07,014] {scheduler_job.py:153} INFO - Started process (PID=920) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:07,020] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:07,021] {logging_mixin.py:112} INFO - [2022-03-22 17:38:07,021] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:07,053] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:07,066] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:07,276] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:07,346] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:07,376] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.362 seconds
[2022-03-22 17:38:11,096] {scheduler_job.py:153} INFO - Started process (PID=923) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:11,105] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:11,107] {logging_mixin.py:112} INFO - [2022-03-22 17:38:11,106] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:11,169] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:11,190] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:11,545] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:11,683] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:11,712] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.618 seconds
[2022-03-22 17:38:15,036] {scheduler_job.py:153} INFO - Started process (PID=925) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:15,041] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:15,042] {logging_mixin.py:112} INFO - [2022-03-22 17:38:15,042] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:15,080] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:15,094] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:15,312] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:15,388] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:15,409] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.373 seconds
[2022-03-22 17:38:19,026] {scheduler_job.py:153} INFO - Started process (PID=927) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:19,032] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:19,034] {logging_mixin.py:112} INFO - [2022-03-22 17:38:19,034] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:19,070] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:19,089] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:19,274] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:19,351] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:19,373] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.347 seconds
[2022-03-22 17:38:23,060] {scheduler_job.py:153} INFO - Started process (PID=929) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:23,066] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:23,067] {logging_mixin.py:112} INFO - [2022-03-22 17:38:23,067] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:23,098] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:23,110] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:23,346] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:23,434] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:23,456] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.396 seconds
[2022-03-22 17:38:27,046] {scheduler_job.py:153} INFO - Started process (PID=931) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:27,053] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:27,054] {logging_mixin.py:112} INFO - [2022-03-22 17:38:27,054] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:27,084] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:27,099] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:27,353] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:27,440] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:27,462] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.416 seconds
[2022-03-22 17:38:31,049] {scheduler_job.py:153} INFO - Started process (PID=933) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:31,056] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:31,058] {logging_mixin.py:112} INFO - [2022-03-22 17:38:31,057] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:31,092] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:31,106] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:31,344] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:31,440] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:31,462] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.413 seconds
[2022-03-22 17:38:35,088] {scheduler_job.py:153} INFO - Started process (PID=935) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:35,093] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:35,094] {logging_mixin.py:112} INFO - [2022-03-22 17:38:35,094] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:35,128] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:35,141] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:35,348] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:35,432] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:35,455] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.368 seconds
[2022-03-22 17:38:39,083] {scheduler_job.py:153} INFO - Started process (PID=937) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:39,090] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:39,091] {logging_mixin.py:112} INFO - [2022-03-22 17:38:39,091] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:39,145] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:39,159] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:39,377] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:39,454] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:39,474] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.392 seconds
[2022-03-22 17:38:43,066] {scheduler_job.py:153} INFO - Started process (PID=940) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:43,073] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:43,075] {logging_mixin.py:112} INFO - [2022-03-22 17:38:43,075] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:43,132] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:43,148] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:43,407] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:43,485] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:43,512] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.446 seconds
[2022-03-22 17:38:47,091] {scheduler_job.py:153} INFO - Started process (PID=942) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:47,096] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:47,098] {logging_mixin.py:112} INFO - [2022-03-22 17:38:47,098] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:47,127] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:47,141] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:47,335] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:47,413] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:47,433] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.342 seconds
[2022-03-22 17:38:51,074] {scheduler_job.py:153} INFO - Started process (PID=944) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:51,081] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:51,082] {logging_mixin.py:112} INFO - [2022-03-22 17:38:51,082] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:51,121] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:51,138] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:51,360] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:51,439] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:51,460] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.387 seconds
[2022-03-22 17:38:55,073] {scheduler_job.py:153} INFO - Started process (PID=946) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:55,079] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:55,080] {logging_mixin.py:112} INFO - [2022-03-22 17:38:55,080] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:55,110] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:55,122] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:55,313] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:55,385] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:55,407] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.335 seconds
[2022-03-22 17:38:59,106] {scheduler_job.py:153} INFO - Started process (PID=948) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:59,111] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:38:59,113] {logging_mixin.py:112} INFO - [2022-03-22 17:38:59,113] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:59,154] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:38:59,169] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:38:59,361] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:38:59,444] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:38:59,466] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.360 seconds
[2022-03-22 17:39:03,100] {scheduler_job.py:153} INFO - Started process (PID=950) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:03,109] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:03,110] {logging_mixin.py:112} INFO - [2022-03-22 17:39:03,110] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:03,140] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:03,156] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:03,390] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:03,472] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:03,493] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.393 seconds
[2022-03-22 17:39:07,095] {scheduler_job.py:153} INFO - Started process (PID=952) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:07,104] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:07,105] {logging_mixin.py:112} INFO - [2022-03-22 17:39:07,105] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:07,136] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:07,149] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:07,340] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:07,406] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:07,423] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.328 seconds
[2022-03-22 17:39:11,119] {scheduler_job.py:153} INFO - Started process (PID=954) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:11,123] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:11,125] {logging_mixin.py:112} INFO - [2022-03-22 17:39:11,125] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:11,159] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:11,171] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:11,391] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:11,467] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:11,491] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.372 seconds
[2022-03-22 17:39:15,119] {scheduler_job.py:153} INFO - Started process (PID=957) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:15,126] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:15,127] {logging_mixin.py:112} INFO - [2022-03-22 17:39:15,127] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:15,195] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:15,231] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:15,527] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:15,658] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:15,685] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.566 seconds
[2022-03-22 17:39:19,099] {scheduler_job.py:153} INFO - Started process (PID=959) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:19,104] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:19,105] {logging_mixin.py:112} INFO - [2022-03-22 17:39:19,105] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:19,138] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:19,152] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:19,484] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:19,576] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:19,602] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.503 seconds
[2022-03-22 17:39:23,123] {scheduler_job.py:153} INFO - Started process (PID=961) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:23,130] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:23,131] {logging_mixin.py:112} INFO - [2022-03-22 17:39:23,131] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:23,167] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:23,181] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:23,490] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:23,601] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:23,624] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.502 seconds
[2022-03-22 17:39:27,131] {scheduler_job.py:153} INFO - Started process (PID=963) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:27,136] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:27,138] {logging_mixin.py:112} INFO - [2022-03-22 17:39:27,138] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:27,176] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:27,190] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:27,406] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:27,482] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:27,503] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.373 seconds
[2022-03-22 17:39:31,121] {scheduler_job.py:153} INFO - Started process (PID=965) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:31,127] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:31,129] {logging_mixin.py:112} INFO - [2022-03-22 17:39:31,128] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:31,161] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:31,174] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:31,410] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:31,498] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:31,521] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.400 seconds
[2022-03-22 17:39:35,147] {scheduler_job.py:153} INFO - Started process (PID=967) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:35,152] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:35,153] {logging_mixin.py:112} INFO - [2022-03-22 17:39:35,153] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:35,183] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:35,195] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:35,393] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:35,471] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:35,492] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.345 seconds
[2022-03-22 17:39:39,142] {scheduler_job.py:153} INFO - Started process (PID=969) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:39,153] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:39,155] {logging_mixin.py:112} INFO - [2022-03-22 17:39:39,154] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:39,184] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:39,198] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:39,423] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:39,490] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:39,515] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.374 seconds
[2022-03-22 17:39:43,125] {scheduler_job.py:153} INFO - Started process (PID=971) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:43,131] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:43,133] {logging_mixin.py:112} INFO - [2022-03-22 17:39:43,132] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:43,173] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:43,188] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:43,434] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:43,512] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:43,535] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.410 seconds
[2022-03-22 17:39:47,243] {scheduler_job.py:153} INFO - Started process (PID=973) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:47,249] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:47,250] {logging_mixin.py:112} INFO - [2022-03-22 17:39:47,250] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:47,289] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:47,297] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:47,556] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:47,642] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:47,661] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.418 seconds
[2022-03-22 17:39:51,160] {scheduler_job.py:153} INFO - Started process (PID=976) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:51,165] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:51,167] {logging_mixin.py:112} INFO - [2022-03-22 17:39:51,167] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:51,204] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:51,214] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:51,431] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:51,507] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:51,538] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.378 seconds
[2022-03-22 17:39:55,161] {scheduler_job.py:153} INFO - Started process (PID=978) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:55,174] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:55,175] {logging_mixin.py:112} INFO - [2022-03-22 17:39:55,175] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:55,215] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:55,223] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:55,424] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:55,504] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:55,527] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.367 seconds
[2022-03-22 17:39:59,182] {scheduler_job.py:153} INFO - Started process (PID=980) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:59,187] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:39:59,189] {logging_mixin.py:112} INFO - [2022-03-22 17:39:59,188] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:59,223] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:39:59,233] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:39:59,423] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:39:59,504] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:39:59,525] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.343 seconds
[2022-03-22 17:40:03,181] {scheduler_job.py:153} INFO - Started process (PID=982) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:03,186] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:03,188] {logging_mixin.py:112} INFO - [2022-03-22 17:40:03,187] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:03,230] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:03,240] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:03,465] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:40:03,531] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:40:03,549] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.368 seconds
[2022-03-22 17:40:07,162] {scheduler_job.py:153} INFO - Started process (PID=984) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:07,169] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:07,171] {logging_mixin.py:112} INFO - [2022-03-22 17:40:07,171] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:07,226] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:07,244] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:07,568] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:40:07,673] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:40:07,714] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.552 seconds
[2022-03-22 17:40:11,285] {scheduler_job.py:153} INFO - Started process (PID=986) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:11,302] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:11,304] {logging_mixin.py:112} INFO - [2022-03-22 17:40:11,303] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:11,375] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:11,389] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:11,664] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:40:12,240] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:40:12,283] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.998 seconds
[2022-03-22 17:40:16,223] {scheduler_job.py:153} INFO - Started process (PID=988) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:16,230] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:16,232] {logging_mixin.py:112} INFO - [2022-03-22 17:40:16,232] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:16,279] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:16,288] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:16,820] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:40:16,967] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:40:16,991] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.768 seconds
[2022-03-22 17:40:19,281] {scheduler_job.py:153} INFO - Started process (PID=990) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:19,298] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:19,309] {logging_mixin.py:112} INFO - [2022-03-22 17:40:19,300] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:19,794] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:19,827] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:25,637] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:40:26,320] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:40:26,363] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 7.081 seconds
[2022-03-22 17:40:27,206] {scheduler_job.py:153} INFO - Started process (PID=993) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:27,240] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:27,242] {logging_mixin.py:112} INFO - [2022-03-22 17:40:27,241] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:27,293] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:27,305] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:27,628] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:40:27,724] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:40:27,750] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.544 seconds
[2022-03-22 17:40:28,494] {scheduler_job.py:153} INFO - Started process (PID=995) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:28,525] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:28,530] {logging_mixin.py:112} INFO - [2022-03-22 17:40:28,529] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:28,608] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:28,639] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:29,165] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:40:29,394] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:40:29,441] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.948 seconds
[2022-03-22 17:40:32,468] {scheduler_job.py:153} INFO - Started process (PID=997) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:32,480] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:32,481] {logging_mixin.py:112} INFO - [2022-03-22 17:40:32,481] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:32,521] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:32,530] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:32,821] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.353 seconds
[2022-03-22 17:40:36,553] {scheduler_job.py:153} INFO - Started process (PID=999) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:36,598] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:36,609] {logging_mixin.py:112} INFO - [2022-03-22 17:40:36,608] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:37,169] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:37,201] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:38,070] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.517 seconds
[2022-03-22 17:40:40,703] {scheduler_job.py:153} INFO - Started process (PID=1001) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:40,766] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:40,806] {logging_mixin.py:112} INFO - [2022-03-22 17:40:40,805] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:41,314] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:41,368] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:42,925] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 2.221 seconds
[2022-03-22 17:40:45,372] {scheduler_job.py:153} INFO - Started process (PID=1003) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:45,398] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:45,402] {logging_mixin.py:112} INFO - [2022-03-22 17:40:45,401] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:45,478] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:45,493] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:45,894] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.522 seconds
[2022-03-22 17:40:49,398] {scheduler_job.py:153} INFO - Started process (PID=1005) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:49,421] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:49,423] {logging_mixin.py:112} INFO - [2022-03-22 17:40:49,422] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:49,493] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:49,507] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:49,893] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.495 seconds
[2022-03-22 17:40:53,359] {scheduler_job.py:153} INFO - Started process (PID=1007) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:53,375] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:53,376] {logging_mixin.py:112} INFO - [2022-03-22 17:40:53,376] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:53,430] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:53,439] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:53,709] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.351 seconds
[2022-03-22 17:40:57,393] {scheduler_job.py:153} INFO - Started process (PID=1010) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:57,417] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:40:57,421] {logging_mixin.py:112} INFO - [2022-03-22 17:40:57,421] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:57,534] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:40:57,551] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:40:57,908] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.515 seconds
[2022-03-22 17:41:01,476] {scheduler_job.py:153} INFO - Started process (PID=1012) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:01,494] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:01,496] {logging_mixin.py:112} INFO - [2022-03-22 17:41:01,496] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:01,565] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:01,577] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:02,144] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.668 seconds
[2022-03-22 17:41:05,374] {scheduler_job.py:153} INFO - Started process (PID=1014) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:05,391] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:05,393] {logging_mixin.py:112} INFO - [2022-03-22 17:41:05,393] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:05,442] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:05,457] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:05,666] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.292 seconds
[2022-03-22 17:41:09,376] {scheduler_job.py:153} INFO - Started process (PID=1016) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:09,388] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:09,389] {logging_mixin.py:112} INFO - [2022-03-22 17:41:09,389] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:09,426] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:09,436] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:10,026] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.650 seconds
[2022-03-22 17:41:13,425] {scheduler_job.py:153} INFO - Started process (PID=1018) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:13,435] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:13,438] {logging_mixin.py:112} INFO - [2022-03-22 17:41:13,438] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:13,493] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:13,511] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:13,738] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.313 seconds
[2022-03-22 17:41:17,404] {scheduler_job.py:153} INFO - Started process (PID=1020) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:17,413] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:17,416] {logging_mixin.py:112} INFO - [2022-03-22 17:41:17,415] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:17,465] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:17,487] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:17,728] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.324 seconds
[2022-03-22 17:41:21,404] {scheduler_job.py:153} INFO - Started process (PID=1022) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:21,411] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:21,417] {logging_mixin.py:112} INFO - [2022-03-22 17:41:21,417] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:21,478] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:21,494] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:22,218] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.815 seconds
[2022-03-22 17:41:25,436] {scheduler_job.py:153} INFO - Started process (PID=1024) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:25,443] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:25,445] {logging_mixin.py:112} INFO - [2022-03-22 17:41:25,445] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:25,492] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:25,504] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:25,711] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.276 seconds
[2022-03-22 17:41:29,417] {scheduler_job.py:153} INFO - Started process (PID=1026) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:29,424] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:29,425] {logging_mixin.py:112} INFO - [2022-03-22 17:41:29,425] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:29,472] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:29,493] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:29,837] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.420 seconds
[2022-03-22 17:41:33,412] {scheduler_job.py:153} INFO - Started process (PID=1029) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:33,424] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:33,426] {logging_mixin.py:112} INFO - [2022-03-22 17:41:33,425] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:33,480] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:33,494] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:33,701] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.289 seconds
[2022-03-22 17:41:37,443] {scheduler_job.py:153} INFO - Started process (PID=1031) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:37,456] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:37,458] {logging_mixin.py:112} INFO - [2022-03-22 17:41:37,458] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:37,504] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:37,517] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:37,710] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.267 seconds
[2022-03-22 17:41:41,468] {scheduler_job.py:153} INFO - Started process (PID=1033) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:41,484] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:41,487] {logging_mixin.py:112} INFO - [2022-03-22 17:41:41,487] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:41,545] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:41,567] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:41,864] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.396 seconds
[2022-03-22 17:41:45,431] {scheduler_job.py:153} INFO - Started process (PID=1035) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:45,441] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:45,442] {logging_mixin.py:112} INFO - [2022-03-22 17:41:45,442] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:45,494] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:45,507] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:45,711] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.281 seconds
[2022-03-22 17:41:49,474] {scheduler_job.py:153} INFO - Started process (PID=1037) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:49,480] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:49,481] {logging_mixin.py:112} INFO - [2022-03-22 17:41:49,481] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:49,524] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:49,537] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:49,803] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.329 seconds
[2022-03-22 17:41:53,510] {scheduler_job.py:153} INFO - Started process (PID=1039) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:53,543] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:53,553] {logging_mixin.py:112} INFO - [2022-03-22 17:41:53,553] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:53,665] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:53,691] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:53,885] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.376 seconds
[2022-03-22 17:41:57,459] {scheduler_job.py:153} INFO - Started process (PID=1041) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:57,477] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:41:57,481] {logging_mixin.py:112} INFO - [2022-03-22 17:41:57,481] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:57,627] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:41:57,654] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:41:58,336] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.877 seconds
[2022-03-22 17:42:01,731] {scheduler_job.py:153} INFO - Started process (PID=1043) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:01,749] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:01,758] {logging_mixin.py:112} INFO - [2022-03-22 17:42:01,753] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:01,842] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:01,877] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:03,060] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.329 seconds
[2022-03-22 17:42:08,074] {scheduler_job.py:153} INFO - Started process (PID=1046) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:08,091] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:08,093] {logging_mixin.py:112} INFO - [2022-03-22 17:42:08,093] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:08,316] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:08,363] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:10,263] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 2.189 seconds
[2022-03-22 17:42:12,522] {scheduler_job.py:153} INFO - Started process (PID=1048) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:12,540] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:12,550] {logging_mixin.py:112} INFO - [2022-03-22 17:42:12,550] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:12,657] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:12,692] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:13,222] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.700 seconds
[2022-03-22 17:42:14,428] {scheduler_job.py:153} INFO - Started process (PID=1050) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:14,446] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:14,451] {logging_mixin.py:112} INFO - [2022-03-22 17:42:14,451] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:14,611] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:14,638] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:15,017] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.589 seconds
[2022-03-22 17:42:18,427] {scheduler_job.py:153} INFO - Started process (PID=1052) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:18,438] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:18,441] {logging_mixin.py:112} INFO - [2022-03-22 17:42:18,440] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:18,485] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:18,497] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:18,701] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.274 seconds
[2022-03-22 17:42:22,450] {scheduler_job.py:153} INFO - Started process (PID=1054) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:22,456] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:22,458] {logging_mixin.py:112} INFO - [2022-03-22 17:42:22,458] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:22,495] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:22,504] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:23,127] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.677 seconds
[2022-03-22 17:42:26,426] {scheduler_job.py:153} INFO - Started process (PID=1056) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:26,437] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:26,439] {logging_mixin.py:112} INFO - [2022-03-22 17:42:26,438] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:26,490] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:26,505] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:26,734] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.309 seconds
[2022-03-22 17:42:30,447] {scheduler_job.py:153} INFO - Started process (PID=1058) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:30,466] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:30,468] {logging_mixin.py:112} INFO - [2022-03-22 17:42:30,468] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:30,532] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:30,551] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:30,780] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.333 seconds
[2022-03-22 17:42:34,475] {scheduler_job.py:153} INFO - Started process (PID=1060) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:34,488] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:34,490] {logging_mixin.py:112} INFO - [2022-03-22 17:42:34,490] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:34,544] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:34,559] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:34,789] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.314 seconds
[2022-03-22 17:42:39,114] {scheduler_job.py:153} INFO - Started process (PID=1062) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:39,135] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:39,260] {logging_mixin.py:112} INFO - [2022-03-22 17:42:39,260] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:39,342] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:39,358] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:39,584] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.470 seconds
[2022-03-22 17:42:42,507] {scheduler_job.py:153} INFO - Started process (PID=1065) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:42,522] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:42,524] {logging_mixin.py:112} INFO - [2022-03-22 17:42:42,524] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:42,630] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:42,659] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:42,977] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.470 seconds
[2022-03-22 17:42:46,486] {scheduler_job.py:153} INFO - Started process (PID=1067) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:46,494] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:46,496] {logging_mixin.py:112} INFO - [2022-03-22 17:42:46,496] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:46,579] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:46,595] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:46,823] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.337 seconds
[2022-03-22 17:42:50,460] {scheduler_job.py:153} INFO - Started process (PID=1069) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:50,471] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:50,473] {logging_mixin.py:112} INFO - [2022-03-22 17:42:50,473] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:50,532] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:50,553] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:50,787] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.327 seconds
[2022-03-22 17:42:54,609] {scheduler_job.py:153} INFO - Started process (PID=1071) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:54,626] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:54,637] {logging_mixin.py:112} INFO - [2022-03-22 17:42:54,637] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:54,867] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:54,912] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:55,574] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.966 seconds
[2022-03-22 17:42:58,511] {scheduler_job.py:153} INFO - Started process (PID=1073) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:58,519] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:42:58,521] {logging_mixin.py:112} INFO - [2022-03-22 17:42:58,521] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:58,571] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:42:58,583] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:42:58,790] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.279 seconds
[2022-03-22 17:43:02,480] {scheduler_job.py:153} INFO - Started process (PID=1075) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:02,491] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:02,492] {logging_mixin.py:112} INFO - [2022-03-22 17:43:02,492] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:02,539] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:02,553] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:02,719] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.239 seconds
[2022-03-22 17:43:06,468] {scheduler_job.py:153} INFO - Started process (PID=1077) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:06,475] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:06,476] {logging_mixin.py:112} INFO - [2022-03-22 17:43:06,476] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:06,523] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:06,539] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:06,725] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.258 seconds
[2022-03-22 17:43:10,577] {scheduler_job.py:153} INFO - Started process (PID=1079) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:10,593] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:10,602] {logging_mixin.py:112} INFO - [2022-03-22 17:43:10,602] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:10,698] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:10,715] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:11,150] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.573 seconds
[2022-03-22 17:43:14,487] {scheduler_job.py:153} INFO - Started process (PID=1081) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:14,497] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:14,499] {logging_mixin.py:112} INFO - [2022-03-22 17:43:14,499] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:14,552] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:14,568] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:14,910] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.423 seconds
[2022-03-22 17:43:18,507] {scheduler_job.py:153} INFO - Started process (PID=1084) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:18,517] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:18,518] {logging_mixin.py:112} INFO - [2022-03-22 17:43:18,518] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:18,557] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:18,567] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:18,773] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.266 seconds
[2022-03-22 17:43:22,535] {scheduler_job.py:153} INFO - Started process (PID=1086) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:22,540] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:22,541] {logging_mixin.py:112} INFO - [2022-03-22 17:43:22,541] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:22,587] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:22,598] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:22,785] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.250 seconds
[2022-03-22 17:43:26,532] {scheduler_job.py:153} INFO - Started process (PID=1088) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:26,548] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:26,552] {logging_mixin.py:112} INFO - [2022-03-22 17:43:26,551] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:26,651] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:26,671] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:26,935] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.403 seconds
[2022-03-22 17:43:30,551] {scheduler_job.py:153} INFO - Started process (PID=1090) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:30,568] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:30,570] {logging_mixin.py:112} INFO - [2022-03-22 17:43:30,570] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:30,634] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:30,644] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:31,389] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.838 seconds
[2022-03-22 17:43:34,720] {scheduler_job.py:153} INFO - Started process (PID=1092) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:34,933] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:35,090] {logging_mixin.py:112} INFO - [2022-03-22 17:43:35,090] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:35,970] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:35,988] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:36,992] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 2.273 seconds
[2022-03-22 17:43:38,510] {scheduler_job.py:153} INFO - Started process (PID=1094) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:38,518] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:38,520] {logging_mixin.py:112} INFO - [2022-03-22 17:43:38,520] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:38,564] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:38,587] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:38,764] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.254 seconds
[2022-03-22 17:43:42,551] {scheduler_job.py:153} INFO - Started process (PID=1096) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:42,567] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:42,574] {logging_mixin.py:112} INFO - [2022-03-22 17:43:42,573] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:42,652] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:42,674] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:42,952] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.402 seconds
[2022-03-22 17:43:46,579] {scheduler_job.py:153} INFO - Started process (PID=1098) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:46,596] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:46,600] {logging_mixin.py:112} INFO - [2022-03-22 17:43:46,600] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:46,685] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:46,703] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:46,922] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.343 seconds
[2022-03-22 17:43:50,549] {scheduler_job.py:153} INFO - Started process (PID=1101) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:50,557] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:50,559] {logging_mixin.py:112} INFO - [2022-03-22 17:43:50,559] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:50,610] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:50,626] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:50,894] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.345 seconds
[2022-03-22 17:43:54,564] {scheduler_job.py:153} INFO - Started process (PID=1103) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:54,578] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:54,580] {logging_mixin.py:112} INFO - [2022-03-22 17:43:54,579] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:54,634] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:54,667] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:55,209] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.645 seconds
[2022-03-22 17:43:58,566] {scheduler_job.py:153} INFO - Started process (PID=1105) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:58,572] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:43:58,573] {logging_mixin.py:112} INFO - [2022-03-22 17:43:58,573] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:58,611] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:43:58,625] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:43:58,794] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.229 seconds
[2022-03-22 17:44:02,587] {scheduler_job.py:153} INFO - Started process (PID=1107) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:02,599] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:02,600] {logging_mixin.py:112} INFO - [2022-03-22 17:44:02,600] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:02,631] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:02,640] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:02,793] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.205 seconds
[2022-03-22 17:44:06,576] {scheduler_job.py:153} INFO - Started process (PID=1109) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:06,582] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:06,584] {logging_mixin.py:112} INFO - [2022-03-22 17:44:06,584] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:06,621] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:06,632] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:06,838] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.262 seconds
[2022-03-22 17:44:10,634] {scheduler_job.py:153} INFO - Started process (PID=1111) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:10,651] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:10,653] {logging_mixin.py:112} INFO - [2022-03-22 17:44:10,652] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:10,782] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:10,826] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:11,105] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.471 seconds
[2022-03-22 17:44:14,720] {scheduler_job.py:153} INFO - Started process (PID=1113) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:14,726] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:14,728] {logging_mixin.py:112} INFO - [2022-03-22 17:44:14,727] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:14,761] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:14,770] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:14,909] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.190 seconds
[2022-03-22 17:44:18,726] {scheduler_job.py:153} INFO - Started process (PID=1115) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:18,738] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:18,741] {logging_mixin.py:112} INFO - [2022-03-22 17:44:18,740] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:18,792] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:18,810] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:19,176] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.450 seconds
[2022-03-22 17:44:22,769] {scheduler_job.py:153} INFO - Started process (PID=1118) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:22,779] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:22,784] {logging_mixin.py:112} INFO - [2022-03-22 17:44:22,783] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:22,892] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:22,921] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:23,398] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.634 seconds
[2022-03-22 17:44:26,776] {scheduler_job.py:153} INFO - Started process (PID=1120) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:26,790] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:26,793] {logging_mixin.py:112} INFO - [2022-03-22 17:44:26,793] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:26,984] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:26,997] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:27,188] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.413 seconds
[2022-03-22 17:44:30,748] {scheduler_job.py:153} INFO - Started process (PID=1122) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:30,759] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:30,761] {logging_mixin.py:112} INFO - [2022-03-22 17:44:30,761] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:30,828] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:30,840] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:31,137] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.389 seconds
[2022-03-22 17:44:34,749] {scheduler_job.py:153} INFO - Started process (PID=1124) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:34,755] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:34,757] {logging_mixin.py:112} INFO - [2022-03-22 17:44:34,756] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:34,797] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:34,809] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:34,992] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.244 seconds
[2022-03-22 17:44:38,748] {scheduler_job.py:153} INFO - Started process (PID=1126) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:38,754] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:38,758] {logging_mixin.py:112} INFO - [2022-03-22 17:44:38,758] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:38,791] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:38,802] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:38,972] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.223 seconds
[2022-03-22 17:44:42,777] {scheduler_job.py:153} INFO - Started process (PID=1128) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:42,789] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:42,791] {logging_mixin.py:112} INFO - [2022-03-22 17:44:42,791] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:42,876] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:42,906] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:43,127] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.350 seconds
[2022-03-22 17:44:46,761] {scheduler_job.py:153} INFO - Started process (PID=1130) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:46,771] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:46,773] {logging_mixin.py:112} INFO - [2022-03-22 17:44:46,773] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:46,830] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:46,849] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:47,075] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.315 seconds
[2022-03-22 17:44:50,770] {scheduler_job.py:153} INFO - Started process (PID=1132) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:50,778] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:50,781] {logging_mixin.py:112} INFO - [2022-03-22 17:44:50,781] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:50,842] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:50,855] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:51,093] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.324 seconds
[2022-03-22 17:44:55,008] {scheduler_job.py:153} INFO - Started process (PID=1135) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:55,032] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:55,072] {logging_mixin.py:112} INFO - [2022-03-22 17:44:55,071] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:55,540] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:55,575] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:56,025] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.016 seconds
[2022-03-22 17:44:58,824] {scheduler_job.py:153} INFO - Started process (PID=1137) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:58,834] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:44:58,841] {logging_mixin.py:112} INFO - [2022-03-22 17:44:58,839] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:58,920] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:44:58,932] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:44:59,152] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.328 seconds
[2022-03-22 17:45:02,845] {scheduler_job.py:153} INFO - Started process (PID=1139) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:02,871] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:02,873] {logging_mixin.py:112} INFO - [2022-03-22 17:45:02,873] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:03,221] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:03,266] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:03,625] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.780 seconds
[2022-03-22 17:45:06,804] {scheduler_job.py:153} INFO - Started process (PID=1141) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:06,821] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:06,823] {logging_mixin.py:112} INFO - [2022-03-22 17:45:06,822] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:06,904] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:06,937] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:07,262] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.458 seconds
[2022-03-22 17:45:10,868] {scheduler_job.py:153} INFO - Started process (PID=1143) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:10,885] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:10,889] {logging_mixin.py:112} INFO - [2022-03-22 17:45:10,889] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:10,952] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:10,967] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:11,334] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.465 seconds
[2022-03-22 17:45:14,814] {scheduler_job.py:153} INFO - Started process (PID=1145) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:14,823] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:14,825] {logging_mixin.py:112} INFO - [2022-03-22 17:45:14,825] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:14,872] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:14,883] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:15,165] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.351 seconds
[2022-03-22 17:45:18,903] {scheduler_job.py:153} INFO - Started process (PID=1147) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:18,914] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:18,921] {logging_mixin.py:112} INFO - [2022-03-22 17:45:18,920] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:18,974] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:18,992] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:19,388] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.490 seconds
[2022-03-22 17:45:22,813] {scheduler_job.py:153} INFO - Started process (PID=1149) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:22,821] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:22,822] {logging_mixin.py:112} INFO - [2022-03-22 17:45:22,822] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:22,875] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:22,888] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:23,063] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.250 seconds
[2022-03-22 17:45:26,819] {scheduler_job.py:153} INFO - Started process (PID=1151) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:26,827] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:26,828] {logging_mixin.py:112} INFO - [2022-03-22 17:45:26,828] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:26,896] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:26,915] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:27,091] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.272 seconds
[2022-03-22 17:45:30,827] {scheduler_job.py:153} INFO - Started process (PID=1154) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:30,841] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:30,843] {logging_mixin.py:112} INFO - [2022-03-22 17:45:30,843] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:30,899] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:30,917] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:31,176] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.350 seconds
[2022-03-22 17:45:34,809] {scheduler_job.py:153} INFO - Started process (PID=1156) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:34,815] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:34,817] {logging_mixin.py:112} INFO - [2022-03-22 17:45:34,817] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:34,852] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:34,860] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:35,187] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.378 seconds
[2022-03-22 17:45:38,821] {scheduler_job.py:153} INFO - Started process (PID=1158) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:38,827] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:38,829] {logging_mixin.py:112} INFO - [2022-03-22 17:45:38,829] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:38,888] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:38,899] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:39,146] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.326 seconds
[2022-03-22 17:45:42,852] {scheduler_job.py:153} INFO - Started process (PID=1160) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:42,867] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:42,869] {logging_mixin.py:112} INFO - [2022-03-22 17:45:42,869] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:42,924] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:42,937] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:43,228] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.377 seconds
[2022-03-22 17:45:46,842] {scheduler_job.py:153} INFO - Started process (PID=1162) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:46,852] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:46,855] {logging_mixin.py:112} INFO - [2022-03-22 17:45:46,855] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:46,905] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:46,915] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:47,153] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.311 seconds
[2022-03-22 17:45:50,942] {scheduler_job.py:153} INFO - Started process (PID=1164) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:50,953] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:50,957] {logging_mixin.py:112} INFO - [2022-03-22 17:45:50,955] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:51,061] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:51,080] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:51,459] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.517 seconds
[2022-03-22 17:45:54,842] {scheduler_job.py:153} INFO - Started process (PID=1166) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:54,850] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:54,852] {logging_mixin.py:112} INFO - [2022-03-22 17:45:54,851] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:54,897] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:54,914] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:55,267] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.425 seconds
[2022-03-22 17:45:58,918] {scheduler_job.py:153} INFO - Started process (PID=1168) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:58,928] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:45:58,932] {logging_mixin.py:112} INFO - [2022-03-22 17:45:58,932] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:58,998] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:45:59,016] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:45:59,287] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.369 seconds
[2022-03-22 17:46:03,115] {scheduler_job.py:153} INFO - Started process (PID=1171) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:03,139] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:03,141] {logging_mixin.py:112} INFO - [2022-03-22 17:46:03,140] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:03,298] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:03,364] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:04,141] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.027 seconds
[2022-03-22 17:46:07,440] {scheduler_job.py:153} INFO - Started process (PID=1173) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:07,448] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:07,451] {logging_mixin.py:112} INFO - [2022-03-22 17:46:07,450] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:07,513] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:07,529] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:08,228] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.789 seconds
[2022-03-22 17:46:11,257] {scheduler_job.py:153} INFO - Started process (PID=1175) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:11,275] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:11,286] {logging_mixin.py:112} INFO - [2022-03-22 17:46:11,285] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:11,350] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:11,370] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:11,720] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.464 seconds
[2022-03-22 17:46:14,908] {scheduler_job.py:153} INFO - Started process (PID=1177) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:14,914] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:14,916] {logging_mixin.py:112} INFO - [2022-03-22 17:46:14,916] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:14,958] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:14,967] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:15,762] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.855 seconds
[2022-03-22 17:46:19,736] {scheduler_job.py:153} INFO - Started process (PID=1179) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:19,752] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:19,756] {logging_mixin.py:112} INFO - [2022-03-22 17:46:19,756] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:19,808] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:19,825] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:20,102] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.366 seconds
[2022-03-22 17:46:22,934] {scheduler_job.py:153} INFO - Started process (PID=1181) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:22,945] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:22,951] {logging_mixin.py:112} INFO - [2022-03-22 17:46:22,950] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:23,024] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:23,046] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:23,342] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.408 seconds
[2022-03-22 17:46:26,942] {scheduler_job.py:153} INFO - Started process (PID=1183) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:26,960] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:26,968] {logging_mixin.py:112} INFO - [2022-03-22 17:46:26,968] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:27,032] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:27,051] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:27,812] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.871 seconds
[2022-03-22 17:46:31,058] {scheduler_job.py:153} INFO - Started process (PID=1185) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:31,067] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:31,070] {logging_mixin.py:112} INFO - [2022-03-22 17:46:31,070] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:31,205] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:31,225] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:32,017] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.960 seconds
[2022-03-22 17:46:34,957] {scheduler_job.py:153} INFO - Started process (PID=1187) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:34,968] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:34,970] {logging_mixin.py:112} INFO - [2022-03-22 17:46:34,970] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:35,041] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:35,055] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:35,316] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.360 seconds
[2022-03-22 17:46:38,987] {scheduler_job.py:153} INFO - Started process (PID=1190) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:39,000] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:39,002] {logging_mixin.py:112} INFO - [2022-03-22 17:46:39,002] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:39,087] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:39,098] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:39,311] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.323 seconds
[2022-03-22 17:46:43,078] {scheduler_job.py:153} INFO - Started process (PID=1192) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:43,091] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:43,092] {logging_mixin.py:112} INFO - [2022-03-22 17:46:43,092] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:43,161] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:43,190] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:43,740] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.663 seconds
[2022-03-22 17:46:47,013] {scheduler_job.py:153} INFO - Started process (PID=1194) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:47,024] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:47,026] {logging_mixin.py:112} INFO - [2022-03-22 17:46:47,025] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:47,083] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:47,099] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:48,373] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.360 seconds
[2022-03-22 17:46:50,974] {scheduler_job.py:153} INFO - Started process (PID=1196) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:50,983] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:50,984] {logging_mixin.py:112} INFO - [2022-03-22 17:46:50,984] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:51,024] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:51,037] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:51,207] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.233 seconds
[2022-03-22 17:46:54,999] {scheduler_job.py:153} INFO - Started process (PID=1198) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:55,006] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:55,008] {logging_mixin.py:112} INFO - [2022-03-22 17:46:55,008] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:55,061] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:55,074] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:55,261] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.261 seconds
[2022-03-22 17:46:58,984] {scheduler_job.py:153} INFO - Started process (PID=1200) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:58,992] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:46:58,994] {logging_mixin.py:112} INFO - [2022-03-22 17:46:58,993] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:59,030] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:46:59,039] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:46:59,190] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.206 seconds
[2022-03-22 17:47:02,987] {scheduler_job.py:153} INFO - Started process (PID=1202) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:02,994] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:47:03,002] {logging_mixin.py:112} INFO - [2022-03-22 17:47:03,002] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:03,062] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:47:03,085] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:03,274] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.286 seconds
[2022-03-22 17:47:07,226] {scheduler_job.py:153} INFO - Started process (PID=1204) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:07,245] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:47:07,249] {logging_mixin.py:112} INFO - [2022-03-22 17:47:07,248] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:07,343] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:47:07,366] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:07,558] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.332 seconds
[2022-03-22 17:47:11,034] {scheduler_job.py:153} INFO - Started process (PID=1207) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:11,043] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:47:11,044] {logging_mixin.py:112} INFO - [2022-03-22 17:47:11,044] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:11,104] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:47:11,120] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:11,367] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.333 seconds
[2022-03-22 17:47:15,001] {scheduler_job.py:153} INFO - Started process (PID=1209) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:15,009] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:47:15,010] {logging_mixin.py:112} INFO - [2022-03-22 17:47:15,010] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:15,058] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:47:15,073] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:15,256] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.256 seconds
[2022-03-22 17:47:19,049] {scheduler_job.py:153} INFO - Started process (PID=1211) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:19,055] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:47:19,057] {logging_mixin.py:112} INFO - [2022-03-22 17:47:19,057] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:19,102] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:47:19,121] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:19,286] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.238 seconds
[2022-03-22 17:47:23,006] {scheduler_job.py:153} INFO - Started process (PID=1213) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:23,013] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:47:23,015] {logging_mixin.py:112} INFO - [2022-03-22 17:47:23,015] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:23,056] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:47:23,065] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:23,235] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.229 seconds
[2022-03-22 17:47:27,018] {scheduler_job.py:153} INFO - Started process (PID=1215) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:27,029] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:47:27,031] {logging_mixin.py:112} INFO - [2022-03-22 17:47:27,031] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:27,083] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:47:27,098] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:27,311] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.292 seconds
[2022-03-22 17:47:31,077] {scheduler_job.py:153} INFO - Started process (PID=1217) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:31,089] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:47:31,090] {logging_mixin.py:112} INFO - [2022-03-22 17:47:31,090] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:31,158] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:47:31,171] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:31,364] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.287 seconds
[2022-03-22 17:47:35,026] {scheduler_job.py:153} INFO - Started process (PID=1219) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:35,036] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:47:35,037] {logging_mixin.py:112} INFO - [2022-03-22 17:47:35,037] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:35,084] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:47:35,099] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:35,320] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.294 seconds
[2022-03-22 17:47:39,038] {scheduler_job.py:153} INFO - Started process (PID=1221) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:39,047] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:47:39,051] {logging_mixin.py:112} INFO - [2022-03-22 17:47:39,051] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:39,100] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:47:39,112] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:39,306] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.269 seconds
[2022-03-22 17:47:54,181] {scheduler_job.py:153} INFO - Started process (PID=1224) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:54,294] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:47:54,364] {logging_mixin.py:112} INFO - [2022-03-22 17:47:54,364] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:47:57,896] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:47:58,026] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:02,699] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 8.517 seconds
[2022-03-22 17:48:04,081] {scheduler_job.py:153} INFO - Started process (PID=1226) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:04,091] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:04,097] {logging_mixin.py:112} INFO - [2022-03-22 17:48:04,093] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:04,168] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:04,189] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:04,763] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.682 seconds
[2022-03-22 17:48:09,989] {scheduler_job.py:153} INFO - Started process (PID=1228) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:10,047] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:10,079] {logging_mixin.py:112} INFO - [2022-03-22 17:48:10,079] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:11,649] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:11,670] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:12,072] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 2.083 seconds
[2022-03-22 17:48:13,435] {scheduler_job.py:153} INFO - Started process (PID=1230) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:13,441] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:13,442] {logging_mixin.py:112} INFO - [2022-03-22 17:48:13,442] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:13,489] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:13,501] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:14,090] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.655 seconds
[2022-03-22 17:48:16,693] {scheduler_job.py:153} INFO - Started process (PID=1232) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:16,700] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:16,702] {logging_mixin.py:112} INFO - [2022-03-22 17:48:16,701] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:16,745] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:16,764] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:17,108] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.416 seconds
[2022-03-22 17:48:20,689] {scheduler_job.py:153} INFO - Started process (PID=1234) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:20,702] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:20,704] {logging_mixin.py:112} INFO - [2022-03-22 17:48:20,704] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:20,770] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:20,785] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:21,242] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.553 seconds
[2022-03-22 17:48:24,764] {scheduler_job.py:153} INFO - Started process (PID=1236) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:24,773] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:24,775] {logging_mixin.py:112} INFO - [2022-03-22 17:48:24,775] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:24,828] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:24,851] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:25,361] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.599 seconds
[2022-03-22 17:48:28,712] {scheduler_job.py:153} INFO - Started process (PID=1238) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:28,720] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:28,722] {logging_mixin.py:112} INFO - [2022-03-22 17:48:28,722] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:28,769] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:28,782] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:29,081] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.369 seconds
[2022-03-22 17:48:32,709] {scheduler_job.py:153} INFO - Started process (PID=1240) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:32,720] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:32,721] {logging_mixin.py:112} INFO - [2022-03-22 17:48:32,721] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:32,786] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:32,809] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:33,154] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.445 seconds
[2022-03-22 17:48:37,433] {scheduler_job.py:153} INFO - Started process (PID=1243) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:37,442] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:37,445] {logging_mixin.py:112} INFO - [2022-03-22 17:48:37,444] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:37,634] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:37,667] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:38,019] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.586 seconds
[2022-03-22 17:48:40,799] {scheduler_job.py:153} INFO - Started process (PID=1245) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:40,812] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:40,814] {logging_mixin.py:112} INFO - [2022-03-22 17:48:40,814] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:40,865] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:40,877] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:41,109] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.310 seconds
[2022-03-22 17:48:44,802] {scheduler_job.py:153} INFO - Started process (PID=1247) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:44,810] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:44,814] {logging_mixin.py:112} INFO - [2022-03-22 17:48:44,814] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:44,846] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:44,857] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:45,027] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.225 seconds
[2022-03-22 17:48:48,846] {scheduler_job.py:153} INFO - Started process (PID=1249) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:48,852] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:48,853] {logging_mixin.py:112} INFO - [2022-03-22 17:48:48,853] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:48,884] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:48,898] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:49,118] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.272 seconds
[2022-03-22 17:48:52,824] {scheduler_job.py:153} INFO - Started process (PID=1251) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:52,830] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:52,833] {logging_mixin.py:112} INFO - [2022-03-22 17:48:52,832] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:52,868] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:52,877] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:53,079] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.256 seconds
[2022-03-22 17:48:56,822] {scheduler_job.py:153} INFO - Started process (PID=1253) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:56,836] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:48:56,838] {logging_mixin.py:112} INFO - [2022-03-22 17:48:56,838] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:56,882] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:48:56,895] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:48:57,099] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.276 seconds
[2022-03-22 17:49:00,901] {scheduler_job.py:153} INFO - Started process (PID=1255) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:00,913] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:00,917] {logging_mixin.py:112} INFO - [2022-03-22 17:49:00,916] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:01,008] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:01,028] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:01,206] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.305 seconds
[2022-03-22 17:49:04,813] {scheduler_job.py:153} INFO - Started process (PID=1257) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:04,819] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:04,820] {logging_mixin.py:112} INFO - [2022-03-22 17:49:04,820] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:04,853] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:04,864] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:05,017] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.205 seconds
[2022-03-22 17:49:08,835] {scheduler_job.py:153} INFO - Started process (PID=1259) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:08,841] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:08,843] {logging_mixin.py:112} INFO - [2022-03-22 17:49:08,843] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:08,875] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:08,884] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:09,056] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.221 seconds
[2022-03-22 17:49:12,882] {scheduler_job.py:153} INFO - Started process (PID=1262) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:12,889] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:12,890] {logging_mixin.py:112} INFO - [2022-03-22 17:49:12,890] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:12,937] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:12,956] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:13,122] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.240 seconds
[2022-03-22 17:49:16,838] {scheduler_job.py:153} INFO - Started process (PID=1264) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:16,844] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:16,845] {logging_mixin.py:112} INFO - [2022-03-22 17:49:16,845] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:16,881] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:16,889] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:17,040] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.202 seconds
[2022-03-22 17:49:20,847] {scheduler_job.py:153} INFO - Started process (PID=1266) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:20,855] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:20,856] {logging_mixin.py:112} INFO - [2022-03-22 17:49:20,856] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:20,894] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:20,904] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:21,093] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.245 seconds
[2022-03-22 17:49:24,887] {scheduler_job.py:153} INFO - Started process (PID=1268) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:24,892] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:24,894] {logging_mixin.py:112} INFO - [2022-03-22 17:49:24,893] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:24,929] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:24,939] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:25,097] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.210 seconds
[2022-03-22 17:49:28,858] {scheduler_job.py:153} INFO - Started process (PID=1270) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:28,865] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:28,866] {logging_mixin.py:112} INFO - [2022-03-22 17:49:28,866] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:28,915] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:28,932] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:29,118] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.260 seconds
[2022-03-22 17:49:32,855] {scheduler_job.py:153} INFO - Started process (PID=1272) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:32,861] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:32,863] {logging_mixin.py:112} INFO - [2022-03-22 17:49:32,863] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:32,900] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:32,909] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:33,044] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.189 seconds
[2022-03-22 17:49:36,891] {scheduler_job.py:153} INFO - Started process (PID=1274) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:36,896] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:36,898] {logging_mixin.py:112} INFO - [2022-03-22 17:49:36,898] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:36,943] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:36,952] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:37,122] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.231 seconds
[2022-03-22 17:49:40,882] {scheduler_job.py:153} INFO - Started process (PID=1276) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:40,893] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:40,894] {logging_mixin.py:112} INFO - [2022-03-22 17:49:40,894] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:40,931] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:40,941] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:41,123] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.241 seconds
[2022-03-22 17:49:44,891] {scheduler_job.py:153} INFO - Started process (PID=1279) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:44,903] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:44,906] {logging_mixin.py:112} INFO - [2022-03-22 17:49:44,905] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:44,983] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:45,004] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:45,257] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.366 seconds
[2022-03-22 17:49:49,040] {scheduler_job.py:153} INFO - Started process (PID=1281) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:49,047] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:49,048] {logging_mixin.py:112} INFO - [2022-03-22 17:49:49,048] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:49,084] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:49,094] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:49,244] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.205 seconds
[2022-03-22 17:49:52,894] {scheduler_job.py:153} INFO - Started process (PID=1283) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:52,902] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:52,903] {logging_mixin.py:112} INFO - [2022-03-22 17:49:52,903] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:52,947] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:52,958] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:53,140] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.246 seconds
[2022-03-22 17:49:56,894] {scheduler_job.py:153} INFO - Started process (PID=1285) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:56,900] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:49:56,901] {logging_mixin.py:112} INFO - [2022-03-22 17:49:56,901] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:56,939] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:49:56,953] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:49:57,100] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.207 seconds
[2022-03-22 17:50:00,926] {scheduler_job.py:153} INFO - Started process (PID=1287) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:00,931] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:00,933] {logging_mixin.py:112} INFO - [2022-03-22 17:50:00,933] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:00,979] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:00,988] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:01,139] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.213 seconds
[2022-03-22 17:50:04,910] {scheduler_job.py:153} INFO - Started process (PID=1289) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:04,917] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:04,918] {logging_mixin.py:112} INFO - [2022-03-22 17:50:04,918] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:04,953] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:04,963] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:05,137] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.227 seconds
[2022-03-22 17:50:08,912] {scheduler_job.py:153} INFO - Started process (PID=1291) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:08,920] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:08,922] {logging_mixin.py:112} INFO - [2022-03-22 17:50:08,921] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:08,954] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:08,964] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:09,453] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.541 seconds
[2022-03-22 17:50:12,966] {scheduler_job.py:153} INFO - Started process (PID=1293) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:12,972] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:12,974] {logging_mixin.py:112} INFO - [2022-03-22 17:50:12,974] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:13,067] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:13,098] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:13,305] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.338 seconds
[2022-03-22 17:50:17,071] {scheduler_job.py:153} INFO - Started process (PID=1296) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:17,090] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:17,126] {logging_mixin.py:112} INFO - [2022-03-22 17:50:17,125] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:17,236] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:17,270] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:17,791] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.720 seconds
[2022-03-22 17:50:20,918] {scheduler_job.py:153} INFO - Started process (PID=1298) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:20,926] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:20,928] {logging_mixin.py:112} INFO - [2022-03-22 17:50:20,928] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:20,993] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:21,009] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:21,251] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.333 seconds
[2022-03-22 17:50:24,991] {scheduler_job.py:153} INFO - Started process (PID=1300) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:25,000] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:25,002] {logging_mixin.py:112} INFO - [2022-03-22 17:50:25,002] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:25,091] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:25,106] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:25,394] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.404 seconds
[2022-03-22 17:50:28,932] {scheduler_job.py:153} INFO - Started process (PID=1302) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:28,939] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:28,941] {logging_mixin.py:112} INFO - [2022-03-22 17:50:28,941] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:28,982] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:28,997] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:29,171] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.239 seconds
[2022-03-22 17:50:32,947] {scheduler_job.py:153} INFO - Started process (PID=1304) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:32,952] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:32,953] {logging_mixin.py:112} INFO - [2022-03-22 17:50:32,953] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:32,983] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:32,992] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:33,131] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.184 seconds
[2022-03-22 17:50:37,000] {scheduler_job.py:153} INFO - Started process (PID=1306) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:37,017] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:37,019] {logging_mixin.py:112} INFO - [2022-03-22 17:50:37,018] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:37,089] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:37,122] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:37,887] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.887 seconds
[2022-03-22 17:50:40,948] {scheduler_job.py:153} INFO - Started process (PID=1308) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:40,958] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:40,960] {logging_mixin.py:112} INFO - [2022-03-22 17:50:40,960] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:41,029] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:41,043] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:41,268] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.320 seconds
[2022-03-22 17:50:44,954] {scheduler_job.py:153} INFO - Started process (PID=1310) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:44,961] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:44,963] {logging_mixin.py:112} INFO - [2022-03-22 17:50:44,963] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:45,022] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:45,038] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:45,277] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.323 seconds
[2022-03-22 17:50:49,326] {scheduler_job.py:153} INFO - Started process (PID=1312) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:49,340] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:49,342] {logging_mixin.py:112} INFO - [2022-03-22 17:50:49,342] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:49,429] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:49,479] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:50,764] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.436 seconds
[2022-03-22 17:50:53,668] {scheduler_job.py:153} INFO - Started process (PID=1315) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:53,684] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:53,687] {logging_mixin.py:112} INFO - [2022-03-22 17:50:53,687] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:53,812] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:53,840] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:54,209] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.541 seconds
[2022-03-22 17:50:56,966] {scheduler_job.py:153} INFO - Started process (PID=1317) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:56,975] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:50:56,977] {logging_mixin.py:112} INFO - [2022-03-22 17:50:56,976] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:57,029] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:50:57,052] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:50:57,275] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.310 seconds
[2022-03-22 17:51:01,183] {scheduler_job.py:153} INFO - Started process (PID=1319) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:01,189] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:01,190] {logging_mixin.py:112} INFO - [2022-03-22 17:51:01,190] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:01,251] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:01,268] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:01,505] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.323 seconds
[2022-03-22 17:51:05,248] {scheduler_job.py:153} INFO - Started process (PID=1321) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:05,338] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:05,361] {logging_mixin.py:112} INFO - [2022-03-22 17:51:05,360] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:05,678] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:05,711] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:07,187] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.939 seconds
[2022-03-22 17:51:09,478] {scheduler_job.py:153} INFO - Started process (PID=1323) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:09,489] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:09,491] {logging_mixin.py:112} INFO - [2022-03-22 17:51:09,491] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:09,603] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:09,617] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:10,021] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.543 seconds
[2022-03-22 17:51:13,482] {scheduler_job.py:153} INFO - Started process (PID=1325) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:13,492] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:13,494] {logging_mixin.py:112} INFO - [2022-03-22 17:51:13,493] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:13,558] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:13,575] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:13,945] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.464 seconds
[2022-03-22 17:51:17,485] {scheduler_job.py:153} INFO - Started process (PID=1327) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:17,492] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:17,494] {logging_mixin.py:112} INFO - [2022-03-22 17:51:17,493] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:17,547] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:17,561] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:17,784] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.299 seconds
[2022-03-22 17:51:21,489] {scheduler_job.py:153} INFO - Started process (PID=1329) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:21,497] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:21,501] {logging_mixin.py:112} INFO - [2022-03-22 17:51:21,500] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:21,546] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:21,558] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:21,807] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.317 seconds
[2022-03-22 17:51:25,703] {scheduler_job.py:153} INFO - Started process (PID=1331) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:25,721] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:25,724] {logging_mixin.py:112} INFO - [2022-03-22 17:51:25,724] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:25,826] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:25,852] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:26,457] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.754 seconds
[2022-03-22 17:51:29,521] {scheduler_job.py:153} INFO - Started process (PID=1334) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:29,532] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:29,539] {logging_mixin.py:112} INFO - [2022-03-22 17:51:29,539] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:29,627] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:29,648] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:30,183] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.659 seconds
[2022-03-22 17:51:33,526] {scheduler_job.py:153} INFO - Started process (PID=1336) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:33,542] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:33,543] {logging_mixin.py:112} INFO - [2022-03-22 17:51:33,543] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:33,614] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:33,634] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:34,992] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.467 seconds
[2022-03-22 17:51:37,714] {scheduler_job.py:153} INFO - Started process (PID=1338) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:37,738] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:37,752] {logging_mixin.py:112} INFO - [2022-03-22 17:51:37,750] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:37,863] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:37,888] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:38,557] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.843 seconds
[2022-03-22 17:51:41,510] {scheduler_job.py:153} INFO - Started process (PID=1340) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:41,522] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:41,523] {logging_mixin.py:112} INFO - [2022-03-22 17:51:41,523] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:41,586] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:41,602] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:42,026] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.515 seconds
[2022-03-22 17:51:45,524] {scheduler_job.py:153} INFO - Started process (PID=1342) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:45,535] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:45,537] {logging_mixin.py:112} INFO - [2022-03-22 17:51:45,537] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:45,592] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:45,607] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:45,972] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.448 seconds
[2022-03-22 17:51:49,567] {scheduler_job.py:153} INFO - Started process (PID=1344) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:49,582] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:49,589] {logging_mixin.py:112} INFO - [2022-03-22 17:51:49,589] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:49,689] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:49,718] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:50,481] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.913 seconds
[2022-03-22 17:51:53,674] {scheduler_job.py:153} INFO - Started process (PID=1346) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:53,684] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:53,705] {logging_mixin.py:112} INFO - [2022-03-22 17:51:53,705] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:54,047] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:54,161] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:54,592] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.919 seconds
[2022-03-22 17:51:57,528] {scheduler_job.py:153} INFO - Started process (PID=1348) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:57,537] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:51:57,539] {logging_mixin.py:112} INFO - [2022-03-22 17:51:57,539] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:57,592] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:51:57,606] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:51:57,835] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.307 seconds
[2022-03-22 17:52:01,579] {scheduler_job.py:153} INFO - Started process (PID=1351) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:01,594] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:01,596] {logging_mixin.py:112} INFO - [2022-03-22 17:52:01,596] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:01,710] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:01,743] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:02,067] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.487 seconds
[2022-03-22 17:52:05,554] {scheduler_job.py:153} INFO - Started process (PID=1353) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:05,562] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:05,565] {logging_mixin.py:112} INFO - [2022-03-22 17:52:05,565] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:05,625] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:05,642] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:05,862] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.309 seconds
[2022-03-22 17:52:09,544] {scheduler_job.py:153} INFO - Started process (PID=1355) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:09,556] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:09,558] {logging_mixin.py:112} INFO - [2022-03-22 17:52:09,557] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:09,621] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:09,638] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:09,840] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.295 seconds
[2022-03-22 17:52:13,570] {scheduler_job.py:153} INFO - Started process (PID=1357) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:13,584] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:13,587] {logging_mixin.py:112} INFO - [2022-03-22 17:52:13,587] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:13,655] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:13,678] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:13,905] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.336 seconds
[2022-03-22 17:52:17,558] {scheduler_job.py:153} INFO - Started process (PID=1359) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:17,568] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:17,569] {logging_mixin.py:112} INFO - [2022-03-22 17:52:17,569] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:17,627] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:17,641] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:17,868] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.311 seconds
[2022-03-22 17:52:21,562] {scheduler_job.py:153} INFO - Started process (PID=1361) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:21,575] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:21,577] {logging_mixin.py:112} INFO - [2022-03-22 17:52:21,576] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:21,636] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:21,652] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:21,879] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.317 seconds
[2022-03-22 17:52:25,581] {scheduler_job.py:153} INFO - Started process (PID=1363) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:25,593] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:25,594] {logging_mixin.py:112} INFO - [2022-03-22 17:52:25,594] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:25,658] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:25,674] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:25,916] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.335 seconds
[2022-03-22 17:52:29,657] {scheduler_job.py:153} INFO - Started process (PID=1365) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:29,677] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:29,690] {logging_mixin.py:112} INFO - [2022-03-22 17:52:29,690] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:29,762] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:29,790] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:30,277] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.620 seconds
[2022-03-22 17:52:33,585] {scheduler_job.py:153} INFO - Started process (PID=1368) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:33,594] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:33,597] {logging_mixin.py:112} INFO - [2022-03-22 17:52:33,596] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:33,654] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:33,668] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:33,886] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.301 seconds
[2022-03-22 17:52:37,580] {scheduler_job.py:153} INFO - Started process (PID=1370) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:37,591] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:37,593] {logging_mixin.py:112} INFO - [2022-03-22 17:52:37,593] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:37,640] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:37,652] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:37,849] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.269 seconds
[2022-03-22 17:52:41,580] {scheduler_job.py:153} INFO - Started process (PID=1372) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:41,586] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:41,588] {logging_mixin.py:112} INFO - [2022-03-22 17:52:41,588] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:41,629] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:41,641] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:41,811] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.232 seconds
[2022-03-22 17:52:45,595] {scheduler_job.py:153} INFO - Started process (PID=1374) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:45,602] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:45,603] {logging_mixin.py:112} INFO - [2022-03-22 17:52:45,603] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:45,635] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:45,643] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:45,859] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.264 seconds
[2022-03-22 17:52:49,606] {scheduler_job.py:153} INFO - Started process (PID=1376) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:49,617] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:49,619] {logging_mixin.py:112} INFO - [2022-03-22 17:52:49,619] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:49,658] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:49,670] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:49,989] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.383 seconds
[2022-03-22 17:52:53,611] {scheduler_job.py:153} INFO - Started process (PID=1378) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:53,618] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:53,620] {logging_mixin.py:112} INFO - [2022-03-22 17:52:53,620] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:53,662] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:53,673] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:53,861] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.251 seconds
[2022-03-22 17:52:57,607] {scheduler_job.py:153} INFO - Started process (PID=1380) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:57,675] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:52:57,687] {logging_mixin.py:112} INFO - [2022-03-22 17:52:57,687] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:58,050] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:52:58,071] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:52:58,302] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.696 seconds
[2022-03-22 17:53:01,627] {scheduler_job.py:153} INFO - Started process (PID=1382) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:01,651] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:01,654] {logging_mixin.py:112} INFO - [2022-03-22 17:53:01,654] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:01,736] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:01,761] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:02,129] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.502 seconds
[2022-03-22 17:53:05,633] {scheduler_job.py:153} INFO - Started process (PID=1384) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:05,653] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:05,677] {logging_mixin.py:112} INFO - [2022-03-22 17:53:05,676] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:05,860] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:05,878] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:06,154] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.521 seconds
[2022-03-22 17:53:09,884] {scheduler_job.py:153} INFO - Started process (PID=1387) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:09,911] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:09,926] {logging_mixin.py:112} INFO - [2022-03-22 17:53:09,926] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:10,037] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:10,051] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:10,260] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.376 seconds
[2022-03-22 17:53:13,662] {scheduler_job.py:153} INFO - Started process (PID=1389) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:13,683] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:13,690] {logging_mixin.py:112} INFO - [2022-03-22 17:53:13,689] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:13,932] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:13,966] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:14,410] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.749 seconds
[2022-03-22 17:53:17,698] {scheduler_job.py:153} INFO - Started process (PID=1391) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:17,718] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:17,725] {logging_mixin.py:112} INFO - [2022-03-22 17:53:17,724] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:17,948] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:17,987] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:18,457] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.759 seconds
[2022-03-22 17:53:21,639] {scheduler_job.py:153} INFO - Started process (PID=1393) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:21,654] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:21,657] {logging_mixin.py:112} INFO - [2022-03-22 17:53:21,657] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:21,743] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:21,760] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:21,989] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.351 seconds
[2022-03-22 17:53:25,677] {scheduler_job.py:153} INFO - Started process (PID=1395) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:25,698] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:25,703] {logging_mixin.py:112} INFO - [2022-03-22 17:53:25,703] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:25,773] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:25,798] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:26,099] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.431 seconds
[2022-03-22 17:53:30,201] {scheduler_job.py:153} INFO - Started process (PID=1397) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:30,232] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:30,240] {logging_mixin.py:112} INFO - [2022-03-22 17:53:30,240] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:30,992] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:31,014] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:33,153] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 2.952 seconds
[2022-03-22 17:53:35,141] {scheduler_job.py:153} INFO - Started process (PID=1399) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:35,154] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:35,156] {logging_mixin.py:112} INFO - [2022-03-22 17:53:35,156] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:35,214] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:35,232] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:35,594] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.453 seconds
[2022-03-22 17:53:38,245] {scheduler_job.py:153} INFO - Started process (PID=1401) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:38,266] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:38,277] {logging_mixin.py:112} INFO - [2022-03-22 17:53:38,276] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:38,451] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:38,473] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:38,909] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.664 seconds
[2022-03-22 17:53:42,207] {scheduler_job.py:153} INFO - Started process (PID=1404) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:42,226] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:42,238] {logging_mixin.py:112} INFO - [2022-03-22 17:53:42,238] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:42,361] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:42,380] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:42,739] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.532 seconds
[2022-03-22 17:53:48,071] {scheduler_job.py:153} INFO - Started process (PID=1406) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:48,104] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:48,107] {logging_mixin.py:112} INFO - [2022-03-22 17:53:48,107] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:48,401] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:48,434] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:48,905] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.835 seconds
[2022-03-22 17:53:50,399] {scheduler_job.py:153} INFO - Started process (PID=1408) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:50,426] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:50,429] {logging_mixin.py:112} INFO - [2022-03-22 17:53:50,429] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:50,578] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:50,612] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:51,073] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.674 seconds
[2022-03-22 17:53:54,188] {scheduler_job.py:153} INFO - Started process (PID=1410) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:54,195] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:54,200] {logging_mixin.py:112} INFO - [2022-03-22 17:53:54,200] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:54,269] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:54,285] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:54,618] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.431 seconds
[2022-03-22 17:53:58,242] {scheduler_job.py:153} INFO - Started process (PID=1412) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:58,253] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:53:58,254] {logging_mixin.py:112} INFO - [2022-03-22 17:53:58,254] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:58,312] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:53:58,328] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:53:58,672] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.430 seconds
[2022-03-22 17:54:02,218] {scheduler_job.py:153} INFO - Started process (PID=1414) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:02,230] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:54:02,240] {logging_mixin.py:112} INFO - [2022-03-22 17:54:02,239] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:02,302] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:54:02,318] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:02,551] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.333 seconds
[2022-03-22 17:54:06,201] {scheduler_job.py:153} INFO - Started process (PID=1416) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:06,211] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:54:06,214] {logging_mixin.py:112} INFO - [2022-03-22 17:54:06,214] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:06,285] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:54:06,307] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:06,663] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.462 seconds
[2022-03-22 17:54:10,226] {scheduler_job.py:153} INFO - Started process (PID=1418) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:10,232] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:54:10,235] {logging_mixin.py:112} INFO - [2022-03-22 17:54:10,235] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:10,279] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:54:10,295] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:10,493] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.267 seconds
[2022-03-22 17:54:14,219] {scheduler_job.py:153} INFO - Started process (PID=1420) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:14,228] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:54:14,234] {logging_mixin.py:112} INFO - [2022-03-22 17:54:14,233] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:14,290] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:54:14,308] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:14,789] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.571 seconds
[2022-03-22 17:54:18,209] {scheduler_job.py:153} INFO - Started process (PID=1423) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:18,220] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:54:18,222] {logging_mixin.py:112} INFO - [2022-03-22 17:54:18,221] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:18,277] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:54:18,293] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:18,534] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.325 seconds
[2022-03-22 17:54:22,260] {scheduler_job.py:153} INFO - Started process (PID=1425) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:22,273] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:54:22,275] {logging_mixin.py:112} INFO - [2022-03-22 17:54:22,274] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:22,428] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:54:22,470] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:22,738] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.478 seconds
[2022-03-22 17:54:26,260] {scheduler_job.py:153} INFO - Started process (PID=1427) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:26,276] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:54:26,278] {logging_mixin.py:112} INFO - [2022-03-22 17:54:26,277] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:26,352] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:54:26,379] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:26,673] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.414 seconds
[2022-03-22 17:54:30,413] {scheduler_job.py:153} INFO - Started process (PID=1429) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:30,433] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:54:30,442] {logging_mixin.py:112} INFO - [2022-03-22 17:54:30,441] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:30,582] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:54:30,632] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:31,136] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:54:31,254] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:54:31,308] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.895 seconds
[2022-03-22 17:54:34,341] {scheduler_job.py:153} INFO - Started process (PID=1431) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:34,359] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:54:34,369] {logging_mixin.py:112} INFO - [2022-03-22 17:54:34,369] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:34,447] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:54:34,462] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:34,851] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:54:35,003] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:54:35,061] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.720 seconds
[2022-03-22 17:54:38,404] {scheduler_job.py:153} INFO - Started process (PID=1433) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:38,422] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:54:38,440] {logging_mixin.py:112} INFO - [2022-03-22 17:54:38,439] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:38,703] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:54:38,736] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:54:39,345] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:54:39,474] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 17:54:35.643029+00:00: manual__2022-03-22T17:54:35.643029+00:00, externally triggered: True>
[2022-03-22 17:54:39,636] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:54:39,663] {scheduler_job.py:1640} INFO - Creating / updating <TaskInstance: bigquery_data_load.load_data 2022-03-22 17:54:35.643029+00:00 [scheduled]> in ORM
[2022-03-22 17:54:39,839] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.436 seconds
[2022-03-22 17:55:09,073] {scheduler_job.py:153} INFO - Started process (PID=1442) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:09,083] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:09,085] {logging_mixin.py:112} INFO - [2022-03-22 17:55:09,084] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:09,129] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:09,141] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:09,400] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:09,533] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 17:54:35.643029+00:00: manual__2022-03-22T17:54:35.643029+00:00, externally triggered: True>
[2022-03-22 17:55:09,850] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:09,876] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.802 seconds
[2022-03-22 17:55:13,227] {scheduler_job.py:153} INFO - Started process (PID=1444) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:13,253] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:13,256] {logging_mixin.py:112} INFO - [2022-03-22 17:55:13,256] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:13,418] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:13,453] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:14,658] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:14,749] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 17:54:35.643029+00:00: manual__2022-03-22T17:54:35.643029+00:00, externally triggered: True>
[2022-03-22 17:55:14,788] {logging_mixin.py:112} INFO - [2022-03-22 17:55:14,788] {dagrun.py:309} INFO - Marking run <DagRun bigquery_data_load @ 2022-03-22 17:54:35.643029+00:00: manual__2022-03-22T17:54:35.643029+00:00, externally triggered: True> failed
[2022-03-22 17:55:15,000] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:15,022] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.795 seconds
[2022-03-22 17:55:16,954] {scheduler_job.py:153} INFO - Started process (PID=1446) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:16,963] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:16,969] {logging_mixin.py:112} INFO - [2022-03-22 17:55:16,968] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:17,025] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:17,046] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:17,463] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:17,549] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:17,571] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.617 seconds
[2022-03-22 17:55:20,968] {scheduler_job.py:153} INFO - Started process (PID=1448) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:20,974] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:20,975] {logging_mixin.py:112} INFO - [2022-03-22 17:55:20,975] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:21,009] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:21,018] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:21,303] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:21,385] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:21,406] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.438 seconds
[2022-03-22 17:55:24,951] {scheduler_job.py:153} INFO - Started process (PID=1451) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:24,960] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:24,976] {logging_mixin.py:112} INFO - [2022-03-22 17:55:24,976] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:25,035] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:25,051] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:25,573] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:25,705] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:25,758] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.807 seconds
[2022-03-22 17:55:28,951] {scheduler_job.py:153} INFO - Started process (PID=1453) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:28,958] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:28,959] {logging_mixin.py:112} INFO - [2022-03-22 17:55:28,959] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:28,992] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:29,001] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:29,377] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:29,456] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:29,476] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.525 seconds
[2022-03-22 17:55:32,973] {scheduler_job.py:153} INFO - Started process (PID=1455) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:32,978] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:32,982] {logging_mixin.py:112} INFO - [2022-03-22 17:55:32,981] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:33,013] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:33,025] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:33,359] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:33,450] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:33,483] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.510 seconds
[2022-03-22 17:55:36,984] {scheduler_job.py:153} INFO - Started process (PID=1457) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:36,991] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:36,992] {logging_mixin.py:112} INFO - [2022-03-22 17:55:36,992] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:37,024] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:37,036] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:37,408] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:37,498] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:37,524] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.541 seconds
[2022-03-22 17:55:40,984] {scheduler_job.py:153} INFO - Started process (PID=1459) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:40,992] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:40,993] {logging_mixin.py:112} INFO - [2022-03-22 17:55:40,993] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:41,050] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:41,065] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:41,476] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:41,587] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:41,613] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.628 seconds
[2022-03-22 17:55:45,020] {scheduler_job.py:153} INFO - Started process (PID=1461) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:45,029] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:45,031] {logging_mixin.py:112} INFO - [2022-03-22 17:55:45,031] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:45,097] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:45,112] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:45,524] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:45,629] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:45,655] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.636 seconds
[2022-03-22 17:55:49,008] {scheduler_job.py:153} INFO - Started process (PID=1463) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:49,020] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:49,022] {logging_mixin.py:112} INFO - [2022-03-22 17:55:49,022] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:49,126] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:49,148] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:49,606] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:49,774] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:49,845] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.837 seconds
[2022-03-22 17:55:53,148] {scheduler_job.py:153} INFO - Started process (PID=1465) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:53,218] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:55:53,296] {logging_mixin.py:112} INFO - [2022-03-22 17:55:53,296] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:53,475] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:55:53,505] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:55:54,440] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:55:54,529] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:55:54,552] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.404 seconds
[2022-03-22 17:56:00,435] {scheduler_job.py:153} INFO - Started process (PID=1468) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:00,458] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:00,539] {logging_mixin.py:112} INFO - [2022-03-22 17:56:00,538] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:00,797] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:00,831] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:05,045] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:05,234] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:05,356] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 4.921 seconds
[2022-03-22 17:56:06,111] {scheduler_job.py:153} INFO - Started process (PID=1470) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:06,119] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:06,120] {logging_mixin.py:112} INFO - [2022-03-22 17:56:06,120] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:06,189] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:06,203] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:06,603] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:06,793] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:06,829] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.718 seconds
[2022-03-22 17:56:07,828] {scheduler_job.py:153} INFO - Started process (PID=1472) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:07,842] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:07,844] {logging_mixin.py:112} INFO - [2022-03-22 17:56:07,843] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:07,890] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:07,900] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:08,103] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:08,184] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:08,205] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.377 seconds
[2022-03-22 17:56:11,793] {scheduler_job.py:153} INFO - Started process (PID=1474) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:11,802] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:11,804] {logging_mixin.py:112} INFO - [2022-03-22 17:56:11,803] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:11,864] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:11,878] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:12,182] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:12,302] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:12,332] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.539 seconds
[2022-03-22 17:56:15,810] {scheduler_job.py:153} INFO - Started process (PID=1476) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:15,816] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:15,817] {logging_mixin.py:112} INFO - [2022-03-22 17:56:15,817] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:15,846] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:15,855] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:16,232] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:16,318] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:16,344] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.534 seconds
[2022-03-22 17:56:19,791] {scheduler_job.py:153} INFO - Started process (PID=1478) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:19,797] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:19,799] {logging_mixin.py:112} INFO - [2022-03-22 17:56:19,799] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:19,832] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:19,841] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:20,125] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:20,211] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:20,237] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.446 seconds
[2022-03-22 17:56:23,814] {scheduler_job.py:153} INFO - Started process (PID=1480) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:23,821] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:23,822] {logging_mixin.py:112} INFO - [2022-03-22 17:56:23,822] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:23,854] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:23,862] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:24,162] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:24,247] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:24,268] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.454 seconds
[2022-03-22 17:56:27,820] {scheduler_job.py:153} INFO - Started process (PID=1482) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:27,825] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:27,826] {logging_mixin.py:112} INFO - [2022-03-22 17:56:27,826] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:27,864] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:27,873] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:28,098] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:28,175] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:28,199] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.379 seconds
[2022-03-22 17:56:31,820] {scheduler_job.py:153} INFO - Started process (PID=1484) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:31,833] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:31,834] {logging_mixin.py:112} INFO - [2022-03-22 17:56:31,834] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:31,865] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:31,874] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:32,086] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:32,166] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:32,187] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.367 seconds
[2022-03-22 17:56:35,824] {scheduler_job.py:153} INFO - Started process (PID=1486) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:35,833] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:35,834] {logging_mixin.py:112} INFO - [2022-03-22 17:56:35,834] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:35,873] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:35,885] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:36,383] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:36,478] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:36,525] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.701 seconds
[2022-03-22 17:56:39,871] {scheduler_job.py:153} INFO - Started process (PID=1489) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:39,880] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:39,882] {logging_mixin.py:112} INFO - [2022-03-22 17:56:39,882] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:40,916] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:40,963] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:41,753] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:41,878] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:41,955] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 2.084 seconds
[2022-03-22 17:56:44,134] {scheduler_job.py:153} INFO - Started process (PID=1491) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:44,140] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:44,141] {logging_mixin.py:112} INFO - [2022-03-22 17:56:44,141] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:44,172] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:44,182] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:44,422] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:44,527] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:44,560] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.426 seconds
[2022-03-22 17:56:48,266] {scheduler_job.py:153} INFO - Started process (PID=1493) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:48,272] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:48,301] {logging_mixin.py:112} INFO - [2022-03-22 17:56:48,301] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:48,391] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:48,424] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:48,903] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:49,057] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:49,086] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.831 seconds
[2022-03-22 17:56:52,218] {scheduler_job.py:153} INFO - Started process (PID=1495) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:52,234] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:52,236] {logging_mixin.py:112} INFO - [2022-03-22 17:56:52,236] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:52,553] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:52,584] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:53,051] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:53,195] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:53,253] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.035 seconds
[2022-03-22 17:56:56,312] {scheduler_job.py:153} INFO - Started process (PID=1497) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:56,320] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:56:56,321] {logging_mixin.py:112} INFO - [2022-03-22 17:56:56,321] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:56,385] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:56:56,447] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:56:57,153] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:56:57,304] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:56:57,388] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.076 seconds
[2022-03-22 17:57:00,909] {scheduler_job.py:153} INFO - Started process (PID=1499) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:00,945] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:00,974] {logging_mixin.py:112} INFO - [2022-03-22 17:57:00,974] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:01,060] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:01,074] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:01,477] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:01,637] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:01,673] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.764 seconds
[2022-03-22 17:57:04,840] {scheduler_job.py:153} INFO - Started process (PID=1534) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:04,857] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:04,859] {logging_mixin.py:112} INFO - [2022-03-22 17:57:04,858] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:04,934] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:04,955] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:05,241] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:05,323] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:05,357] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.517 seconds
[2022-03-22 17:57:08,844] {scheduler_job.py:153} INFO - Started process (PID=1551) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:08,862] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:08,893] {logging_mixin.py:112} INFO - [2022-03-22 17:57:08,893] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:08,970] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:08,992] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:09,387] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:09,540] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:09,587] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.743 seconds
[2022-03-22 17:57:12,837] {scheduler_job.py:153} INFO - Started process (PID=1559) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:12,845] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:12,848] {logging_mixin.py:112} INFO - [2022-03-22 17:57:12,847] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:12,914] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:12,930] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:13,292] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:13,427] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:13,465] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.628 seconds
[2022-03-22 17:57:16,862] {scheduler_job.py:153} INFO - Started process (PID=1561) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:16,880] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:16,887] {logging_mixin.py:112} INFO - [2022-03-22 17:57:16,887] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:16,952] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:16,974] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:17,329] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:17,444] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:17,469] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.606 seconds
[2022-03-22 17:57:20,832] {scheduler_job.py:153} INFO - Started process (PID=1563) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:20,840] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:20,842] {logging_mixin.py:112} INFO - [2022-03-22 17:57:20,842] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:20,884] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:20,897] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:21,160] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:21,260] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:21,286] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.454 seconds
[2022-03-22 17:57:24,850] {scheduler_job.py:153} INFO - Started process (PID=1565) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:24,865] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:24,875] {logging_mixin.py:112} INFO - [2022-03-22 17:57:24,873] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:25,006] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:25,019] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:25,368] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:25,454] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:25,475] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.624 seconds
[2022-03-22 17:57:28,877] {scheduler_job.py:153} INFO - Started process (PID=1567) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:28,890] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:28,893] {logging_mixin.py:112} INFO - [2022-03-22 17:57:28,893] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:29,045] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:29,083] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:29,791] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:30,020] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:30,064] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.186 seconds
[2022-03-22 17:57:33,111] {scheduler_job.py:153} INFO - Started process (PID=1569) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:33,134] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:33,137] {logging_mixin.py:112} INFO - [2022-03-22 17:57:33,136] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:33,257] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:33,269] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:33,509] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:33,627] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:33,656] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.546 seconds
[2022-03-22 17:57:36,884] {scheduler_job.py:153} INFO - Started process (PID=1571) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:36,890] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:36,892] {logging_mixin.py:112} INFO - [2022-03-22 17:57:36,891] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:36,926] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:36,936] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:37,188] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:37,274] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:37,295] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.411 seconds
[2022-03-22 17:57:41,090] {scheduler_job.py:153} INFO - Started process (PID=1573) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:41,102] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:41,107] {logging_mixin.py:112} INFO - [2022-03-22 17:57:41,107] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:41,178] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:41,195] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:42,207] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:42,396] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:42,452] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.362 seconds
[2022-03-22 17:57:47,029] {scheduler_job.py:153} INFO - Started process (PID=1576) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:47,038] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:47,040] {logging_mixin.py:112} INFO - [2022-03-22 17:57:47,039] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:47,306] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:47,320] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:48,073] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:48,222] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:48,244] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.216 seconds
[2022-03-22 17:57:49,918] {scheduler_job.py:153} INFO - Started process (PID=1578) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:49,929] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:49,932] {logging_mixin.py:112} INFO - [2022-03-22 17:57:49,931] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:49,979] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:49,988] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:50,241] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:50,334] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:50,374] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.456 seconds
[2022-03-22 17:57:53,927] {scheduler_job.py:153} INFO - Started process (PID=1580) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:53,935] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:53,937] {logging_mixin.py:112} INFO - [2022-03-22 17:57:53,937] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:53,971] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:53,981] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:54,387] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:54,647] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:54,677] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.750 seconds
[2022-03-22 17:57:57,928] {scheduler_job.py:153} INFO - Started process (PID=1582) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:57,943] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:57:57,945] {logging_mixin.py:112} INFO - [2022-03-22 17:57:57,945] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:58,122] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:57:58,137] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:57:59,274] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:57:59,466] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:57:59,507] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.579 seconds
[2022-03-22 17:58:02,995] {scheduler_job.py:153} INFO - Started process (PID=1584) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:03,169] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:03,259] {logging_mixin.py:112} INFO - [2022-03-22 17:58:03,259] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:04,478] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:04,508] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:05,871] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:06,186] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:06,220] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 3.225 seconds
[2022-03-22 17:58:06,798] {scheduler_job.py:153} INFO - Started process (PID=1586) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:06,804] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:06,807] {logging_mixin.py:112} INFO - [2022-03-22 17:58:06,807] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:06,859] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:06,878] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:07,175] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:07,589] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:07,621] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.824 seconds
[2022-03-22 17:58:10,409] {scheduler_job.py:153} INFO - Started process (PID=1588) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:10,414] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:10,416] {logging_mixin.py:112} INFO - [2022-03-22 17:58:10,416] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:10,449] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:10,457] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:10,677] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:10,761] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:10,789] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.380 seconds
[2022-03-22 17:58:14,478] {scheduler_job.py:153} INFO - Started process (PID=1590) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:14,492] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:14,494] {logging_mixin.py:112} INFO - [2022-03-22 17:58:14,494] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:14,593] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:14,617] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:15,142] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:15,793] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:15,904] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.426 seconds
[2022-03-22 17:58:18,470] {scheduler_job.py:153} INFO - Started process (PID=1593) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:18,476] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:18,478] {logging_mixin.py:112} INFO - [2022-03-22 17:58:18,477] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:18,531] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:18,540] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:18,841] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:18,923] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:18,955] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.485 seconds
[2022-03-22 17:58:22,458] {scheduler_job.py:153} INFO - Started process (PID=1596) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:22,486] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:22,489] {logging_mixin.py:112} INFO - [2022-03-22 17:58:22,488] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:22,545] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:22,559] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:22,846] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:22,922] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:22,951] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.493 seconds
[2022-03-22 17:58:26,434] {scheduler_job.py:153} INFO - Started process (PID=1598) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:26,440] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:26,441] {logging_mixin.py:112} INFO - [2022-03-22 17:58:26,441] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:26,490] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:26,500] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:26,733] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:26,812] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:26,835] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.402 seconds
[2022-03-22 17:58:30,469] {scheduler_job.py:153} INFO - Started process (PID=1600) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:30,477] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:30,479] {logging_mixin.py:112} INFO - [2022-03-22 17:58:30,478] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:30,520] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:30,531] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:30,756] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:30,840] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:30,872] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.403 seconds
[2022-03-22 17:58:34,479] {scheduler_job.py:153} INFO - Started process (PID=1602) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:34,486] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:34,488] {logging_mixin.py:112} INFO - [2022-03-22 17:58:34,487] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:34,523] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:34,533] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:34,762] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:34,844] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:34,865] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.386 seconds
[2022-03-22 17:58:38,461] {scheduler_job.py:153} INFO - Started process (PID=1604) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:38,469] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:38,470] {logging_mixin.py:112} INFO - [2022-03-22 17:58:38,470] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:38,510] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:38,520] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:38,750] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:38,841] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:38,865] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.404 seconds
[2022-03-22 17:58:42,474] {scheduler_job.py:153} INFO - Started process (PID=1606) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:42,483] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:42,484] {logging_mixin.py:112} INFO - [2022-03-22 17:58:42,484] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:42,517] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:42,527] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:42,780] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:42,869] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:42,890] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.416 seconds
[2022-03-22 17:58:46,470] {scheduler_job.py:153} INFO - Started process (PID=1608) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:46,475] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:46,476] {logging_mixin.py:112} INFO - [2022-03-22 17:58:46,476] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:46,515] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:46,523] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:46,732] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:46,816] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:46,842] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.373 seconds
[2022-03-22 17:58:50,477] {scheduler_job.py:153} INFO - Started process (PID=1610) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:50,483] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:50,484] {logging_mixin.py:112} INFO - [2022-03-22 17:58:50,484] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:50,517] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:50,525] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:50,728] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:50,808] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:50,835] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.358 seconds
[2022-03-22 17:58:54,487] {scheduler_job.py:153} INFO - Started process (PID=1613) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:54,492] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:54,494] {logging_mixin.py:112} INFO - [2022-03-22 17:58:54,494] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:54,546] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:54,554] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:54,807] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:54,899] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:54,921] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.434 seconds
[2022-03-22 17:58:58,477] {scheduler_job.py:153} INFO - Started process (PID=1615) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:58,483] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:58:58,486] {logging_mixin.py:112} INFO - [2022-03-22 17:58:58,485] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:58,530] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:58:58,540] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:58:58,751] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:58:58,834] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:58:58,858] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.382 seconds
[2022-03-22 17:59:02,491] {scheduler_job.py:153} INFO - Started process (PID=1617) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:02,498] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:02,500] {logging_mixin.py:112} INFO - [2022-03-22 17:59:02,500] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:02,532] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:02,540] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:02,762] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:02,828] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:02,854] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.371 seconds
[2022-03-22 17:59:06,494] {scheduler_job.py:153} INFO - Started process (PID=1619) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:06,501] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:06,503] {logging_mixin.py:112} INFO - [2022-03-22 17:59:06,503] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:06,534] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:06,542] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:06,807] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:06,886] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:06,909] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.415 seconds
[2022-03-22 17:59:10,497] {scheduler_job.py:153} INFO - Started process (PID=1621) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:10,503] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:10,504] {logging_mixin.py:112} INFO - [2022-03-22 17:59:10,504] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:10,538] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:10,547] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:10,768] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:10,851] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:10,876] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.379 seconds
[2022-03-22 17:59:14,489] {scheduler_job.py:153} INFO - Started process (PID=1623) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:14,493] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:14,495] {logging_mixin.py:112} INFO - [2022-03-22 17:59:14,495] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:14,525] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:14,533] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:14,730] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:14,812] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:14,894] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.406 seconds
[2022-03-22 17:59:18,522] {scheduler_job.py:153} INFO - Started process (PID=1625) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:18,529] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:18,531] {logging_mixin.py:112} INFO - [2022-03-22 17:59:18,530] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:18,577] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:18,587] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:18,824] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:18,916] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:18,941] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.419 seconds
[2022-03-22 17:59:22,505] {scheduler_job.py:153} INFO - Started process (PID=1627) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:22,511] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:22,512] {logging_mixin.py:112} INFO - [2022-03-22 17:59:22,512] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:22,542] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:22,551] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:22,740] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:22,819] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:22,842] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.337 seconds
[2022-03-22 17:59:26,523] {scheduler_job.py:153} INFO - Started process (PID=1630) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:26,532] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:26,534] {logging_mixin.py:112} INFO - [2022-03-22 17:59:26,534] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:26,593] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:26,605] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:26,918] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:27,036] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:27,068] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.545 seconds
[2022-03-22 17:59:30,529] {scheduler_job.py:153} INFO - Started process (PID=1632) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:30,537] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:30,539] {logging_mixin.py:112} INFO - [2022-03-22 17:59:30,538] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:30,582] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:30,591] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:30,844] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:30,927] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:30,953] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.424 seconds
[2022-03-22 17:59:34,529] {scheduler_job.py:153} INFO - Started process (PID=1634) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:34,538] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:34,540] {logging_mixin.py:112} INFO - [2022-03-22 17:59:34,540] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:34,583] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:34,593] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:34,815] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:34,902] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:34,925] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.396 seconds
[2022-03-22 17:59:38,537] {scheduler_job.py:153} INFO - Started process (PID=1636) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:38,547] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:38,549] {logging_mixin.py:112} INFO - [2022-03-22 17:59:38,548] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:38,588] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:38,596] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:38,811] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:38,889] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:38,917] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.380 seconds
[2022-03-22 17:59:42,546] {scheduler_job.py:153} INFO - Started process (PID=1638) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:42,553] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:42,555] {logging_mixin.py:112} INFO - [2022-03-22 17:59:42,555] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:42,587] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:42,595] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:42,821] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:42,905] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:42,933] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.387 seconds
[2022-03-22 17:59:46,540] {scheduler_job.py:153} INFO - Started process (PID=1640) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:46,548] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:46,550] {logging_mixin.py:112} INFO - [2022-03-22 17:59:46,550] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:46,583] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:46,591] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:46,808] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:46,874] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:46,892] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.352 seconds
[2022-03-22 17:59:50,548] {scheduler_job.py:153} INFO - Started process (PID=1642) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:50,555] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:50,557] {logging_mixin.py:112} INFO - [2022-03-22 17:59:50,557] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:50,589] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:50,597] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:50,789] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:50,875] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:50,898] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.350 seconds
[2022-03-22 17:59:54,551] {scheduler_job.py:153} INFO - Started process (PID=1644) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:54,564] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:54,572] {logging_mixin.py:112} INFO - [2022-03-22 17:59:54,572] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:54,604] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:54,612] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:54,900] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:54,993] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:55,013] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.462 seconds
[2022-03-22 17:59:58,558] {scheduler_job.py:153} INFO - Started process (PID=1646) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:58,567] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 17:59:58,569] {logging_mixin.py:112} INFO - [2022-03-22 17:59:58,569] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:58,606] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 17:59:58,616] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 17:59:59,179] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 17:59:59,308] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 17:59:59,338] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.780 seconds
[2022-03-22 18:00:02,559] {scheduler_job.py:153} INFO - Started process (PID=1649) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:02,568] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:00:02,569] {logging_mixin.py:112} INFO - [2022-03-22 18:00:02,569] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:02,608] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:00:02,619] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:02,844] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:00:03,188] {scheduler_job.py:1294} INFO - Created <DagRun bigquery_data_load @ 2022-03-22T17:00:00+00:00: scheduled__2022-03-22T17:00:00+00:00, externally triggered: False>
[2022-03-22 18:00:03,208] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 17:00:00+00:00: scheduled__2022-03-22T17:00:00+00:00, externally triggered: False>
[2022-03-22 18:00:03,300] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:00:03,320] {scheduler_job.py:1640} INFO - Creating / updating <TaskInstance: bigquery_data_load.load_data 2022-03-22 17:00:00+00:00 [scheduled]> in ORM
[2022-03-22 18:00:03,453] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.894 seconds
[2022-03-22 18:00:20,451] {scheduler_job.py:153} INFO - Started process (PID=1657) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:20,457] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:00:20,458] {logging_mixin.py:112} INFO - [2022-03-22 18:00:20,458] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:20,491] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:00:20,500] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:20,734] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:00:20,817] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 17:00:00+00:00: scheduled__2022-03-22T17:00:00+00:00, externally triggered: False>
[2022-03-22 18:00:21,003] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:00:21,025] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.574 seconds
[2022-03-22 18:00:24,386] {scheduler_job.py:153} INFO - Started process (PID=1659) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:24,396] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:00:24,398] {logging_mixin.py:112} INFO - [2022-03-22 18:00:24,397] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:24,431] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:00:24,443] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:24,695] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:00:24,778] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 17:00:00+00:00: scheduled__2022-03-22T17:00:00+00:00, externally triggered: False>
[2022-03-22 18:00:24,821] {logging_mixin.py:112} INFO - [2022-03-22 18:00:24,821] {dagrun.py:309} INFO - Marking run <DagRun bigquery_data_load @ 2022-03-22 17:00:00+00:00: scheduled__2022-03-22T17:00:00+00:00, externally triggered: False> failed
[2022-03-22 18:00:24,920] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:00:24,948] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.562 seconds
[2022-03-22 18:00:28,373] {scheduler_job.py:153} INFO - Started process (PID=1661) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:28,382] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:00:28,383] {logging_mixin.py:112} INFO - [2022-03-22 18:00:28,383] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:28,420] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:00:28,435] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:28,672] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:00:28,754] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:00:28,777] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.404 seconds
[2022-03-22 18:00:32,408] {scheduler_job.py:153} INFO - Started process (PID=1664) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:32,416] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:00:32,418] {logging_mixin.py:112} INFO - [2022-03-22 18:00:32,418] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:32,490] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:00:32,502] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:32,782] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:00:32,904] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:00:32,931] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.524 seconds
[2022-03-22 18:00:36,392] {scheduler_job.py:153} INFO - Started process (PID=1666) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:36,397] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:00:36,401] {logging_mixin.py:112} INFO - [2022-03-22 18:00:36,401] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:36,449] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:00:36,458] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:36,683] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:00:36,760] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:00:36,781] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.390 seconds
[2022-03-22 18:00:40,382] {scheduler_job.py:153} INFO - Started process (PID=1668) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:40,388] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:00:40,390] {logging_mixin.py:112} INFO - [2022-03-22 18:00:40,389] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:40,439] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:00:40,453] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:40,689] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:00:40,777] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:00:40,805] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.423 seconds
[2022-03-22 18:00:44,405] {scheduler_job.py:153} INFO - Started process (PID=1670) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:44,410] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:00:44,412] {logging_mixin.py:112} INFO - [2022-03-22 18:00:44,411] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:44,462] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:00:44,472] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:44,702] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:00:44,943] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:00:44,997] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.592 seconds
[2022-03-22 18:00:48,432] {scheduler_job.py:153} INFO - Started process (PID=1672) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:48,466] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:00:48,470] {logging_mixin.py:112} INFO - [2022-03-22 18:00:48,470] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:48,591] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:00:48,633] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:49,114] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:00:49,299] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:00:49,332] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.900 seconds
[2022-03-22 18:00:52,418] {scheduler_job.py:153} INFO - Started process (PID=1674) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:52,427] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:00:52,430] {logging_mixin.py:112} INFO - [2022-03-22 18:00:52,429] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:52,537] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:00:52,547] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:52,768] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:00:52,887] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:00:52,907] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.489 seconds
[2022-03-22 18:00:56,413] {scheduler_job.py:153} INFO - Started process (PID=1676) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:56,420] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:00:56,422] {logging_mixin.py:112} INFO - [2022-03-22 18:00:56,422] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:56,460] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:00:56,470] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:00:56,765] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:00:56,860] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:00:56,880] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.467 seconds
[2022-03-22 18:01:00,418] {scheduler_job.py:153} INFO - Started process (PID=1678) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:00,428] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:00,431] {logging_mixin.py:112} INFO - [2022-03-22 18:01:00,431] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:00,610] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:00,680] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:01,191] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:01,538] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:01,719] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.302 seconds
[2022-03-22 18:01:04,416] {scheduler_job.py:153} INFO - Started process (PID=1680) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:04,426] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:04,428] {logging_mixin.py:112} INFO - [2022-03-22 18:01:04,428] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:04,517] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:04,529] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:05,080] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:05,371] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:05,524] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.108 seconds
[2022-03-22 18:01:08,525] {scheduler_job.py:153} INFO - Started process (PID=1683) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:08,533] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:08,535] {logging_mixin.py:112} INFO - [2022-03-22 18:01:08,535] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:08,620] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:08,654] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:09,055] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:09,226] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:09,270] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.745 seconds
[2022-03-22 18:01:12,448] {scheduler_job.py:153} INFO - Started process (PID=1685) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:12,464] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:12,466] {logging_mixin.py:112} INFO - [2022-03-22 18:01:12,466] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:12,570] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:12,589] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:12,974] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:13,119] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:13,167] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.718 seconds
[2022-03-22 18:01:16,708] {scheduler_job.py:153} INFO - Started process (PID=1687) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:16,725] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:16,727] {logging_mixin.py:112} INFO - [2022-03-22 18:01:16,726] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:16,792] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:16,826] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:17,498] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:18,127] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:18,182] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.475 seconds
[2022-03-22 18:01:21,022] {scheduler_job.py:153} INFO - Started process (PID=1689) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:21,039] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:21,041] {logging_mixin.py:112} INFO - [2022-03-22 18:01:21,041] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:21,109] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:21,146] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:21,502] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:21,638] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:21,675] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.653 seconds
[2022-03-22 18:01:24,792] {scheduler_job.py:153} INFO - Started process (PID=1691) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:24,808] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:24,812] {logging_mixin.py:112} INFO - [2022-03-22 18:01:24,811] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:24,906] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:24,922] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:25,256] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:25,356] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:25,383] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.591 seconds
[2022-03-22 18:01:28,733] {scheduler_job.py:153} INFO - Started process (PID=1693) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:28,740] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:28,741] {logging_mixin.py:112} INFO - [2022-03-22 18:01:28,741] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:28,779] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:28,790] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:29,078] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:29,157] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:29,193] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.461 seconds
[2022-03-22 18:01:32,756] {scheduler_job.py:153} INFO - Started process (PID=1695) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:32,761] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:32,764] {logging_mixin.py:112} INFO - [2022-03-22 18:01:32,763] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:32,803] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:32,830] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:33,106] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:33,193] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:33,218] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.462 seconds
[2022-03-22 18:01:37,396] {scheduler_job.py:153} INFO - Started process (PID=1698) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:37,403] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:37,405] {logging_mixin.py:112} INFO - [2022-03-22 18:01:37,405] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:37,441] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:37,456] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:37,684] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:37,780] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:37,802] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.407 seconds
[2022-03-22 18:01:41,442] {scheduler_job.py:153} INFO - Started process (PID=1716) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:41,451] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:41,452] {logging_mixin.py:112} INFO - [2022-03-22 18:01:41,452] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:41,504] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:41,517] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:41,906] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:42,240] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:42,283] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.842 seconds
[2022-03-22 18:01:45,426] {scheduler_job.py:153} INFO - Started process (PID=1733) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:45,433] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:45,434] {logging_mixin.py:112} INFO - [2022-03-22 18:01:45,434] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:45,467] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:45,477] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:46,554] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:46,846] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:46,945] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.519 seconds
[2022-03-22 18:01:49,424] {scheduler_job.py:153} INFO - Started process (PID=1735) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:49,436] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:49,438] {logging_mixin.py:112} INFO - [2022-03-22 18:01:49,437] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:49,499] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:49,520] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:49,833] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:49,922] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:49,946] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.522 seconds
[2022-03-22 18:01:53,442] {scheduler_job.py:153} INFO - Started process (PID=1737) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:53,450] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:53,451] {logging_mixin.py:112} INFO - [2022-03-22 18:01:53,451] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:53,484] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:53,494] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:53,789] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:53,885] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:53,907] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.465 seconds
[2022-03-22 18:01:57,436] {scheduler_job.py:153} INFO - Started process (PID=1739) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:57,452] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:01:57,454] {logging_mixin.py:112} INFO - [2022-03-22 18:01:57,454] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:57,562] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:01:57,582] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:01:57,936] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:01:58,023] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:01:58,045] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.610 seconds
[2022-03-22 18:02:01,633] {scheduler_job.py:153} INFO - Started process (PID=1741) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:01,644] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:01,647] {logging_mixin.py:112} INFO - [2022-03-22 18:02:01,646] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:01,694] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:01,708] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:02,145] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:02,333] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:02,371] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.738 seconds
[2022-03-22 18:02:05,442] {scheduler_job.py:153} INFO - Started process (PID=1743) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:05,449] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:05,451] {logging_mixin.py:112} INFO - [2022-03-22 18:02:05,451] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:05,493] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:05,506] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:05,768] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:06,050] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:06,076] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.635 seconds
[2022-03-22 18:02:09,520] {scheduler_job.py:153} INFO - Started process (PID=1745) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:09,535] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:09,555] {logging_mixin.py:112} INFO - [2022-03-22 18:02:09,555] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:09,628] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:09,668] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:10,157] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:10,287] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:10,385] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.865 seconds
[2022-03-22 18:02:13,523] {scheduler_job.py:153} INFO - Started process (PID=1747) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:13,534] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:13,536] {logging_mixin.py:112} INFO - [2022-03-22 18:02:13,536] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:13,630] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:13,650] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:14,222] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:14,451] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:14,483] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.961 seconds
[2022-03-22 18:02:17,614] {scheduler_job.py:153} INFO - Started process (PID=1750) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:17,623] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:17,625] {logging_mixin.py:112} INFO - [2022-03-22 18:02:17,624] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:17,761] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:17,800] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:18,338] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:18,674] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:18,722] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.108 seconds
[2022-03-22 18:02:21,614] {scheduler_job.py:153} INFO - Started process (PID=1752) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:21,625] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:21,628] {logging_mixin.py:112} INFO - [2022-03-22 18:02:21,628] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:21,684] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:21,705] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:21,992] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:22,107] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:22,135] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.521 seconds
[2022-03-22 18:02:25,601] {scheduler_job.py:153} INFO - Started process (PID=1754) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:25,612] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:25,617] {logging_mixin.py:112} INFO - [2022-03-22 18:02:25,617] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:25,725] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:25,741] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:26,108] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:26,245] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:26,277] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.677 seconds
[2022-03-22 18:02:30,285] {scheduler_job.py:153} INFO - Started process (PID=1756) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:30,327] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:30,343] {logging_mixin.py:112} INFO - [2022-03-22 18:02:30,343] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:30,614] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:30,650] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:31,099] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:31,194] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:31,223] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.938 seconds
[2022-03-22 18:02:33,793] {scheduler_job.py:153} INFO - Started process (PID=1758) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:33,804] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:33,806] {logging_mixin.py:112} INFO - [2022-03-22 18:02:33,805] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:33,900] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:33,940] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:34,441] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:34,661] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:34,725] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.932 seconds
[2022-03-22 18:02:37,734] {scheduler_job.py:153} INFO - Started process (PID=1760) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:37,740] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:37,741] {logging_mixin.py:112} INFO - [2022-03-22 18:02:37,741] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:37,789] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:37,798] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:38,000] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:38,079] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:38,100] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.366 seconds
[2022-03-22 18:02:41,747] {scheduler_job.py:153} INFO - Started process (PID=1762) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:41,756] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:41,757] {logging_mixin.py:112} INFO - [2022-03-22 18:02:41,757] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:41,798] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:41,810] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:42,162] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:42,282] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:42,303] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.558 seconds
[2022-03-22 18:02:45,758] {scheduler_job.py:153} INFO - Started process (PID=1764) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:45,766] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:45,768] {logging_mixin.py:112} INFO - [2022-03-22 18:02:45,768] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:45,822] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:45,839] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:46,284] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:46,415] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:46,465] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.707 seconds
[2022-03-22 18:02:49,843] {scheduler_job.py:153} INFO - Started process (PID=1767) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:49,853] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:49,868] {logging_mixin.py:112} INFO - [2022-03-22 18:02:49,867] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:49,942] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:49,975] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:50,382] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:50,635] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:50,666] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.823 seconds
[2022-03-22 18:02:53,761] {scheduler_job.py:153} INFO - Started process (PID=1769) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:53,768] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:53,770] {logging_mixin.py:112} INFO - [2022-03-22 18:02:53,769] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:53,807] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:53,819] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:54,143] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:54,262] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:54,283] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.522 seconds
[2022-03-22 18:02:57,771] {scheduler_job.py:153} INFO - Started process (PID=1771) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:57,777] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:02:57,778] {logging_mixin.py:112} INFO - [2022-03-22 18:02:57,778] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:57,819] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:02:57,829] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:02:58,111] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:02:58,195] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:02:58,218] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.447 seconds
[2022-03-22 18:03:02,312] {scheduler_job.py:153} INFO - Started process (PID=1773) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:02,328] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:03:02,331] {logging_mixin.py:112} INFO - [2022-03-22 18:03:02,330] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:02,388] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:03:02,403] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:02,942] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:03:03,229] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:03:03,276] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.964 seconds
[2022-03-22 18:03:05,880] {scheduler_job.py:153} INFO - Started process (PID=1776) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:05,897] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:03:05,899] {logging_mixin.py:112} INFO - [2022-03-22 18:03:05,898] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:06,110] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:03:06,136] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:06,812] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:03:06,982] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:03:07,040] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.160 seconds
[2022-03-22 18:03:09,814] {scheduler_job.py:153} INFO - Started process (PID=1778) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:09,820] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:03:09,821] {logging_mixin.py:112} INFO - [2022-03-22 18:03:09,821] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:09,858] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:03:09,868] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:10,092] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:03:10,182] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:03:10,205] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.392 seconds
[2022-03-22 18:03:13,823] {scheduler_job.py:153} INFO - Started process (PID=1797) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:13,836] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:03:13,837] {logging_mixin.py:112} INFO - [2022-03-22 18:03:13,837] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:13,886] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:03:13,909] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:14,133] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:03:14,206] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:03:14,234] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.411 seconds
[2022-03-22 18:03:17,819] {scheduler_job.py:153} INFO - Started process (PID=1829) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:17,834] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:03:17,842] {logging_mixin.py:112} INFO - [2022-03-22 18:03:17,841] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:17,935] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:03:17,942] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:03:17,968] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:18,302] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:03:18,425] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:03:18,455] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.636 seconds
[2022-03-22 18:03:21,835] {scheduler_job.py:153} INFO - Started process (PID=1831) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:21,845] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:03:21,850] {logging_mixin.py:112} INFO - [2022-03-22 18:03:21,850] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:21,895] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:03:21,902] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:03:21,912] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:22,164] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:03:22,254] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:03:22,277] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.442 seconds
[2022-03-22 18:03:25,968] {scheduler_job.py:153} INFO - Started process (PID=1834) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:25,984] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:03:25,993] {logging_mixin.py:112} INFO - [2022-03-22 18:03:25,992] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:26,057] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:03:26,066] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:03:26,091] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:26,654] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.686 seconds
[2022-03-22 18:03:29,921] {scheduler_job.py:153} INFO - Started process (PID=1836) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:29,945] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:03:29,946] {logging_mixin.py:112} INFO - [2022-03-22 18:03:29,946] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:30,075] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:03:30,080] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:03:30,096] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:30,554] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:03:30,643] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:03:30,668] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.752 seconds
[2022-03-22 18:03:33,839] {scheduler_job.py:153} INFO - Started process (PID=1838) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:33,852] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:03:33,853] {logging_mixin.py:112} INFO - [2022-03-22 18:03:33,853] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:33,953] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:03:33,959] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:03:33,982] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:34,817] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:03:35,245] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:03:34.079853+00:00: manual__2022-03-22T18:03:34.079853+00:00, externally triggered: True>
[2022-03-22 18:03:35,623] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:03:35,697] {scheduler_job.py:1640} INFO - Creating / updating <TaskInstance: bigquery_data_load.load_data 2022-03-22 18:03:34.079853+00:00 [scheduled]> in ORM
[2022-03-22 18:03:35,936] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 2.097 seconds
[2022-03-22 18:03:39,407] {scheduler_job.py:153} INFO - Started process (PID=1840) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:39,432] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:03:39,439] {logging_mixin.py:112} INFO - [2022-03-22 18:03:39,439] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:39,676] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:03:39,684] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:03:39,716] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:03:42,073] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:03:42,365] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:03:34.079853+00:00: manual__2022-03-22T18:03:34.079853+00:00, externally triggered: True>
[2022-03-22 18:03:42,762] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:03:42,829] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 3.422 seconds
[2022-03-22 18:04:03,577] {scheduler_job.py:153} INFO - Started process (PID=1849) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:03,584] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:03,587] {logging_mixin.py:112} INFO - [2022-03-22 18:04:03,586] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:03,628] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:03,640] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:03,653] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:03,891] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:04,012] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:03:34.079853+00:00: manual__2022-03-22T18:03:34.079853+00:00, externally triggered: True>
[2022-03-22 18:04:04,240] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:04,261] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.684 seconds
[2022-03-22 18:04:07,578] {scheduler_job.py:153} INFO - Started process (PID=1851) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:07,589] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:07,591] {logging_mixin.py:112} INFO - [2022-03-22 18:04:07,590] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:07,634] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:07,640] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:07,653] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:07,919] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:07,998] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:03:34.079853+00:00: manual__2022-03-22T18:03:34.079853+00:00, externally triggered: True>
[2022-03-22 18:04:08,038] {logging_mixin.py:112} INFO - [2022-03-22 18:04:08,038] {dagrun.py:309} INFO - Marking run <DagRun bigquery_data_load @ 2022-03-22 18:03:34.079853+00:00: manual__2022-03-22T18:03:34.079853+00:00, externally triggered: True> failed
[2022-03-22 18:04:08,124] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:08,146] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.568 seconds
[2022-03-22 18:04:11,603] {scheduler_job.py:153} INFO - Started process (PID=1853) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:11,613] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:11,615] {logging_mixin.py:112} INFO - [2022-03-22 18:04:11,615] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:11,654] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:11,659] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:11,670] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:11,897] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:11,987] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:12,008] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.405 seconds
[2022-03-22 18:04:15,640] {scheduler_job.py:153} INFO - Started process (PID=1855) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:15,649] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:15,652] {logging_mixin.py:112} INFO - [2022-03-22 18:04:15,652] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:15,696] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:15,704] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:15,717] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:16,122] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:16,244] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:16,286] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.647 seconds
[2022-03-22 18:04:19,601] {scheduler_job.py:153} INFO - Started process (PID=1857) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:19,607] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:19,608] {logging_mixin.py:112} INFO - [2022-03-22 18:04:19,608] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:19,639] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:19,643] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:19,657] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:19,877] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:19,955] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:19,976] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.375 seconds
[2022-03-22 18:04:23,612] {scheduler_job.py:153} INFO - Started process (PID=1859) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:23,623] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:23,624] {logging_mixin.py:112} INFO - [2022-03-22 18:04:23,624] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:23,662] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:23,667] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:23,678] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:23,900] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:23,979] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:24,003] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.392 seconds
[2022-03-22 18:04:27,612] {scheduler_job.py:153} INFO - Started process (PID=1861) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:27,619] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:27,621] {logging_mixin.py:112} INFO - [2022-03-22 18:04:27,620] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:27,653] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:27,658] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:27,667] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:27,875] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:27,966] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:27,989] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.377 seconds
[2022-03-22 18:04:31,620] {scheduler_job.py:153} INFO - Started process (PID=1864) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:31,628] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:31,630] {logging_mixin.py:112} INFO - [2022-03-22 18:04:31,630] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:31,703] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:31,710] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:31,723] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:32,062] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:32,188] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:32,221] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.601 seconds
[2022-03-22 18:04:35,628] {scheduler_job.py:153} INFO - Started process (PID=1866) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:35,640] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:35,642] {logging_mixin.py:112} INFO - [2022-03-22 18:04:35,642] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:35,674] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:35,681] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:35,690] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:35,939] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:36,060] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:36,089] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.461 seconds
[2022-03-22 18:04:39,629] {scheduler_job.py:153} INFO - Started process (PID=1868) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:39,634] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:39,636] {logging_mixin.py:112} INFO - [2022-03-22 18:04:39,636] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:39,661] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:39,666] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:39,674] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:39,870] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:39,950] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:39,971] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.342 seconds
[2022-03-22 18:04:43,636] {scheduler_job.py:153} INFO - Started process (PID=1870) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:43,641] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:43,642] {logging_mixin.py:112} INFO - [2022-03-22 18:04:43,642] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:43,671] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:43,676] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:43,685] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:43,993] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:44,115] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:44,144] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.509 seconds
[2022-03-22 18:04:47,660] {scheduler_job.py:153} INFO - Started process (PID=1872) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:47,671] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:47,673] {logging_mixin.py:112} INFO - [2022-03-22 18:04:47,673] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:47,708] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:47,714] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:47,723] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:47,995] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:48,086] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:48,108] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.448 seconds
[2022-03-22 18:04:51,621] {scheduler_job.py:153} INFO - Started process (PID=1874) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:51,626] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:51,628] {logging_mixin.py:112} INFO - [2022-03-22 18:04:51,628] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:51,669] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:51,676] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:51,688] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:51,922] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:52,004] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:52,040] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.419 seconds
[2022-03-22 18:04:55,685] {scheduler_job.py:153} INFO - Started process (PID=1876) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:55,699] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:55,706] {logging_mixin.py:112} INFO - [2022-03-22 18:04:55,706] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:55,768] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:55,774] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:55,791] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:56,203] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:04:56,313] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:04:56,336] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.651 seconds
[2022-03-22 18:04:59,745] {scheduler_job.py:153} INFO - Started process (PID=1878) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:59,759] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:04:59,767] {logging_mixin.py:112} INFO - [2022-03-22 18:04:59,767] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:04:59,826] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:04:59,833] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:04:59,844] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:00,274] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:00,368] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:00,390] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.645 seconds
[2022-03-22 18:05:03,660] {scheduler_job.py:153} INFO - Started process (PID=1881) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:03,669] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:03,671] {logging_mixin.py:112} INFO - [2022-03-22 18:05:03,670] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:03,723] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:03,733] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:03,749] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:04,057] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:04,183] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:04,221] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.561 seconds
[2022-03-22 18:05:07,659] {scheduler_job.py:153} INFO - Started process (PID=1883) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:07,666] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:07,668] {logging_mixin.py:112} INFO - [2022-03-22 18:05:07,668] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:07,705] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:07,711] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:07,722] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:07,968] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:08,050] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:08,073] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.414 seconds
[2022-03-22 18:05:11,677] {scheduler_job.py:153} INFO - Started process (PID=1885) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:11,685] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:11,686] {logging_mixin.py:112} INFO - [2022-03-22 18:05:11,686] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:11,724] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:11,731] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:11,740] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:12,084] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:12,169] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:12,192] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.515 seconds
[2022-03-22 18:05:15,690] {scheduler_job.py:153} INFO - Started process (PID=1887) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:15,703] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:15,704] {logging_mixin.py:112} INFO - [2022-03-22 18:05:15,704] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:15,761] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:15,768] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:15,784] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:16,545] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:16,773] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:16,819] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.130 seconds
[2022-03-22 18:05:27,264] {scheduler_job.py:153} INFO - Started process (PID=1889) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:27,271] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:27,272] {logging_mixin.py:112} INFO - [2022-03-22 18:05:27,272] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:27,310] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:27,317] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:27,332] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:27,733] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:28,132] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:28,160] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.896 seconds
[2022-03-22 18:05:30,315] {scheduler_job.py:153} INFO - Started process (PID=1891) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:30,322] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:30,331] {logging_mixin.py:112} INFO - [2022-03-22 18:05:30,331] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:30,374] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:30,380] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:30,400] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:30,850] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:30,980] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:31,015] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.700 seconds
[2022-03-22 18:05:34,316] {scheduler_job.py:153} INFO - Started process (PID=1893) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:34,323] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:34,324] {logging_mixin.py:112} INFO - [2022-03-22 18:05:34,324] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:34,351] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:34,357] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:34,365] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:34,647] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:34,770] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:34,803] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.487 seconds
[2022-03-22 18:05:38,335] {scheduler_job.py:153} INFO - Started process (PID=1896) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:38,342] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:38,343] {logging_mixin.py:112} INFO - [2022-03-22 18:05:38,343] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:38,374] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:38,378] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:38,388] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:38,594] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:38,676] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:38,698] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.363 seconds
[2022-03-22 18:05:42,328] {scheduler_job.py:153} INFO - Started process (PID=1898) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:42,336] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:42,338] {logging_mixin.py:112} INFO - [2022-03-22 18:05:42,338] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:42,371] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:42,375] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:42,387] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:42,611] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:42,689] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:42,709] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.381 seconds
[2022-03-22 18:05:46,326] {scheduler_job.py:153} INFO - Started process (PID=1900) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:46,332] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:46,334] {logging_mixin.py:112} INFO - [2022-03-22 18:05:46,334] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:46,365] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:46,368] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:46,376] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:46,573] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:46,659] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:46,692] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.366 seconds
[2022-03-22 18:05:50,353] {scheduler_job.py:153} INFO - Started process (PID=1902) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:50,358] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:50,360] {logging_mixin.py:112} INFO - [2022-03-22 18:05:50,359] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:50,399] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:50,404] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:50,414] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:50,677] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:50,759] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:50,782] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.428 seconds
[2022-03-22 18:05:54,339] {scheduler_job.py:153} INFO - Started process (PID=1904) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:54,352] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:54,353] {logging_mixin.py:112} INFO - [2022-03-22 18:05:54,353] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:54,386] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:54,391] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:54,402] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:54,742] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:54,824] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:54,853] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.515 seconds
[2022-03-22 18:05:58,345] {scheduler_job.py:153} INFO - Started process (PID=1906) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:58,351] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:05:58,353] {logging_mixin.py:112} INFO - [2022-03-22 18:05:58,352] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:58,387] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:05:58,392] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:05:58,401] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:05:58,593] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:05:58,667] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:05:58,686] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.341 seconds
[2022-03-22 18:06:02,363] {scheduler_job.py:153} INFO - Started process (PID=1908) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:02,369] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:02,370] {logging_mixin.py:112} INFO - [2022-03-22 18:06:02,370] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:02,397] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:02,401] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:02,409] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:02,610] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:02,688] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:02,709] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.346 seconds
[2022-03-22 18:06:06,436] {scheduler_job.py:153} INFO - Started process (PID=1910) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:06,454] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:06,456] {logging_mixin.py:112} INFO - [2022-03-22 18:06:06,456] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:06,511] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:06,517] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:06,529] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:06,838] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:06,953] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:06,990] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.555 seconds
[2022-03-22 18:06:10,337] {scheduler_job.py:153} INFO - Started process (PID=1913) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:10,346] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:10,349] {logging_mixin.py:112} INFO - [2022-03-22 18:06:10,349] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:10,382] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:10,387] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:10,396] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:10,628] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:10,707] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:10,730] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.393 seconds
[2022-03-22 18:06:14,469] {scheduler_job.py:153} INFO - Started process (PID=1915) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:14,497] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:14,499] {logging_mixin.py:112} INFO - [2022-03-22 18:06:14,499] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:14,546] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:14,552] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:14,566] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:14,976] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:15,084] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:15,134] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.665 seconds
[2022-03-22 18:06:18,364] {scheduler_job.py:153} INFO - Started process (PID=1917) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:18,381] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:18,383] {logging_mixin.py:112} INFO - [2022-03-22 18:06:18,383] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:18,427] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:18,432] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:18,452] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:18,945] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:19,079] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:19,106] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.743 seconds
[2022-03-22 18:06:22,367] {scheduler_job.py:153} INFO - Started process (PID=1919) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:22,375] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:22,376] {logging_mixin.py:112} INFO - [2022-03-22 18:06:22,376] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:22,406] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:22,410] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:22,422] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:22,625] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:22,705] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:22,725] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.358 seconds
[2022-03-22 18:06:26,396] {scheduler_job.py:153} INFO - Started process (PID=1921) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:26,404] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:26,406] {logging_mixin.py:112} INFO - [2022-03-22 18:06:26,405] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:26,445] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:26,451] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:26,460] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:26,785] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:26,899] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:26,930] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.534 seconds
[2022-03-22 18:06:30,384] {scheduler_job.py:153} INFO - Started process (PID=1923) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:30,392] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:30,393] {logging_mixin.py:112} INFO - [2022-03-22 18:06:30,393] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:30,426] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:30,432] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:30,440] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:30,674] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:30,818] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:30,856] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.473 seconds
[2022-03-22 18:06:34,363] {scheduler_job.py:153} INFO - Started process (PID=1925) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:34,370] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:34,371] {logging_mixin.py:112} INFO - [2022-03-22 18:06:34,371] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:34,410] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:34,416] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:34,426] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:34,700] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:34,805] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:34,827] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.464 seconds
[2022-03-22 18:06:38,415] {scheduler_job.py:153} INFO - Started process (PID=1927) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:38,422] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:38,424] {logging_mixin.py:112} INFO - [2022-03-22 18:06:38,424] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:38,455] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:38,459] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:38,469] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:38,689] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:38,774] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:38,795] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.381 seconds
[2022-03-22 18:06:42,420] {scheduler_job.py:153} INFO - Started process (PID=1930) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:42,463] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:42,486] {logging_mixin.py:112} INFO - [2022-03-22 18:06:42,485] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:42,565] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:42,573] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:42,593] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:43,042] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:43,196] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:43,242] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.823 seconds
[2022-03-22 18:06:46,378] {scheduler_job.py:153} INFO - Started process (PID=1932) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:46,384] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:46,385] {logging_mixin.py:112} INFO - [2022-03-22 18:06:46,385] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:46,412] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:46,417] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:46,424] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:46,691] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:46,835] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:46,862] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.484 seconds
[2022-03-22 18:06:50,455] {scheduler_job.py:153} INFO - Started process (PID=1934) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:50,462] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:50,464] {logging_mixin.py:112} INFO - [2022-03-22 18:06:50,463] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:50,508] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:50,513] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:50,523] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:50,760] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:50,840] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:50,861] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.407 seconds
[2022-03-22 18:06:54,417] {scheduler_job.py:153} INFO - Started process (PID=1936) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:54,424] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:54,425] {logging_mixin.py:112} INFO - [2022-03-22 18:06:54,425] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:54,453] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:54,457] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:54,465] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:54,734] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:54,824] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:54,846] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.428 seconds
[2022-03-22 18:06:58,413] {scheduler_job.py:153} INFO - Started process (PID=1938) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:58,419] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:06:58,421] {logging_mixin.py:112} INFO - [2022-03-22 18:06:58,420] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:58,456] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:06:58,461] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:06:58,470] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:06:58,748] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:06:58,993] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:06:59,030] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.617 seconds
[2022-03-22 18:07:02,441] {scheduler_job.py:153} INFO - Started process (PID=1940) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:02,447] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:07:02,448] {logging_mixin.py:112} INFO - [2022-03-22 18:07:02,448] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:02,474] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:07:02,478] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:07:02,486] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:02,682] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:07:02,763] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:07:02,785] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.344 seconds
[2022-03-22 18:07:06,428] {scheduler_job.py:153} INFO - Started process (PID=1942) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:06,439] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:07:06,440] {logging_mixin.py:112} INFO - [2022-03-22 18:07:06,440] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:06,476] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:07:06,482] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:07:06,491] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:06,714] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:07:06,800] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:07:06,822] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.395 seconds
[2022-03-22 18:07:10,417] {scheduler_job.py:153} INFO - Started process (PID=1944) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:10,424] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:07:10,425] {logging_mixin.py:112} INFO - [2022-03-22 18:07:10,425] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:10,462] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:07:10,468] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:07:10,494] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:10,767] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:07:10,876] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:07:10,904] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.487 seconds
[2022-03-22 18:07:14,505] {scheduler_job.py:153} INFO - Started process (PID=1947) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:14,516] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:07:14,518] {logging_mixin.py:112} INFO - [2022-03-22 18:07:14,518] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:14,577] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:07:14,585] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:07:14,612] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:15,052] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:07:15,221] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:07:15,278] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.773 seconds
[2022-03-22 18:07:18,438] {scheduler_job.py:153} INFO - Started process (PID=1949) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:18,447] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:07:18,449] {logging_mixin.py:112} INFO - [2022-03-22 18:07:18,449] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:18,479] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:07:18,484] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:07:18,493] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:18,730] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:07:18,821] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:07:18,857] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.420 seconds
[2022-03-22 18:07:22,446] {scheduler_job.py:153} INFO - Started process (PID=1951) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:22,452] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:07:22,453] {logging_mixin.py:112} INFO - [2022-03-22 18:07:22,453] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:22,482] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:07:22,486] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:07:22,494] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:22,709] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:07:22,803] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:07:22,838] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.392 seconds
[2022-03-22 18:07:26,521] {scheduler_job.py:153} INFO - Started process (PID=1953) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:26,535] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:07:26,537] {logging_mixin.py:112} INFO - [2022-03-22 18:07:26,537] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:26,594] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:07:26,601] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:07:26,625] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:26,970] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:07:27,099] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:07:27,125] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.605 seconds
[2022-03-22 18:07:30,472] {scheduler_job.py:153} INFO - Started process (PID=1955) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:30,485] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:07:30,488] {logging_mixin.py:112} INFO - [2022-03-22 18:07:30,487] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:30,562] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:07:30,569] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:07:30,579] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:30,879] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:07:30,963] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:07:30,987] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.515 seconds
[2022-03-22 18:07:34,505] {scheduler_job.py:153} INFO - Started process (PID=1957) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:34,513] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:07:34,520] {logging_mixin.py:112} INFO - [2022-03-22 18:07:34,519] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:34,607] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:07:34,614] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:07:34,639] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:34,913] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:07:35,009] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:07:35,037] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.532 seconds
[2022-03-22 18:07:38,525] {scheduler_job.py:153} INFO - Started process (PID=1959) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:38,533] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:07:38,535] {logging_mixin.py:112} INFO - [2022-03-22 18:07:38,535] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:38,582] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:07:38,587] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:07:38,602] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:39,245] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:07:39,396] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:07:38.645671+00:00: manual__2022-03-22T18:07:38.645671+00:00, externally triggered: True>
[2022-03-22 18:07:39,874] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:07:39,936] {scheduler_job.py:1640} INFO - Creating / updating <TaskInstance: bigquery_data_load.load_data 2022-03-22 18:07:38.645671+00:00 [scheduled]> in ORM
[2022-03-22 18:07:40,294] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.769 seconds
[2022-03-22 18:07:57,985] {scheduler_job.py:153} INFO - Started process (PID=1968) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:57,990] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:07:57,992] {logging_mixin.py:112} INFO - [2022-03-22 18:07:57,991] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:58,029] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:07:58,034] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:07:58,044] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:07:58,257] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:07:58,346] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:07:38.645671+00:00: manual__2022-03-22T18:07:38.645671+00:00, externally triggered: True>
[2022-03-22 18:07:58,582] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:07:58,610] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.625 seconds
[2022-03-22 18:08:01,967] {scheduler_job.py:153} INFO - Started process (PID=1970) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:01,972] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:01,974] {logging_mixin.py:112} INFO - [2022-03-22 18:08:01,974] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:02,001] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:02,005] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:02,014] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:02,242] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:02,315] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:07:38.645671+00:00: manual__2022-03-22T18:07:38.645671+00:00, externally triggered: True>
[2022-03-22 18:08:02,352] {logging_mixin.py:112} INFO - [2022-03-22 18:08:02,352] {dagrun.py:309} INFO - Marking run <DagRun bigquery_data_load @ 2022-03-22 18:07:38.645671+00:00: manual__2022-03-22T18:07:38.645671+00:00, externally triggered: True> failed
[2022-03-22 18:08:02,434] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:02,452] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.485 seconds
[2022-03-22 18:08:05,971] {scheduler_job.py:153} INFO - Started process (PID=1972) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:05,977] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:05,979] {logging_mixin.py:112} INFO - [2022-03-22 18:08:05,979] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:06,070] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:06,075] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:06,089] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:06,339] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:06,431] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:06,456] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.485 seconds
[2022-03-22 18:08:10,003] {scheduler_job.py:153} INFO - Started process (PID=1974) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:10,009] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:10,011] {logging_mixin.py:112} INFO - [2022-03-22 18:08:10,010] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:10,041] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:10,046] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:10,056] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:10,324] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:10,407] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:10,429] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.426 seconds
[2022-03-22 18:08:13,988] {scheduler_job.py:153} INFO - Started process (PID=1976) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:14,000] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:14,002] {logging_mixin.py:112} INFO - [2022-03-22 18:08:14,001] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:14,059] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:14,065] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:14,078] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:14,320] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:14,406] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:14,428] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.440 seconds
[2022-03-22 18:08:17,978] {scheduler_job.py:153} INFO - Started process (PID=1978) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:17,987] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:17,989] {logging_mixin.py:112} INFO - [2022-03-22 18:08:17,988] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:18,022] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:18,026] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:18,036] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:18,268] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:18,353] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:18,376] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.398 seconds
[2022-03-22 18:08:22,009] {scheduler_job.py:153} INFO - Started process (PID=1981) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:22,015] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:22,016] {logging_mixin.py:112} INFO - [2022-03-22 18:08:22,016] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:22,072] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:22,077] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:22,093] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:22,458] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:22,674] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:22,720] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.711 seconds
[2022-03-22 18:08:26,726] {scheduler_job.py:153} INFO - Started process (PID=1983) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:26,777] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:26,788] {logging_mixin.py:112} INFO - [2022-03-22 18:08:26,784] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:26,925] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:26,932] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:26,973] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:27,285] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:27,370] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:27,400] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.674 seconds
[2022-03-22 18:08:30,141] {scheduler_job.py:153} INFO - Started process (PID=1985) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:30,148] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:30,150] {logging_mixin.py:112} INFO - [2022-03-22 18:08:30,149] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:30,182] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:30,186] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:30,196] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:30,622] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:30,696] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:30,715] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.574 seconds
[2022-03-22 18:08:34,152] {scheduler_job.py:153} INFO - Started process (PID=1987) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:34,157] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:34,158] {logging_mixin.py:112} INFO - [2022-03-22 18:08:34,158] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:34,187] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:34,191] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:34,201] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:34,424] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:34,513] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:34,536] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.385 seconds
[2022-03-22 18:08:38,169] {scheduler_job.py:153} INFO - Started process (PID=1989) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:38,177] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:38,178] {logging_mixin.py:112} INFO - [2022-03-22 18:08:38,178] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:38,218] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:38,223] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:38,232] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:38,519] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:38,610] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:38,638] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.469 seconds
[2022-03-22 18:08:42,161] {scheduler_job.py:153} INFO - Started process (PID=1991) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:42,166] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:42,168] {logging_mixin.py:112} INFO - [2022-03-22 18:08:42,167] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:42,210] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:42,216] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:42,227] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:42,466] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:42,550] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:42,581] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.420 seconds
[2022-03-22 18:08:46,166] {scheduler_job.py:153} INFO - Started process (PID=1993) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:46,173] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:46,175] {logging_mixin.py:112} INFO - [2022-03-22 18:08:46,175] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:46,206] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:46,210] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:46,221] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:46,510] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:46,626] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:46,658] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.492 seconds
[2022-03-22 18:08:50,170] {scheduler_job.py:153} INFO - Started process (PID=1995) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:50,180] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:50,185] {logging_mixin.py:112} INFO - [2022-03-22 18:08:50,185] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:50,232] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:50,239] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:50,249] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:50,669] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:50,788] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:50,817] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.647 seconds
[2022-03-22 18:08:54,178] {scheduler_job.py:153} INFO - Started process (PID=1998) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:54,186] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:54,188] {logging_mixin.py:112} INFO - [2022-03-22 18:08:54,188] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:54,456] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:54,466] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:54,484] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:54,852] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:54,960] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:54,988] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.810 seconds
[2022-03-22 18:08:58,510] {scheduler_job.py:153} INFO - Started process (PID=2000) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:58,519] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:08:58,521] {logging_mixin.py:112} INFO - [2022-03-22 18:08:58,521] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:58,558] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:08:58,564] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:08:58,576] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:08:58,883] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:08:58,983] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:08:59,012] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.502 seconds
[2022-03-22 18:09:02,524] {scheduler_job.py:153} INFO - Started process (PID=2002) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:02,536] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:02,538] {logging_mixin.py:112} INFO - [2022-03-22 18:09:02,537] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:02,589] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:02,594] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:02,611] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:02,888] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:02,992] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:03,016] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.493 seconds
[2022-03-22 18:09:06,548] {scheduler_job.py:153} INFO - Started process (PID=2004) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:06,554] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:06,555] {logging_mixin.py:112} INFO - [2022-03-22 18:09:06,555] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:06,586] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:06,590] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:06,600] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:06,852] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:06,953] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:06,975] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.427 seconds
[2022-03-22 18:09:10,522] {scheduler_job.py:153} INFO - Started process (PID=2006) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:10,528] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:10,533] {logging_mixin.py:112} INFO - [2022-03-22 18:09:10,532] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:10,592] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:10,612] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:10,623] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:10,946] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:11,059] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:11,087] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.565 seconds
[2022-03-22 18:09:14,528] {scheduler_job.py:153} INFO - Started process (PID=2008) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:14,536] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:14,538] {logging_mixin.py:112} INFO - [2022-03-22 18:09:14,538] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:14,576] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:14,581] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:14,592] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:14,869] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:14,978] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:15,002] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.474 seconds
[2022-03-22 18:09:18,567] {scheduler_job.py:153} INFO - Started process (PID=2010) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:18,575] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:18,576] {logging_mixin.py:112} INFO - [2022-03-22 18:09:18,576] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:18,625] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:18,630] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:18,644] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:18,959] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:19,062] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:19,097] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.530 seconds
[2022-03-22 18:09:22,522] {scheduler_job.py:153} INFO - Started process (PID=2012) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:22,527] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:22,529] {logging_mixin.py:112} INFO - [2022-03-22 18:09:22,528] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:22,557] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:22,561] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:22,570] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:22,789] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:22,879] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:22,902] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.380 seconds
[2022-03-22 18:09:26,559] {scheduler_job.py:153} INFO - Started process (PID=2015) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:26,577] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:26,578] {logging_mixin.py:112} INFO - [2022-03-22 18:09:26,578] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:26,675] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:26,684] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:26,709] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:27,044] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:27,246] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:27,337] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.778 seconds
[2022-03-22 18:09:30,573] {scheduler_job.py:153} INFO - Started process (PID=2017) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:30,578] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:30,580] {logging_mixin.py:112} INFO - [2022-03-22 18:09:30,580] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:30,617] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:30,621] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:30,634] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:30,849] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:30,927] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:30,951] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.378 seconds
[2022-03-22 18:09:34,556] {scheduler_job.py:153} INFO - Started process (PID=2019) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:34,575] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:34,578] {logging_mixin.py:112} INFO - [2022-03-22 18:09:34,577] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:34,680] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:34,690] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:34,706] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:35,154] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:35,338] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:35,380] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.823 seconds
[2022-03-22 18:09:38,565] {scheduler_job.py:153} INFO - Started process (PID=2021) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:38,575] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:38,627] {logging_mixin.py:112} INFO - [2022-03-22 18:09:38,627] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:38,809] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:38,817] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:38,836] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:39,095] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:39,201] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:39,487] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.922 seconds
[2022-03-22 18:09:42,579] {scheduler_job.py:153} INFO - Started process (PID=2023) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:42,587] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:42,589] {logging_mixin.py:112} INFO - [2022-03-22 18:09:42,589] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:42,628] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:42,634] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:42,645] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:42,906] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:43,005] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:43,050] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.471 seconds
[2022-03-22 18:09:46,567] {scheduler_job.py:153} INFO - Started process (PID=2025) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:46,574] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:46,576] {logging_mixin.py:112} INFO - [2022-03-22 18:09:46,576] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:46,611] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:46,617] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:46,628] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:46,896] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:47,010] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:47,039] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.472 seconds
[2022-03-22 18:09:50,582] {scheduler_job.py:153} INFO - Started process (PID=2027) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:50,595] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:50,600] {logging_mixin.py:112} INFO - [2022-03-22 18:09:50,600] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:50,654] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:50,660] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:50,675] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:51,042] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:51,173] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:51,219] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.636 seconds
[2022-03-22 18:09:54,960] {scheduler_job.py:153} INFO - Started process (PID=2029) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:54,977] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:54,992] {logging_mixin.py:112} INFO - [2022-03-22 18:09:54,991] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:55,112] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:55,122] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:55,138] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:55,473] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:55,580] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:55,609] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.650 seconds
[2022-03-22 18:09:58,585] {scheduler_job.py:153} INFO - Started process (PID=2031) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:58,602] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:09:58,605] {logging_mixin.py:112} INFO - [2022-03-22 18:09:58,605] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:58,828] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:09:58,836] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:09:58,854] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:09:59,442] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:09:59,641] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:09:59,691] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.107 seconds
[2022-03-22 18:10:02,664] {scheduler_job.py:153} INFO - Started process (PID=2033) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:02,689] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:02,691] {logging_mixin.py:112} INFO - [2022-03-22 18:10:02,691] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:02,756] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:02,762] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:02,781] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:03,935] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:04,305] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:04,393] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.729 seconds
[2022-03-22 18:10:06,691] {scheduler_job.py:153} INFO - Started process (PID=2035) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:06,702] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:06,707] {logging_mixin.py:112} INFO - [2022-03-22 18:10:06,706] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:06,806] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:06,812] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:06,830] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:07,137] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:07,587] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:07,622] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.931 seconds
[2022-03-22 18:10:10,607] {scheduler_job.py:153} INFO - Started process (PID=2037) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:10,618] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:10,620] {logging_mixin.py:112} INFO - [2022-03-22 18:10:10,620] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:10,661] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:10,671] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:10,689] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:11,008] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:11,149] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:11,180] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.573 seconds
[2022-03-22 18:10:14,615] {scheduler_job.py:153} INFO - Started process (PID=2039) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:14,632] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:14,634] {logging_mixin.py:112} INFO - [2022-03-22 18:10:14,634] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:14,689] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:14,694] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:14,713] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:15,064] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:15,190] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:15,216] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.602 seconds
[2022-03-22 18:10:18,645] {scheduler_job.py:153} INFO - Started process (PID=2041) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:18,654] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:18,655] {logging_mixin.py:112} INFO - [2022-03-22 18:10:18,655] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:18,691] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:18,696] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:18,710] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:18,946] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:19,036] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:19,059] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.414 seconds
[2022-03-22 18:10:22,595] {scheduler_job.py:153} INFO - Started process (PID=2043) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:22,602] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:22,603] {logging_mixin.py:112} INFO - [2022-03-22 18:10:22,603] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:22,643] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:22,650] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:22,663] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:22,961] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:23,054] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:23,080] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.486 seconds
[2022-03-22 18:10:26,643] {scheduler_job.py:153} INFO - Started process (PID=2045) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:26,656] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:26,658] {logging_mixin.py:112} INFO - [2022-03-22 18:10:26,658] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:26,702] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:26,707] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:26,720] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:27,032] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:27,120] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:27,145] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.502 seconds
[2022-03-22 18:10:30,614] {scheduler_job.py:153} INFO - Started process (PID=2047) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:30,621] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:30,622] {logging_mixin.py:112} INFO - [2022-03-22 18:10:30,622] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:30,659] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:30,663] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:30,675] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:30,946] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:31,071] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:31,096] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.482 seconds
[2022-03-22 18:10:34,676] {scheduler_job.py:153} INFO - Started process (PID=2049) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:34,733] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:34,736] {logging_mixin.py:112} INFO - [2022-03-22 18:10:34,735] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:34,801] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:34,808] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:34,821] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:35,302] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:35,402] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:35,448] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.773 seconds
[2022-03-22 18:10:38,640] {scheduler_job.py:153} INFO - Started process (PID=2051) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:38,656] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:38,658] {logging_mixin.py:112} INFO - [2022-03-22 18:10:38,658] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:38,755] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:38,761] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:38,777] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:39,246] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:39,357] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:39,394] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.753 seconds
[2022-03-22 18:10:42,677] {scheduler_job.py:153} INFO - Started process (PID=2053) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:42,684] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:42,686] {logging_mixin.py:112} INFO - [2022-03-22 18:10:42,686] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:42,717] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:42,722] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:42,732] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:42,951] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:43,034] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:43,057] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.380 seconds
[2022-03-22 18:10:46,690] {scheduler_job.py:153} INFO - Started process (PID=2055) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:46,695] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:46,698] {logging_mixin.py:112} INFO - [2022-03-22 18:10:46,696] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:46,735] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:46,741] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:46,751] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:47,034] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:47,115] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:47,137] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.447 seconds
[2022-03-22 18:10:50,738] {scheduler_job.py:153} INFO - Started process (PID=2057) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:50,749] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:50,750] {logging_mixin.py:112} INFO - [2022-03-22 18:10:50,750] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:50,788] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:50,794] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:50,806] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:51,019] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:51,110] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:51,135] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.398 seconds
[2022-03-22 18:10:54,707] {scheduler_job.py:153} INFO - Started process (PID=2059) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:54,714] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:54,715] {logging_mixin.py:112} INFO - [2022-03-22 18:10:54,715] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:54,748] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:54,753] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:54,764] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:54,988] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:55,142] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:55,191] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.484 seconds
[2022-03-22 18:10:58,703] {scheduler_job.py:153} INFO - Started process (PID=2061) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:58,709] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:10:58,711] {logging_mixin.py:112} INFO - [2022-03-22 18:10:58,711] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:58,742] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:10:58,748] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:10:58,758] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:10:59,052] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:10:59,165] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:10:59,188] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.485 seconds
[2022-03-22 18:11:02,724] {scheduler_job.py:153} INFO - Started process (PID=2063) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:02,738] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:02,740] {logging_mixin.py:112} INFO - [2022-03-22 18:11:02,740] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:02,786] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:02,793] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:02,805] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:03,087] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:03,188] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:03,215] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.492 seconds
[2022-03-22 18:11:06,716] {scheduler_job.py:153} INFO - Started process (PID=2065) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:06,722] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:06,724] {logging_mixin.py:112} INFO - [2022-03-22 18:11:06,724] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:06,758] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:06,764] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:06,773] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:07,008] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:07,107] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:07,129] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.413 seconds
[2022-03-22 18:11:10,717] {scheduler_job.py:153} INFO - Started process (PID=2067) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:10,725] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:10,726] {logging_mixin.py:112} INFO - [2022-03-22 18:11:10,726] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:10,763] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:10,772] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:10,782] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:11,054] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:11,178] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:11,209] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.492 seconds
[2022-03-22 18:11:14,766] {scheduler_job.py:153} INFO - Started process (PID=2069) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:14,779] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:14,782] {logging_mixin.py:112} INFO - [2022-03-22 18:11:14,781] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:14,813] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:14,820] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:14,829] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:15,127] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:15,216] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:15,237] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.472 seconds
[2022-03-22 18:11:18,736] {scheduler_job.py:153} INFO - Started process (PID=2071) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:18,743] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:18,745] {logging_mixin.py:112} INFO - [2022-03-22 18:11:18,745] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:18,798] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:18,807] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:18,829] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:19,303] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:19,445] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:19,479] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.743 seconds
[2022-03-22 18:11:22,833] {scheduler_job.py:153} INFO - Started process (PID=2073) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:22,855] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:22,860] {logging_mixin.py:112} INFO - [2022-03-22 18:11:22,859] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:22,921] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:22,944] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:22,987] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:23,407] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:23,539] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:23,570] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.737 seconds
[2022-03-22 18:11:26,747] {scheduler_job.py:153} INFO - Started process (PID=2075) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:26,758] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:26,759] {logging_mixin.py:112} INFO - [2022-03-22 18:11:26,759] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:26,791] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:26,799] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:26,809] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:27,155] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:27,233] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:27,255] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.507 seconds
[2022-03-22 18:11:30,768] {scheduler_job.py:153} INFO - Started process (PID=2077) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:30,780] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:30,784] {logging_mixin.py:112} INFO - [2022-03-22 18:11:30,784] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:30,847] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:30,853] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:30,863] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:31,073] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:31,154] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:31,174] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.406 seconds
[2022-03-22 18:11:34,756] {scheduler_job.py:153} INFO - Started process (PID=2079) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:34,769] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:34,772] {logging_mixin.py:112} INFO - [2022-03-22 18:11:34,771] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:34,810] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:34,819] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:34,833] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:35,176] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:35,258] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:35,279] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.524 seconds
[2022-03-22 18:11:38,803] {scheduler_job.py:153} INFO - Started process (PID=2082) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:38,811] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:38,814] {logging_mixin.py:112} INFO - [2022-03-22 18:11:38,813] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:38,871] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:38,882] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:38,897] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:39,202] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:39,294] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:39,318] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.516 seconds
[2022-03-22 18:11:42,770] {scheduler_job.py:153} INFO - Started process (PID=2084) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:42,776] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:42,778] {logging_mixin.py:112} INFO - [2022-03-22 18:11:42,778] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:42,815] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:42,821] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:42,831] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:43,123] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:43,241] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:43,271] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.501 seconds
[2022-03-22 18:11:46,752] {scheduler_job.py:153} INFO - Started process (PID=2086) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:46,757] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:46,759] {logging_mixin.py:112} INFO - [2022-03-22 18:11:46,759] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:46,789] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:46,794] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:46,804] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:47,009] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:47,088] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:47,113] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.361 seconds
[2022-03-22 18:11:50,790] {scheduler_job.py:153} INFO - Started process (PID=2088) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:50,795] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:50,798] {logging_mixin.py:112} INFO - [2022-03-22 18:11:50,798] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:50,832] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:50,837] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:50,845] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:51,073] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:51,154] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:51,178] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.389 seconds
[2022-03-22 18:11:54,778] {scheduler_job.py:153} INFO - Started process (PID=2090) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:54,788] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:54,790] {logging_mixin.py:112} INFO - [2022-03-22 18:11:54,790] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:54,837] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:54,842] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:54,851] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:55,068] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:55,137] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:55,159] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.381 seconds
[2022-03-22 18:11:58,782] {scheduler_job.py:153} INFO - Started process (PID=2092) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:58,787] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:11:58,789] {logging_mixin.py:112} INFO - [2022-03-22 18:11:58,789] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:58,835] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:11:58,841] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:11:58,851] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:11:59,098] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:11:59,180] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:11:59,204] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.422 seconds
[2022-03-22 18:12:02,792] {scheduler_job.py:153} INFO - Started process (PID=2094) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:02,799] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:02,800] {logging_mixin.py:112} INFO - [2022-03-22 18:12:02,800] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:02,832] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:02,838] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:02,850] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:03,095] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:03,166] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:03,186] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.395 seconds
[2022-03-22 18:12:06,809] {scheduler_job.py:153} INFO - Started process (PID=2096) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:06,815] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:06,816] {logging_mixin.py:112} INFO - [2022-03-22 18:12:06,816] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:06,853] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:06,858] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:06,870] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:07,095] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:07,175] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:07,198] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.389 seconds
[2022-03-22 18:12:10,819] {scheduler_job.py:153} INFO - Started process (PID=2099) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:10,837] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:10,840] {logging_mixin.py:112} INFO - [2022-03-22 18:12:10,839] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:11,909] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:11,942] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:12,100] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:14,160] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:15,861] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:16,951] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 6.132 seconds
[2022-03-22 18:12:19,307] {scheduler_job.py:153} INFO - Started process (PID=2101) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:19,325] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:19,344] {logging_mixin.py:112} INFO - [2022-03-22 18:12:19,344] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:19,594] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:19,602] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:19,647] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:20,292] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:20,492] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:20,552] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.245 seconds
[2022-03-22 18:12:21,026] {scheduler_job.py:153} INFO - Started process (PID=2103) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:21,038] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:21,039] {logging_mixin.py:112} INFO - [2022-03-22 18:12:21,039] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:21,076] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:21,081] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:21,091] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:21,310] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:21,489] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:21,544] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.517 seconds
[2022-03-22 18:12:24,264] {scheduler_job.py:153} INFO - Started process (PID=2105) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:24,276] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:24,278] {logging_mixin.py:112} INFO - [2022-03-22 18:12:24,277] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:24,334] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:24,339] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:24,352] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:24,792] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:25,081] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:25,144] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.881 seconds
[2022-03-22 18:12:29,292] {scheduler_job.py:153} INFO - Started process (PID=2107) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:29,308] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:29,346] {logging_mixin.py:112} INFO - [2022-03-22 18:12:29,345] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:31,041] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:31,049] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:31,079] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:32,058] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:32,480] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:32,524] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 3.233 seconds
[2022-03-22 18:12:34,443] {scheduler_job.py:153} INFO - Started process (PID=2109) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:34,470] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:34,472] {logging_mixin.py:112} INFO - [2022-03-22 18:12:34,472] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:34,673] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:34,734] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:34,814] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:35,379] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:35,529] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:35,551] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.108 seconds
[2022-03-22 18:12:38,074] {scheduler_job.py:153} INFO - Started process (PID=2111) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:38,079] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:38,081] {logging_mixin.py:112} INFO - [2022-03-22 18:12:38,081] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:38,108] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:38,112] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:38,120] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:38,330] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:38,506] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:38,529] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.455 seconds
[2022-03-22 18:12:42,099] {scheduler_job.py:153} INFO - Started process (PID=2113) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:42,127] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:42,129] {logging_mixin.py:112} INFO - [2022-03-22 18:12:42,128] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:42,171] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:42,176] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:42,186] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:42,540] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:42,694] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:42,740] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.641 seconds
[2022-03-22 18:12:46,097] {scheduler_job.py:153} INFO - Started process (PID=2116) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:46,110] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:46,111] {logging_mixin.py:112} INFO - [2022-03-22 18:12:46,111] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:46,141] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:46,145] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:46,159] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:46,371] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:46,450] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:46,471] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.374 seconds
[2022-03-22 18:12:50,103] {scheduler_job.py:153} INFO - Started process (PID=2118) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:50,112] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:50,115] {logging_mixin.py:112} INFO - [2022-03-22 18:12:50,115] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:50,144] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:50,154] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:50,163] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:50,375] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:50,454] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:50,476] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.373 seconds
[2022-03-22 18:12:54,113] {scheduler_job.py:153} INFO - Started process (PID=2120) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:54,120] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:54,121] {logging_mixin.py:112} INFO - [2022-03-22 18:12:54,121] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:54,152] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:54,156] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:54,166] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:54,424] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:54,507] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:54,533] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.421 seconds
[2022-03-22 18:12:58,103] {scheduler_job.py:153} INFO - Started process (PID=2122) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:58,110] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:12:58,112] {logging_mixin.py:112} INFO - [2022-03-22 18:12:58,111] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:58,145] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:12:58,151] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:12:58,159] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:12:58,379] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:12:58,527] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:12:58,572] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.469 seconds
[2022-03-22 18:13:02,117] {scheduler_job.py:153} INFO - Started process (PID=2124) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:02,125] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:02,126] {logging_mixin.py:112} INFO - [2022-03-22 18:13:02,126] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:02,158] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:02,162] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:02,170] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:02,450] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:02,533] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:02,555] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.438 seconds
[2022-03-22 18:13:06,170] {scheduler_job.py:153} INFO - Started process (PID=2126) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:06,198] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:06,203] {logging_mixin.py:112} INFO - [2022-03-22 18:13:06,202] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:06,271] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:06,284] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:06,308] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:06,557] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:06,624] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:06,641] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.472 seconds
[2022-03-22 18:13:10,114] {scheduler_job.py:153} INFO - Started process (PID=2128) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:10,123] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:10,125] {logging_mixin.py:112} INFO - [2022-03-22 18:13:10,125] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:10,159] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:10,164] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:10,171] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:10,387] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:10,479] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:10,509] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.395 seconds
[2022-03-22 18:13:14,130] {scheduler_job.py:153} INFO - Started process (PID=2130) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:14,137] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:14,139] {logging_mixin.py:112} INFO - [2022-03-22 18:13:14,138] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:14,165] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:14,168] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:14,176] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:14,400] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:14,482] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:14,508] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.378 seconds
[2022-03-22 18:13:18,146] {scheduler_job.py:153} INFO - Started process (PID=2132) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:18,153] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:18,155] {logging_mixin.py:112} INFO - [2022-03-22 18:13:18,155] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:18,204] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:18,209] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:18,218] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:18,430] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:18,513] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:18,535] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.389 seconds
[2022-03-22 18:13:22,131] {scheduler_job.py:153} INFO - Started process (PID=2134) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:22,142] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:22,146] {logging_mixin.py:112} INFO - [2022-03-22 18:13:22,146] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:22,226] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:22,232] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:22,248] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:22,604] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:22,948] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:22,973] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.842 seconds
[2022-03-22 18:13:26,143] {scheduler_job.py:153} INFO - Started process (PID=2136) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:26,153] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:26,154] {logging_mixin.py:112} INFO - [2022-03-22 18:13:26,154] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:26,184] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:26,188] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:26,196] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:26,404] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:26,486] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:26,508] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.366 seconds
[2022-03-22 18:13:30,144] {scheduler_job.py:153} INFO - Started process (PID=2138) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:30,150] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:30,151] {logging_mixin.py:112} INFO - [2022-03-22 18:13:30,151] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:31,007] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:31,018] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:31,037] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:31,557] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:31,722] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:31,745] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.601 seconds
[2022-03-22 18:13:34,277] {scheduler_job.py:153} INFO - Started process (PID=2140) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:34,287] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:34,292] {logging_mixin.py:112} INFO - [2022-03-22 18:13:34,291] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:34,377] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:34,383] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:34,405] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:34,996] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:35,168] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:35,191] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.914 seconds
[2022-03-22 18:13:38,285] {scheduler_job.py:153} INFO - Started process (PID=2143) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:38,292] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:38,294] {logging_mixin.py:112} INFO - [2022-03-22 18:13:38,294] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:38,350] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:38,356] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:38,369] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:38,669] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:38,749] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:38,771] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.486 seconds
[2022-03-22 18:13:43,092] {scheduler_job.py:153} INFO - Started process (PID=2145) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:43,128] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:43,130] {logging_mixin.py:112} INFO - [2022-03-22 18:13:43,130] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:43,491] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:43,524] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:43,635] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:44,840] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:45,199] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:45,234] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 2.142 seconds
[2022-03-22 18:13:47,121] {scheduler_job.py:153} INFO - Started process (PID=2153) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:47,142] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:47,153] {logging_mixin.py:112} INFO - [2022-03-22 18:13:47,153] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:47,245] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:47,253] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:47,272] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:47,561] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:47,690] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:47,725] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.604 seconds
[2022-03-22 18:13:51,125] {scheduler_job.py:153} INFO - Started process (PID=2156) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:51,133] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:51,134] {logging_mixin.py:112} INFO - [2022-03-22 18:13:51,134] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:51,180] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:51,185] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:51,203] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:51,578] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:51,726] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:51,770] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.645 seconds
[2022-03-22 18:13:55,185] {scheduler_job.py:153} INFO - Started process (PID=2158) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:55,200] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:55,216] {logging_mixin.py:112} INFO - [2022-03-22 18:13:55,216] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:55,345] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:55,352] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:55,390] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:55,987] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:56,387] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:56,460] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.275 seconds
[2022-03-22 18:13:59,121] {scheduler_job.py:153} INFO - Started process (PID=2160) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:59,128] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:13:59,130] {logging_mixin.py:112} INFO - [2022-03-22 18:13:59,130] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:59,162] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:13:59,167] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:13:59,177] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:13:59,411] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:13:59,498] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:13:59,519] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.398 seconds
[2022-03-22 18:14:03,127] {scheduler_job.py:153} INFO - Started process (PID=2162) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:03,140] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:03,142] {logging_mixin.py:112} INFO - [2022-03-22 18:14:03,141] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:03,203] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:03,208] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:03,218] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:03,523] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:14:03,669] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:14:03,715] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.589 seconds
[2022-03-22 18:14:07,150] {scheduler_job.py:153} INFO - Started process (PID=2164) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:07,163] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:07,165] {logging_mixin.py:112} INFO - [2022-03-22 18:14:07,165] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:07,217] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:07,223] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:07,254] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:07,855] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:14:07,938] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:14:07,963] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.813 seconds
[2022-03-22 18:14:11,425] {scheduler_job.py:153} INFO - Started process (PID=2167) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:11,470] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:11,473] {logging_mixin.py:112} INFO - [2022-03-22 18:14:11,472] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:11,545] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:11,563] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:11,597] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:12,069] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:14:12,168] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:14:12,192] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.768 seconds
[2022-03-22 18:14:15,130] {scheduler_job.py:153} INFO - Started process (PID=2169) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:15,139] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:15,141] {logging_mixin.py:112} INFO - [2022-03-22 18:14:15,141] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:15,191] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:15,198] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:15,222] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:15,720] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:14:15,909] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:14:15,959] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.829 seconds
[2022-03-22 18:14:19,147] {scheduler_job.py:153} INFO - Started process (PID=2171) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:19,155] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:19,158] {logging_mixin.py:112} INFO - [2022-03-22 18:14:19,157] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:19,186] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:19,191] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:19,201] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:19,482] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:14:19,567] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:14:19,589] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.442 seconds
[2022-03-22 18:14:23,176] {scheduler_job.py:153} INFO - Started process (PID=2173) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:23,181] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:23,187] {logging_mixin.py:112} INFO - [2022-03-22 18:14:23,187] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:23,223] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:23,228] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:23,240] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:23,480] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:14:23,555] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:14:23,578] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.403 seconds
[2022-03-22 18:14:27,165] {scheduler_job.py:153} INFO - Started process (PID=2175) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:27,178] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:27,179] {logging_mixin.py:112} INFO - [2022-03-22 18:14:27,179] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:27,218] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:27,223] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:27,235] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:27,545] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:14:27,625] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:14:27,648] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.483 seconds
[2022-03-22 18:14:31,167] {scheduler_job.py:153} INFO - Started process (PID=2177) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:31,177] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:31,179] {logging_mixin.py:112} INFO - [2022-03-22 18:14:31,178] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:31,255] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:31,261] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:31,287] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:31,803] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:14:31,929] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:14:31,966] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.799 seconds
[2022-03-22 18:14:35,220] {scheduler_job.py:153} INFO - Started process (PID=2179) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:35,229] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:35,230] {logging_mixin.py:112} INFO - [2022-03-22 18:14:35,230] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:35,296] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:35,307] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:35,327] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:35,623] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:14:35,718] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:14:35,743] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.523 seconds
[2022-03-22 18:14:39,158] {scheduler_job.py:153} INFO - Started process (PID=2181) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:39,164] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:39,166] {logging_mixin.py:112} INFO - [2022-03-22 18:14:39,166] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:39,195] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:39,200] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:39,209] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:39,396] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.238 seconds
[2022-03-22 18:14:43,176] {scheduler_job.py:153} INFO - Started process (PID=2184) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:43,186] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:43,189] {logging_mixin.py:112} INFO - [2022-03-22 18:14:43,189] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:43,231] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:43,238] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:43,258] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:43,444] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.269 seconds
[2022-03-22 18:14:47,206] {scheduler_job.py:153} INFO - Started process (PID=2186) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:47,212] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:47,214] {logging_mixin.py:112} INFO - [2022-03-22 18:14:47,214] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:47,249] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:47,254] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:47,267] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:47,456] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.250 seconds
[2022-03-22 18:14:51,174] {scheduler_job.py:153} INFO - Started process (PID=2188) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:51,189] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:51,191] {logging_mixin.py:112} INFO - [2022-03-22 18:14:51,190] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:51,238] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:51,243] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:51,257] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:51,451] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.277 seconds
[2022-03-22 18:14:55,214] {scheduler_job.py:153} INFO - Started process (PID=2190) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:55,225] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:55,227] {logging_mixin.py:112} INFO - [2022-03-22 18:14:55,226] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:55,274] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:55,282] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:55,296] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:55,736] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.523 seconds
[2022-03-22 18:14:59,303] {scheduler_job.py:153} INFO - Started process (PID=2192) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:59,329] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:14:59,331] {logging_mixin.py:112} INFO - [2022-03-22 18:14:59,330] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:59,387] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:14:59,393] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:14:59,426] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:14:59,704] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.401 seconds
[2022-03-22 18:15:03,196] {scheduler_job.py:153} INFO - Started process (PID=2194) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:03,202] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:03,203] {logging_mixin.py:112} INFO - [2022-03-22 18:15:03,203] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:03,231] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:03,235] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:03,250] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:03,412] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.217 seconds
[2022-03-22 18:15:07,349] {scheduler_job.py:153} INFO - Started process (PID=2196) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:07,382] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:07,392] {logging_mixin.py:112} INFO - [2022-03-22 18:15:07,392] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:07,614] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:07,621] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:07,653] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:08,283] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.934 seconds
[2022-03-22 18:15:15,798] {scheduler_job.py:153} INFO - Started process (PID=2199) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:15,807] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:15,809] {logging_mixin.py:112} INFO - [2022-03-22 18:15:15,809] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:15,853] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:15,859] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:15,876] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:16,156] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.357 seconds
[2022-03-22 18:15:16,774] {scheduler_job.py:153} INFO - Started process (PID=2201) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:16,782] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:16,787] {logging_mixin.py:112} INFO - [2022-03-22 18:15:16,787] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:16,835] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:16,841] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:16,854] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:17,032] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.258 seconds
[2022-03-22 18:15:20,782] {scheduler_job.py:153} INFO - Started process (PID=2203) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:20,789] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:20,791] {logging_mixin.py:112} INFO - [2022-03-22 18:15:20,790] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:20,826] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:20,831] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:20,842] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:21,039] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.257 seconds
[2022-03-22 18:15:24,781] {scheduler_job.py:153} INFO - Started process (PID=2205) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:24,787] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:24,788] {logging_mixin.py:112} INFO - [2022-03-22 18:15:24,788] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:24,822] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:24,827] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:24,838] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:24,996] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.215 seconds
[2022-03-22 18:15:28,896] {scheduler_job.py:153} INFO - Started process (PID=2207) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:28,901] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:28,902] {logging_mixin.py:112} INFO - [2022-03-22 18:15:28,902] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:28,928] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:28,932] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:28,941] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:29,090] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.195 seconds
[2022-03-22 18:15:32,800] {scheduler_job.py:153} INFO - Started process (PID=2209) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:32,806] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:32,808] {logging_mixin.py:112} INFO - [2022-03-22 18:15:32,808] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:32,835] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:32,839] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:32,852] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:33,020] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.220 seconds
[2022-03-22 18:15:36,782] {scheduler_job.py:153} INFO - Started process (PID=2211) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:36,789] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:36,791] {logging_mixin.py:112} INFO - [2022-03-22 18:15:36,791] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:36,821] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:36,826] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:36,834] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:36,988] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.206 seconds
[2022-03-22 18:15:40,799] {scheduler_job.py:153} INFO - Started process (PID=2213) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:40,805] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:40,807] {logging_mixin.py:112} INFO - [2022-03-22 18:15:40,807] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:40,847] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:40,851] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:40,862] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:41,037] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.239 seconds
[2022-03-22 18:15:44,807] {scheduler_job.py:153} INFO - Started process (PID=2215) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:44,813] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:44,815] {logging_mixin.py:112} INFO - [2022-03-22 18:15:44,814] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:44,855] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:44,859] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:44,869] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:45,045] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.237 seconds
[2022-03-22 18:15:48,823] {scheduler_job.py:153} INFO - Started process (PID=2218) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:48,839] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:48,845] {logging_mixin.py:112} INFO - [2022-03-22 18:15:48,845] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:48,882] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:48,894] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:48,908] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:49,116] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.293 seconds
[2022-03-22 18:15:52,814] {scheduler_job.py:153} INFO - Started process (PID=2220) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:52,821] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:52,822] {logging_mixin.py:112} INFO - [2022-03-22 18:15:52,822] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:52,853] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:52,857] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:52,868] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:53,210] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.396 seconds
[2022-03-22 18:15:56,819] {scheduler_job.py:153} INFO - Started process (PID=2222) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:56,828] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:15:56,829] {logging_mixin.py:112} INFO - [2022-03-22 18:15:56,829] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:56,858] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:15:56,863] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:15:56,874] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:15:57,043] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.225 seconds
[2022-03-22 18:16:00,850] {scheduler_job.py:153} INFO - Started process (PID=2224) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:00,866] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:00,869] {logging_mixin.py:112} INFO - [2022-03-22 18:16:00,868] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:00,917] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:00,924] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:00,940] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:01,301] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.451 seconds
[2022-03-22 18:16:05,089] {scheduler_job.py:153} INFO - Started process (PID=2226) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:05,114] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:05,126] {logging_mixin.py:112} INFO - [2022-03-22 18:16:05,126] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:05,434] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:05,440] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:05,472] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:05,820] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.731 seconds
[2022-03-22 18:16:08,843] {scheduler_job.py:153} INFO - Started process (PID=2228) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:08,868] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:08,870] {logging_mixin.py:112} INFO - [2022-03-22 18:16:08,869] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:08,929] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:08,939] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:08,962] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:09,161] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.318 seconds
[2022-03-22 18:16:12,832] {scheduler_job.py:153} INFO - Started process (PID=2230) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:12,839] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:12,840] {logging_mixin.py:112} INFO - [2022-03-22 18:16:12,840] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:12,871] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:12,875] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:12,886] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:13,066] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.234 seconds
[2022-03-22 18:16:16,839] {scheduler_job.py:153} INFO - Started process (PID=2232) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:16,849] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:16,851] {logging_mixin.py:112} INFO - [2022-03-22 18:16:16,851] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:16,899] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:16,905] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:16,919] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:17,145] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.306 seconds
[2022-03-22 18:16:20,907] {scheduler_job.py:153} INFO - Started process (PID=2234) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:20,920] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:20,921] {logging_mixin.py:112} INFO - [2022-03-22 18:16:20,921] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:20,953] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:20,959] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:20,971] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:21,159] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.253 seconds
[2022-03-22 18:16:24,869] {scheduler_job.py:153} INFO - Started process (PID=2237) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:24,877] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:24,878] {logging_mixin.py:112} INFO - [2022-03-22 18:16:24,878] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:24,926] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:24,935] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:24,946] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:25,110] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.241 seconds
[2022-03-22 18:16:28,874] {scheduler_job.py:153} INFO - Started process (PID=2239) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:28,882] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:28,883] {logging_mixin.py:112} INFO - [2022-03-22 18:16:28,883] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:28,910] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:28,914] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:28,922] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:29,076] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.202 seconds
[2022-03-22 18:16:32,894] {scheduler_job.py:153} INFO - Started process (PID=2241) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:32,901] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:32,905] {logging_mixin.py:112} INFO - [2022-03-22 18:16:32,905] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:32,944] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:32,949] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:32,972] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:33,156] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.263 seconds
[2022-03-22 18:16:36,871] {scheduler_job.py:153} INFO - Started process (PID=2243) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:36,877] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:36,878] {logging_mixin.py:112} INFO - [2022-03-22 18:16:36,878] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:36,914] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:36,919] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:36,930] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:37,072] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.201 seconds
[2022-03-22 18:16:40,886] {scheduler_job.py:153} INFO - Started process (PID=2245) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:40,892] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:40,893] {logging_mixin.py:112} INFO - [2022-03-22 18:16:40,893] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:40,946] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:40,952] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:40,969] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:41,220] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.334 seconds
[2022-03-22 18:16:44,894] {scheduler_job.py:153} INFO - Started process (PID=2247) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:44,901] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:44,902] {logging_mixin.py:112} INFO - [2022-03-22 18:16:44,902] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:44,943] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:44,947] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:44,957] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:45,114] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.220 seconds
[2022-03-22 18:16:48,893] {scheduler_job.py:153} INFO - Started process (PID=2250) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:48,900] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:48,903] {logging_mixin.py:112} INFO - [2022-03-22 18:16:48,903] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:48,939] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:48,943] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:48,955] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:49,147] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.254 seconds
[2022-03-22 18:16:52,899] {scheduler_job.py:153} INFO - Started process (PID=2252) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:52,904] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:52,906] {logging_mixin.py:112} INFO - [2022-03-22 18:16:52,906] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:52,938] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:52,942] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:52,952] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:53,132] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.234 seconds
[2022-03-22 18:16:56,915] {scheduler_job.py:153} INFO - Started process (PID=2255) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:56,922] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:16:56,923] {logging_mixin.py:112} INFO - [2022-03-22 18:16:56,923] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:56,958] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:16:56,968] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:16:56,982] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:16:57,156] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.242 seconds
[2022-03-22 18:17:00,970] {scheduler_job.py:153} INFO - Started process (PID=2257) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:00,984] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:00,990] {logging_mixin.py:112} INFO - [2022-03-22 18:17:00,989] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:01,088] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:01,093] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:01,108] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:01,305] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.336 seconds
[2022-03-22 18:17:04,921] {scheduler_job.py:153} INFO - Started process (PID=2259) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:04,928] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:04,929] {logging_mixin.py:112} INFO - [2022-03-22 18:17:04,929] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:04,956] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:04,960] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:04,969] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:05,164] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.243 seconds
[2022-03-22 18:17:08,961] {scheduler_job.py:153} INFO - Started process (PID=2261) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:08,976] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:08,979] {logging_mixin.py:112} INFO - [2022-03-22 18:17:08,978] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:09,009] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:09,016] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:09,024] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:09,273] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.312 seconds
[2022-03-22 18:17:12,952] {scheduler_job.py:153} INFO - Started process (PID=2263) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:12,968] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:12,973] {logging_mixin.py:112} INFO - [2022-03-22 18:17:12,973] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:13,080] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:13,091] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:13,114] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:13,389] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.437 seconds
[2022-03-22 18:17:16,954] {scheduler_job.py:153} INFO - Started process (PID=2265) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:16,966] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:16,974] {logging_mixin.py:112} INFO - [2022-03-22 18:17:16,974] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:17,051] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:17,060] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:17,076] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:17,306] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.353 seconds
[2022-03-22 18:17:20,946] {scheduler_job.py:153} INFO - Started process (PID=2267) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:20,953] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:20,954] {logging_mixin.py:112} INFO - [2022-03-22 18:17:20,954] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:20,984] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:20,989] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:21,001] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:21,160] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.214 seconds
[2022-03-22 18:17:24,955] {scheduler_job.py:153} INFO - Started process (PID=2269) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:24,962] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:24,968] {logging_mixin.py:112} INFO - [2022-03-22 18:17:24,967] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:25,008] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:25,019] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:25,030] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:25,221] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.266 seconds
[2022-03-22 18:17:29,076] {scheduler_job.py:153} INFO - Started process (PID=2272) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:29,092] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:29,094] {logging_mixin.py:112} INFO - [2022-03-22 18:17:29,094] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:29,147] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:29,156] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:29,171] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:29,410] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.334 seconds
[2022-03-22 18:17:32,969] {scheduler_job.py:153} INFO - Started process (PID=2274) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:32,976] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:32,977] {logging_mixin.py:112} INFO - [2022-03-22 18:17:32,977] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:33,006] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:33,011] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:33,019] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:33,199] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.231 seconds
[2022-03-22 18:17:36,969] {scheduler_job.py:153} INFO - Started process (PID=2276) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:36,975] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:36,977] {logging_mixin.py:112} INFO - [2022-03-22 18:17:36,976] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:37,013] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:37,019] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:37,028] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:37,183] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.214 seconds
[2022-03-22 18:17:40,968] {scheduler_job.py:153} INFO - Started process (PID=2278) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:40,974] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:40,975] {logging_mixin.py:112} INFO - [2022-03-22 18:17:40,975] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:41,002] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:41,007] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:41,015] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:41,162] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.193 seconds
[2022-03-22 18:17:44,974] {scheduler_job.py:153} INFO - Started process (PID=2280) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:44,983] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:44,984] {logging_mixin.py:112} INFO - [2022-03-22 18:17:44,984] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:45,023] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:45,028] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:45,040] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:45,408] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.434 seconds
[2022-03-22 18:17:48,996] {scheduler_job.py:153} INFO - Started process (PID=2282) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:49,007] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:49,009] {logging_mixin.py:112} INFO - [2022-03-22 18:17:49,009] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:49,050] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:49,055] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:49,065] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:49,433] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.437 seconds
[2022-03-22 18:17:52,983] {scheduler_job.py:153} INFO - Started process (PID=2284) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:52,988] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:52,990] {logging_mixin.py:112} INFO - [2022-03-22 18:17:52,989] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:53,019] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:53,024] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:53,033] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:53,180] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.197 seconds
[2022-03-22 18:17:56,997] {scheduler_job.py:153} INFO - Started process (PID=2286) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:57,002] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:17:57,004] {logging_mixin.py:112} INFO - [2022-03-22 18:17:57,004] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:57,047] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:17:57,054] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:17:57,067] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:17:57,474] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.478 seconds
[2022-03-22 18:18:00,998] {scheduler_job.py:153} INFO - Started process (PID=2289) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:01,004] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:01,005] {logging_mixin.py:112} INFO - [2022-03-22 18:18:01,005] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:01,036] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:01,040] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:01,054] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:01,401] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.403 seconds
[2022-03-22 18:18:05,011] {scheduler_job.py:153} INFO - Started process (PID=2291) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:05,022] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:05,024] {logging_mixin.py:112} INFO - [2022-03-22 18:18:05,023] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:05,059] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:05,065] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:05,077] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:05,357] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.346 seconds
[2022-03-22 18:18:09,050] {scheduler_job.py:153} INFO - Started process (PID=2293) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:09,077] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:09,085] {logging_mixin.py:112} INFO - [2022-03-22 18:18:09,085] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:09,277] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:09,289] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:09,319] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:09,992] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.941 seconds
[2022-03-22 18:18:13,006] {scheduler_job.py:153} INFO - Started process (PID=2295) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:13,012] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:13,014] {logging_mixin.py:112} INFO - [2022-03-22 18:18:13,014] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:13,046] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:13,052] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:13,061] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:13,257] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.251 seconds
[2022-03-22 18:18:17,054] {scheduler_job.py:153} INFO - Started process (PID=2297) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:17,081] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:17,103] {logging_mixin.py:112} INFO - [2022-03-22 18:18:17,103] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:17,179] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:17,193] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:17,207] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:17,572] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.519 seconds
[2022-03-22 18:18:21,019] {scheduler_job.py:153} INFO - Started process (PID=2299) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:21,035] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:21,036] {logging_mixin.py:112} INFO - [2022-03-22 18:18:21,036] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:21,073] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:21,079] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:21,089] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:21,476] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.457 seconds
[2022-03-22 18:18:25,012] {scheduler_job.py:153} INFO - Started process (PID=2301) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:25,022] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:25,023] {logging_mixin.py:112} INFO - [2022-03-22 18:18:25,023] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:25,054] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:25,060] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:25,069] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:25,342] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.330 seconds
[2022-03-22 18:18:29,017] {scheduler_job.py:153} INFO - Started process (PID=2303) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:29,025] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:29,026] {logging_mixin.py:112} INFO - [2022-03-22 18:18:29,026] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:29,068] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:29,073] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:29,083] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:29,422] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.405 seconds
[2022-03-22 18:18:33,035] {scheduler_job.py:153} INFO - Started process (PID=2306) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:33,047] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:33,049] {logging_mixin.py:112} INFO - [2022-03-22 18:18:33,049] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:33,094] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:33,101] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:33,110] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:33,428] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.393 seconds
[2022-03-22 18:18:37,038] {scheduler_job.py:153} INFO - Started process (PID=2308) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:37,043] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:37,044] {logging_mixin.py:112} INFO - [2022-03-22 18:18:37,044] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:37,072] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:37,076] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:37,084] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:37,264] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.227 seconds
[2022-03-22 18:18:41,033] {scheduler_job.py:153} INFO - Started process (PID=2310) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:41,049] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:41,052] {logging_mixin.py:112} INFO - [2022-03-22 18:18:41,052] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:41,091] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:41,096] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:41,106] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:41,295] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.262 seconds
[2022-03-22 18:18:45,042] {scheduler_job.py:153} INFO - Started process (PID=2312) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:45,048] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:45,050] {logging_mixin.py:112} INFO - [2022-03-22 18:18:45,050] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:45,079] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:45,083] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:45,091] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:45,307] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.265 seconds
[2022-03-22 18:18:49,064] {scheduler_job.py:153} INFO - Started process (PID=2314) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:49,077] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:49,079] {logging_mixin.py:112} INFO - [2022-03-22 18:18:49,079] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:49,121] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:49,126] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:49,136] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:49,302] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.238 seconds
[2022-03-22 18:18:53,049] {scheduler_job.py:153} INFO - Started process (PID=2316) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:53,058] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:53,060] {logging_mixin.py:112} INFO - [2022-03-22 18:18:53,060] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:53,106] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:53,112] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:53,129] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:53,324] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.275 seconds
[2022-03-22 18:18:57,070] {scheduler_job.py:153} INFO - Started process (PID=2318) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:57,082] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:18:57,085] {logging_mixin.py:112} INFO - [2022-03-22 18:18:57,084] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:57,121] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:18:57,125] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:18:57,136] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:18:57,354] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.284 seconds
[2022-03-22 18:19:01,058] {scheduler_job.py:153} INFO - Started process (PID=2320) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:01,065] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:01,067] {logging_mixin.py:112} INFO - [2022-03-22 18:19:01,067] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:01,101] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:01,106] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:01,117] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:01,294] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.236 seconds
[2022-03-22 18:19:05,073] {scheduler_job.py:153} INFO - Started process (PID=2322) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:05,085] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:05,087] {logging_mixin.py:112} INFO - [2022-03-22 18:19:05,087] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:05,124] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:05,129] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:05,144] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:05,380] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.308 seconds
[2022-03-22 18:19:09,208] {scheduler_job.py:153} INFO - Started process (PID=2325) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:09,227] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:09,326] {logging_mixin.py:112} INFO - [2022-03-22 18:19:09,326] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:09,564] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:09,581] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:09,602] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:10,557] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.349 seconds
[2022-03-22 18:19:13,088] {scheduler_job.py:153} INFO - Started process (PID=2327) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:13,100] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:13,106] {logging_mixin.py:112} INFO - [2022-03-22 18:19:13,106] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:13,165] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:13,173] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:13,192] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:13,436] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.349 seconds
[2022-03-22 18:19:17,075] {scheduler_job.py:153} INFO - Started process (PID=2329) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:17,083] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:17,085] {logging_mixin.py:112} INFO - [2022-03-22 18:19:17,084] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:17,111] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:17,115] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:17,125] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:17,437] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:19:17,979] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:19:18,109] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.034 seconds
[2022-03-22 18:19:21,128] {scheduler_job.py:153} INFO - Started process (PID=2331) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:21,143] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:21,146] {logging_mixin.py:112} INFO - [2022-03-22 18:19:21,146] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:21,187] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:21,191] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:21,209] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:21,544] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:19:21,702] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:19:19.341817+00:00: manual__2022-03-22T18:19:19.341817+00:00, externally triggered: True>
[2022-03-22 18:19:21,819] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:19:21,859] {scheduler_job.py:1640} INFO - Creating / updating <TaskInstance: bigquery_data_load.load_data 2022-03-22 18:19:19.341817+00:00 [scheduled]> in ORM
[2022-03-22 18:19:22,125] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.997 seconds
[2022-03-22 18:19:39,827] {scheduler_job.py:153} INFO - Started process (PID=2340) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:39,834] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:39,836] {logging_mixin.py:112} INFO - [2022-03-22 18:19:39,835] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:39,864] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:39,868] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:39,876] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:40,101] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:19:40,183] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:19:19.341817+00:00: manual__2022-03-22T18:19:19.341817+00:00, externally triggered: True>
[2022-03-22 18:19:40,382] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:19:40,401] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.574 seconds
[2022-03-22 18:19:43,815] {scheduler_job.py:153} INFO - Started process (PID=2342) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:43,823] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:43,825] {logging_mixin.py:112} INFO - [2022-03-22 18:19:43,824] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:43,855] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:43,859] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:43,868] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:44,087] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:19:44,157] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:19:19.341817+00:00: manual__2022-03-22T18:19:19.341817+00:00, externally triggered: True>
[2022-03-22 18:19:44,198] {logging_mixin.py:112} INFO - [2022-03-22 18:19:44,198] {dagrun.py:309} INFO - Marking run <DagRun bigquery_data_load @ 2022-03-22 18:19:19.341817+00:00: manual__2022-03-22T18:19:19.341817+00:00, externally triggered: True> failed
[2022-03-22 18:19:44,296] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:19:44,326] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.511 seconds
[2022-03-22 18:19:47,812] {scheduler_job.py:153} INFO - Started process (PID=2344) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:47,820] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:47,823] {logging_mixin.py:112} INFO - [2022-03-22 18:19:47,822] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:47,867] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:47,872] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:47,882] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:48,078] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:19:48,152] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:19:48,176] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.365 seconds
[2022-03-22 18:19:51,861] {scheduler_job.py:153} INFO - Started process (PID=2346) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:51,874] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:51,875] {logging_mixin.py:112} INFO - [2022-03-22 18:19:51,875] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:51,925] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:51,934] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:51,957] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:52,341] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:19:52,443] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:19:52,468] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.607 seconds
[2022-03-22 18:19:55,836] {scheduler_job.py:153} INFO - Started process (PID=2348) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:55,843] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:55,845] {logging_mixin.py:112} INFO - [2022-03-22 18:19:55,844] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:55,877] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:55,886] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:55,895] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:56,129] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:19:56,205] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:19:56,225] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.390 seconds
[2022-03-22 18:19:59,833] {scheduler_job.py:153} INFO - Started process (PID=2350) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:59,840] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:19:59,841] {logging_mixin.py:112} INFO - [2022-03-22 18:19:59,841] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:19:59,867] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:19:59,871] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:19:59,879] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:00,618] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:00,745] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:00,778] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.945 seconds
[2022-03-22 18:20:03,848] {scheduler_job.py:153} INFO - Started process (PID=2352) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:03,855] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:03,856] {logging_mixin.py:112} INFO - [2022-03-22 18:20:03,856] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:03,883] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:03,887] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:03,895] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:04,120] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:04,201] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:04,226] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.378 seconds
[2022-03-22 18:20:07,844] {scheduler_job.py:153} INFO - Started process (PID=2354) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:07,854] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:07,855] {logging_mixin.py:112} INFO - [2022-03-22 18:20:07,855] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:07,971] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:07,975] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:07,984] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:08,201] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:08,288] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:08,312] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.468 seconds
[2022-03-22 18:20:11,842] {scheduler_job.py:153} INFO - Started process (PID=2356) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:11,849] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:11,851] {logging_mixin.py:112} INFO - [2022-03-22 18:20:11,850] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:11,881] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:11,885] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:11,894] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:12,106] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:12,185] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:12,206] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.365 seconds
[2022-03-22 18:20:15,865] {scheduler_job.py:153} INFO - Started process (PID=2358) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:15,871] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:15,872] {logging_mixin.py:112} INFO - [2022-03-22 18:20:15,872] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:15,908] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:15,913] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:15,923] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:16,139] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:16,221] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:16,242] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.378 seconds
[2022-03-22 18:20:19,871] {scheduler_job.py:153} INFO - Started process (PID=2360) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:19,877] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:19,878] {logging_mixin.py:112} INFO - [2022-03-22 18:20:19,878] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:19,924] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:19,928] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:19,940] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:20,222] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:20,307] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:20,329] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.458 seconds
[2022-03-22 18:20:23,855] {scheduler_job.py:153} INFO - Started process (PID=2363) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:23,865] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:23,867] {logging_mixin.py:112} INFO - [2022-03-22 18:20:23,867] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:23,918] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:23,925] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:23,938] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:24,259] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:24,387] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:24,433] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.579 seconds
[2022-03-22 18:20:27,879] {scheduler_job.py:153} INFO - Started process (PID=2365) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:27,886] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:27,888] {logging_mixin.py:112} INFO - [2022-03-22 18:20:27,887] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:27,922] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:27,927] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:27,938] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:28,188] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:28,272] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:28,299] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.420 seconds
[2022-03-22 18:20:31,877] {scheduler_job.py:153} INFO - Started process (PID=2367) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:31,883] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:31,886] {logging_mixin.py:112} INFO - [2022-03-22 18:20:31,886] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:31,936] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:31,941] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:31,953] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:32,193] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:32,276] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:32,302] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.425 seconds
[2022-03-22 18:20:35,876] {scheduler_job.py:153} INFO - Started process (PID=2369) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:35,883] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:35,885] {logging_mixin.py:112} INFO - [2022-03-22 18:20:35,885] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:35,920] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:35,925] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:35,936] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:36,177] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:36,267] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:36,293] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.418 seconds
[2022-03-22 18:20:39,899] {scheduler_job.py:153} INFO - Started process (PID=2371) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:39,905] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:39,906] {logging_mixin.py:112} INFO - [2022-03-22 18:20:39,906] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:39,936] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:39,939] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:39,949] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:40,159] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:40,237] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:40,257] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.358 seconds
[2022-03-22 18:20:43,882] {scheduler_job.py:153} INFO - Started process (PID=2373) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:43,888] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:43,889] {logging_mixin.py:112} INFO - [2022-03-22 18:20:43,889] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:43,931] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:43,936] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:43,945] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:44,155] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:44,228] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:44,247] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.365 seconds
[2022-03-22 18:20:47,890] {scheduler_job.py:153} INFO - Started process (PID=2375) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:47,899] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:47,901] {logging_mixin.py:112} INFO - [2022-03-22 18:20:47,901] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:47,936] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:47,941] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:47,952] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:48,179] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:48,266] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:48,290] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.401 seconds
[2022-03-22 18:20:51,899] {scheduler_job.py:153} INFO - Started process (PID=2377) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:51,904] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:51,905] {logging_mixin.py:112} INFO - [2022-03-22 18:20:51,905] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:51,944] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:51,950] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:51,966] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:52,213] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:52,310] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:52,331] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.432 seconds
[2022-03-22 18:20:55,918] {scheduler_job.py:153} INFO - Started process (PID=2379) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:55,925] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:55,926] {logging_mixin.py:112} INFO - [2022-03-22 18:20:55,926] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:55,974] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:20:55,978] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:20:55,988] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:56,267] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:20:56,360] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:20:56,387] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.471 seconds
[2022-03-22 18:20:59,924] {scheduler_job.py:153} INFO - Started process (PID=2382) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:20:59,936] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:20:59,937] {logging_mixin.py:112} INFO - [2022-03-22 18:20:59,937] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:00,006] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:00,011] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:00,043] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:00,890] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:01,339] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:01,481] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.557 seconds
[2022-03-22 18:21:05,010] {scheduler_job.py:153} INFO - Started process (PID=2384) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:05,034] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:05,037] {logging_mixin.py:112} INFO - [2022-03-22 18:21:05,037] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:05,106] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:05,112] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:05,144] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:06,134] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:06,522] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:06,570] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.560 seconds
[2022-03-22 18:21:10,864] {scheduler_job.py:153} INFO - Started process (PID=2386) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:10,879] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:10,881] {logging_mixin.py:112} INFO - [2022-03-22 18:21:10,881] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:10,930] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:10,937] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:10,967] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:11,253] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:11,334] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:11,357] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.494 seconds
[2022-03-22 18:21:13,788] {scheduler_job.py:153} INFO - Started process (PID=2388) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:13,795] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:13,796] {logging_mixin.py:112} INFO - [2022-03-22 18:21:13,796] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:13,830] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:13,839] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:13,849] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:14,059] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:14,140] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:14,161] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.373 seconds
[2022-03-22 18:21:17,786] {scheduler_job.py:153} INFO - Started process (PID=2390) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:17,793] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:17,795] {logging_mixin.py:112} INFO - [2022-03-22 18:21:17,795] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:17,830] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:17,836] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:17,852] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:18,185] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:18,268] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:18,288] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.501 seconds
[2022-03-22 18:21:21,805] {scheduler_job.py:153} INFO - Started process (PID=2392) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:21,823] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:21,825] {logging_mixin.py:112} INFO - [2022-03-22 18:21:21,824] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:21,887] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:21,893] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:21,927] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:22,252] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:22,328] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:22,350] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.545 seconds
[2022-03-22 18:21:25,799] {scheduler_job.py:153} INFO - Started process (PID=2394) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:25,805] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:25,806] {logging_mixin.py:112} INFO - [2022-03-22 18:21:25,806] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:25,843] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:25,848] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:25,859] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:26,108] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:26,193] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:26,217] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.418 seconds
[2022-03-22 18:21:29,799] {scheduler_job.py:153} INFO - Started process (PID=2396) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:29,805] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:29,807] {logging_mixin.py:112} INFO - [2022-03-22 18:21:29,807] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:29,837] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:29,842] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:29,856] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:30,094] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:30,173] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:30,194] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.395 seconds
[2022-03-22 18:21:33,831] {scheduler_job.py:153} INFO - Started process (PID=2399) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:33,838] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:33,839] {logging_mixin.py:112} INFO - [2022-03-22 18:21:33,839] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:33,880] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:33,887] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:33,897] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:34,130] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:34,228] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:34,254] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.423 seconds
[2022-03-22 18:21:37,807] {scheduler_job.py:153} INFO - Started process (PID=2401) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:37,817] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:37,818] {logging_mixin.py:112} INFO - [2022-03-22 18:21:37,818] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:37,856] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:37,860] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:37,872] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:38,103] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:38,214] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:38,243] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.436 seconds
[2022-03-22 18:21:41,834] {scheduler_job.py:153} INFO - Started process (PID=2403) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:41,841] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:41,843] {logging_mixin.py:112} INFO - [2022-03-22 18:21:41,842] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:41,877] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:41,883] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:41,893] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:42,178] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:42,287] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:42,313] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.479 seconds
[2022-03-22 18:21:45,834] {scheduler_job.py:153} INFO - Started process (PID=2405) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:45,843] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:45,845] {logging_mixin.py:112} INFO - [2022-03-22 18:21:45,845] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:45,879] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:45,884] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:45,894] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:46,259] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:46,369] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:46,400] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.566 seconds
[2022-03-22 18:21:49,822] {scheduler_job.py:153} INFO - Started process (PID=2407) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:49,827] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:49,830] {logging_mixin.py:112} INFO - [2022-03-22 18:21:49,829] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:49,860] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:49,865] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:49,873] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:50,126] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:50,232] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:50,253] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.431 seconds
[2022-03-22 18:21:53,829] {scheduler_job.py:153} INFO - Started process (PID=2409) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:53,835] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:53,837] {logging_mixin.py:112} INFO - [2022-03-22 18:21:53,836] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:53,869] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:53,874] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:53,883] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:54,098] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:54,188] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:54,212] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.383 seconds
[2022-03-22 18:21:57,853] {scheduler_job.py:153} INFO - Started process (PID=2411) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:57,859] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:21:57,860] {logging_mixin.py:112} INFO - [2022-03-22 18:21:57,860] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:57,892] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:21:57,897] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:21:57,907] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:21:58,136] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:21:58,225] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:21:58,249] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.396 seconds
[2022-03-22 18:22:01,858] {scheduler_job.py:153} INFO - Started process (PID=2413) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:01,868] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:22:01,869] {logging_mixin.py:112} INFO - [2022-03-22 18:22:01,869] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:01,921] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:22:01,927] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:22:01,944] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:02,425] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:22:02,571] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:22:02,613] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.755 seconds
[2022-03-22 18:22:05,850] {scheduler_job.py:153} INFO - Started process (PID=2416) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:05,855] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:22:05,857] {logging_mixin.py:112} INFO - [2022-03-22 18:22:05,857] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:05,886] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:22:05,889] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:22:05,900] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:06,194] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:22:06,418] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:22:06,474] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.623 seconds
[2022-03-22 18:22:09,860] {scheduler_job.py:153} INFO - Started process (PID=2418) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:09,867] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:22:09,869] {logging_mixin.py:112} INFO - [2022-03-22 18:22:09,869] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:09,900] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:22:09,905] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:22:09,914] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:10,200] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:22:10,286] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:22:10,308] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.448 seconds
[2022-03-22 18:22:13,864] {scheduler_job.py:153} INFO - Started process (PID=2420) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:13,869] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:22:13,871] {logging_mixin.py:112} INFO - [2022-03-22 18:22:13,870] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:13,904] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:22:13,910] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:22:13,921] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:14,122] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:22:14,193] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:22:14,211] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.347 seconds
[2022-03-22 18:22:17,862] {scheduler_job.py:153} INFO - Started process (PID=2422) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:17,869] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:22:17,870] {logging_mixin.py:112} INFO - [2022-03-22 18:22:17,870] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:17,905] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:22:17,909] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:22:17,920] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:18,229] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:22:18,351] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:22:18,377] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.515 seconds
[2022-03-22 18:22:21,900] {scheduler_job.py:153} INFO - Started process (PID=2424) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:21,927] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:22:21,930] {logging_mixin.py:112} INFO - [2022-03-22 18:22:21,929] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:22,047] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:22:22,056] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:22:22,078] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:22,518] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:22:22,761] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:22:22,846] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.948 seconds
[2022-03-22 18:22:25,866] {scheduler_job.py:153} INFO - Started process (PID=2426) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:25,873] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:22:25,874] {logging_mixin.py:112} INFO - [2022-03-22 18:22:25,874] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:25,904] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:22:25,908] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:22:25,920] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:26,253] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:22:26,365] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:22:26,391] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.525 seconds
[2022-03-22 18:22:30,250] {scheduler_job.py:153} INFO - Started process (PID=2428) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:30,392] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:22:30,408] {logging_mixin.py:112} INFO - [2022-03-22 18:22:30,408] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:30,674] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:22:30,681] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:22:30,715] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:31,487] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:22:31,627] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:22:31,655] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.405 seconds
[2022-03-22 18:22:33,894] {scheduler_job.py:153} INFO - Started process (PID=2430) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:33,904] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:22:33,906] {logging_mixin.py:112} INFO - [2022-03-22 18:22:33,906] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:33,937] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:22:33,941] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:22:33,952] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:34,811] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:22:35,297] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:22:37,203] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 3.309 seconds
[2022-03-22 18:22:57,295] {scheduler_job.py:153} INFO - Started process (PID=2464) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:57,308] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:22:57,310] {logging_mixin.py:112} INFO - [2022-03-22 18:22:57,310] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:57,345] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:22:57,352] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:22:57,365] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:22:57,737] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:22:57,873] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:22:57,905] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.610 seconds
[2022-03-22 18:23:01,211] {scheduler_job.py:153} INFO - Started process (PID=2483) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:01,223] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:01,225] {logging_mixin.py:112} INFO - [2022-03-22 18:23:01,224] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:01,279] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:01,290] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:01,305] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:01,551] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:01,654] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:01,678] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.467 seconds
[2022-03-22 18:23:05,218] {scheduler_job.py:153} INFO - Started process (PID=2487) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:05,225] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:05,227] {logging_mixin.py:112} INFO - [2022-03-22 18:23:05,226] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:05,270] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:05,274] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:05,285] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:05,532] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:05,625] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:05,652] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.434 seconds
[2022-03-22 18:23:09,308] {scheduler_job.py:153} INFO - Started process (PID=2493) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:09,322] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:09,327] {logging_mixin.py:112} INFO - [2022-03-22 18:23:09,327] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:09,384] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:09,393] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:09,438] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:09,738] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:09,869] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:09,899] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.592 seconds
[2022-03-22 18:23:13,228] {scheduler_job.py:153} INFO - Started process (PID=2499) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:13,237] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:13,239] {logging_mixin.py:112} INFO - [2022-03-22 18:23:13,239] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:13,285] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:13,291] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:13,311] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:13,625] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:13,724] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:13,751] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.523 seconds
[2022-03-22 18:23:17,230] {scheduler_job.py:153} INFO - Started process (PID=2503) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:17,236] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:17,237] {logging_mixin.py:112} INFO - [2022-03-22 18:23:17,237] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:17,275] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:17,280] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:17,290] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:17,519] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:17,612] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:17,637] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.407 seconds
[2022-03-22 18:23:21,273] {scheduler_job.py:153} INFO - Started process (PID=2505) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:21,279] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:21,281] {logging_mixin.py:112} INFO - [2022-03-22 18:23:21,281] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:21,313] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:21,319] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:21,337] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:21,599] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:21,684] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:21,710] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.437 seconds
[2022-03-22 18:23:25,246] {scheduler_job.py:153} INFO - Started process (PID=2507) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:25,252] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:25,253] {logging_mixin.py:112} INFO - [2022-03-22 18:23:25,253] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:25,282] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:25,287] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:25,295] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:25,529] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:25,614] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:25,638] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.391 seconds
[2022-03-22 18:23:29,352] {scheduler_job.py:153} INFO - Started process (PID=2509) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:29,368] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:29,370] {logging_mixin.py:112} INFO - [2022-03-22 18:23:29,370] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:29,445] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:29,453] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:29,469] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:29,716] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:29,807] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:29,833] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.482 seconds
[2022-03-22 18:23:33,414] {scheduler_job.py:153} INFO - Started process (PID=2511) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:33,421] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:33,423] {logging_mixin.py:112} INFO - [2022-03-22 18:23:33,422] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:33,497] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:33,505] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:33,543] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:34,008] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:34,130] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:34,164] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.750 seconds
[2022-03-22 18:23:37,638] {scheduler_job.py:153} INFO - Started process (PID=2513) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:37,668] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:37,681] {logging_mixin.py:112} INFO - [2022-03-22 18:23:37,681] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:37,830] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:37,839] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:37,853] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:38,257] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:38,366] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:38,400] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.763 seconds
[2022-03-22 18:23:41,592] {scheduler_job.py:153} INFO - Started process (PID=2515) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:41,608] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:41,610] {logging_mixin.py:112} INFO - [2022-03-22 18:23:41,610] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:41,823] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:41,835] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:41,867] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:42,329] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:42,470] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:42,515] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.923 seconds
[2022-03-22 18:23:45,546] {scheduler_job.py:153} INFO - Started process (PID=2517) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:45,564] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:45,567] {logging_mixin.py:112} INFO - [2022-03-22 18:23:45,567] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:45,629] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:45,638] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:45,655] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:45,960] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:46,050] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:46,074] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.528 seconds
[2022-03-22 18:23:49,505] {scheduler_job.py:153} INFO - Started process (PID=2519) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:49,511] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:49,512] {logging_mixin.py:112} INFO - [2022-03-22 18:23:49,512] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:49,546] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:49,552] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:49,561] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:49,864] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:49,962] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:49,992] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.487 seconds
[2022-03-22 18:23:53,520] {scheduler_job.py:153} INFO - Started process (PID=2521) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:53,525] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:53,527] {logging_mixin.py:112} INFO - [2022-03-22 18:23:53,527] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:53,567] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:53,573] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:53,583] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:53,996] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:54,129] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:54,177] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.656 seconds
[2022-03-22 18:23:57,529] {scheduler_job.py:153} INFO - Started process (PID=2523) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:57,540] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:23:57,541] {logging_mixin.py:112} INFO - [2022-03-22 18:23:57,541] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:57,577] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:23:57,582] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:23:57,590] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:23:57,823] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:23:57,908] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:23:57,930] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.401 seconds
[2022-03-22 18:24:01,529] {scheduler_job.py:153} INFO - Started process (PID=2525) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:01,538] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:01,540] {logging_mixin.py:112} INFO - [2022-03-22 18:24:01,539] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:01,575] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:01,579] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:01,589] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:01,800] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:01,892] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:01,914] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.385 seconds
[2022-03-22 18:24:05,532] {scheduler_job.py:153} INFO - Started process (PID=2527) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:05,538] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:05,540] {logging_mixin.py:112} INFO - [2022-03-22 18:24:05,540] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:05,569] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:05,573] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:05,582] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:05,835] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:05,906] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:05,925] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.393 seconds
[2022-03-22 18:24:09,563] {scheduler_job.py:153} INFO - Started process (PID=2529) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:09,574] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:09,576] {logging_mixin.py:112} INFO - [2022-03-22 18:24:09,576] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:09,619] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:09,626] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:09,641] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:09,913] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:10,008] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:10,031] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.468 seconds
[2022-03-22 18:24:13,541] {scheduler_job.py:153} INFO - Started process (PID=2531) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:13,547] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:13,555] {logging_mixin.py:112} INFO - [2022-03-22 18:24:13,555] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:13,599] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:13,604] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:13,613] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:13,830] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:13,946] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:13,974] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.433 seconds
[2022-03-22 18:24:17,538] {scheduler_job.py:153} INFO - Started process (PID=2533) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:17,544] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:17,545] {logging_mixin.py:112} INFO - [2022-03-22 18:24:17,545] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:17,574] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:17,579] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:17,588] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:18,662] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:18,780] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:18,802] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.265 seconds
[2022-03-22 18:24:21,579] {scheduler_job.py:153} INFO - Started process (PID=2536) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:21,588] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:21,589] {logging_mixin.py:112} INFO - [2022-03-22 18:24:21,589] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:21,624] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:21,629] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:21,639] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:21,974] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:22,061] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:22,084] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.505 seconds
[2022-03-22 18:24:25,560] {scheduler_job.py:153} INFO - Started process (PID=2538) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:25,568] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:25,570] {logging_mixin.py:112} INFO - [2022-03-22 18:24:25,569] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:25,604] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:25,608] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:25,617] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:25,841] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:25,924] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:25,947] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.386 seconds
[2022-03-22 18:24:29,548] {scheduler_job.py:153} INFO - Started process (PID=2540) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:29,553] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:29,555] {logging_mixin.py:112} INFO - [2022-03-22 18:24:29,555] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:29,584] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:29,589] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:29,598] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:29,806] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:29,879] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:29,899] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.351 seconds
[2022-03-22 18:24:33,660] {scheduler_job.py:153} INFO - Started process (PID=2542) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:33,677] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:33,692] {logging_mixin.py:112} INFO - [2022-03-22 18:24:33,691] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:33,847] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:33,855] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:33,872] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:34,114] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:34,419] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:34,495] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.835 seconds
[2022-03-22 18:24:37,568] {scheduler_job.py:153} INFO - Started process (PID=2544) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:37,575] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:37,577] {logging_mixin.py:112} INFO - [2022-03-22 18:24:37,576] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:37,607] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:37,611] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:37,619] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:37,869] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:37,955] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:37,977] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.409 seconds
[2022-03-22 18:24:41,642] {scheduler_job.py:153} INFO - Started process (PID=2546) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:41,678] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:41,690] {logging_mixin.py:112} INFO - [2022-03-22 18:24:41,690] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:41,795] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:41,803] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:41,823] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:42,078] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:42,206] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:42,259] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.617 seconds
[2022-03-22 18:24:45,601] {scheduler_job.py:153} INFO - Started process (PID=2548) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:45,608] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:45,610] {logging_mixin.py:112} INFO - [2022-03-22 18:24:45,609] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:45,647] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:45,653] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:45,663] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:45,896] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:45,972] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:45,993] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.392 seconds
[2022-03-22 18:24:49,584] {scheduler_job.py:153} INFO - Started process (PID=2550) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:49,592] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:49,594] {logging_mixin.py:112} INFO - [2022-03-22 18:24:49,593] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:49,627] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:49,632] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:49,641] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:49,976] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:50,104] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:50,161] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.577 seconds
[2022-03-22 18:24:53,596] {scheduler_job.py:153} INFO - Started process (PID=2552) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:53,604] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:53,606] {logging_mixin.py:112} INFO - [2022-03-22 18:24:53,605] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:53,635] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:53,639] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:53,646] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:53,843] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:53,929] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:53,951] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.356 seconds
[2022-03-22 18:24:57,604] {scheduler_job.py:153} INFO - Started process (PID=2554) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:57,618] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:24:57,620] {logging_mixin.py:112} INFO - [2022-03-22 18:24:57,619] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:57,658] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:24:57,664] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:24:57,677] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:24:57,961] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:24:58,056] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:24:58,078] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.475 seconds
[2022-03-22 18:25:01,604] {scheduler_job.py:153} INFO - Started process (PID=2556) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:01,610] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:01,612] {logging_mixin.py:112} INFO - [2022-03-22 18:25:01,612] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:01,659] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:01,664] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:01,672] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:01,954] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:02,090] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:02,121] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.518 seconds
[2022-03-22 18:25:05,875] {scheduler_job.py:153} INFO - Started process (PID=2558) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:05,892] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:05,896] {logging_mixin.py:112} INFO - [2022-03-22 18:25:05,896] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:06,010] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:06,033] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:06,064] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:06,873] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:07,027] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:07,107] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.233 seconds
[2022-03-22 18:25:09,635] {scheduler_job.py:153} INFO - Started process (PID=2560) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:09,652] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:09,660] {logging_mixin.py:112} INFO - [2022-03-22 18:25:09,659] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:09,713] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:09,718] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:09,734] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:10,055] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:10,145] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:10,167] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.532 seconds
[2022-03-22 18:25:13,711] {scheduler_job.py:153} INFO - Started process (PID=2562) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:13,726] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:13,728] {logging_mixin.py:112} INFO - [2022-03-22 18:25:13,727] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:13,780] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:13,789] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:13,806] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:14,121] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:14,373] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:14,410] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.700 seconds
[2022-03-22 18:25:17,676] {scheduler_job.py:153} INFO - Started process (PID=2564) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:17,684] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:17,689] {logging_mixin.py:112} INFO - [2022-03-22 18:25:17,689] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:17,729] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:17,740] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:17,751] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:18,121] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:18,216] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:18,243] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.567 seconds
[2022-03-22 18:25:21,645] {scheduler_job.py:153} INFO - Started process (PID=2566) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:21,656] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:21,657] {logging_mixin.py:112} INFO - [2022-03-22 18:25:21,657] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:21,757] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:21,763] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:21,773] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:22,093] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:22,190] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:22,245] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.600 seconds
[2022-03-22 18:25:25,634] {scheduler_job.py:153} INFO - Started process (PID=2568) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:25,642] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:25,644] {logging_mixin.py:112} INFO - [2022-03-22 18:25:25,644] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:25,692] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:25,699] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:25,714] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:26,411] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:26,518] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:26,541] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.908 seconds
[2022-03-22 18:25:29,681] {scheduler_job.py:153} INFO - Started process (PID=2570) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:29,693] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:29,695] {logging_mixin.py:112} INFO - [2022-03-22 18:25:29,695] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:29,875] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:29,882] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:29,895] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:30,375] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:30,550] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:30,577] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.895 seconds
[2022-03-22 18:25:33,662] {scheduler_job.py:153} INFO - Started process (PID=2572) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:33,677] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:33,679] {logging_mixin.py:112} INFO - [2022-03-22 18:25:33,678] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:33,737] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:33,743] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:33,762] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:34,176] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:34,314] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:34,351] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.689 seconds
[2022-03-22 18:25:37,649] {scheduler_job.py:153} INFO - Started process (PID=2574) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:37,655] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:37,656] {logging_mixin.py:112} INFO - [2022-03-22 18:25:37,656] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:37,690] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:37,694] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:37,712] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:38,158] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:38,271] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:38,297] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.649 seconds
[2022-03-22 18:25:41,671] {scheduler_job.py:153} INFO - Started process (PID=2576) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:41,684] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:41,686] {logging_mixin.py:112} INFO - [2022-03-22 18:25:41,685] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:41,745] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:41,750] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:41,779] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:42,394] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:42,677] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:42,814] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.143 seconds
[2022-03-22 18:25:45,701] {scheduler_job.py:153} INFO - Started process (PID=2578) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:45,746] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:45,763] {logging_mixin.py:112} INFO - [2022-03-22 18:25:45,763] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:45,972] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:45,978] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:46,014] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:46,592] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:46,748] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:46,794] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.093 seconds
[2022-03-22 18:25:49,656] {scheduler_job.py:153} INFO - Started process (PID=2580) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:49,662] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:49,666] {logging_mixin.py:112} INFO - [2022-03-22 18:25:49,663] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:49,719] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:49,724] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:49,759] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:50,063] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:50,277] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:50,313] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.658 seconds
[2022-03-22 18:25:53,665] {scheduler_job.py:153} INFO - Started process (PID=2582) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:53,671] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:53,673] {logging_mixin.py:112} INFO - [2022-03-22 18:25:53,673] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:53,703] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:53,708] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:53,726] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:53,989] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:54,067] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:54,091] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.426 seconds
[2022-03-22 18:25:57,672] {scheduler_job.py:153} INFO - Started process (PID=2584) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:57,680] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:25:57,683] {logging_mixin.py:112} INFO - [2022-03-22 18:25:57,683] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:57,719] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:25:57,724] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:25:57,746] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:25:58,295] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:25:58,405] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:25:58,434] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.763 seconds
[2022-03-22 18:26:01,664] {scheduler_job.py:153} INFO - Started process (PID=2586) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:01,671] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:01,673] {logging_mixin.py:112} INFO - [2022-03-22 18:26:01,673] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:01,715] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:01,720] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:01,738] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:01,985] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:02,064] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:02,086] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.422 seconds
[2022-03-22 18:26:05,713] {scheduler_job.py:153} INFO - Started process (PID=2605) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:05,725] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:05,727] {logging_mixin.py:112} INFO - [2022-03-22 18:26:05,726] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:05,796] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:05,807] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:05,829] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:06,094] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:06,180] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:06,205] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.492 seconds
[2022-03-22 18:26:09,693] {scheduler_job.py:153} INFO - Started process (PID=2609) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:09,704] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:09,706] {logging_mixin.py:112} INFO - [2022-03-22 18:26:09,706] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:09,735] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:09,740] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:09,755] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:10,021] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:10,111] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:10,134] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.441 seconds
[2022-03-22 18:26:13,686] {scheduler_job.py:153} INFO - Started process (PID=2614) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:13,694] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:13,696] {logging_mixin.py:112} INFO - [2022-03-22 18:26:13,696] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:13,744] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:13,758] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:13,778] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:14,103] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:14,216] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:14,249] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.563 seconds
[2022-03-22 18:26:17,694] {scheduler_job.py:153} INFO - Started process (PID=2621) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:17,701] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:17,703] {logging_mixin.py:112} INFO - [2022-03-22 18:26:17,703] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:17,752] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:17,758] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:17,778] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:18,094] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:18,208] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:18,246] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.552 seconds
[2022-03-22 18:26:21,703] {scheduler_job.py:153} INFO - Started process (PID=2625) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:21,714] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:21,719] {logging_mixin.py:112} INFO - [2022-03-22 18:26:21,718] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:21,769] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:21,775] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:21,817] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:22,110] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:22,217] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:22,246] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.544 seconds
[2022-03-22 18:26:25,710] {scheduler_job.py:153} INFO - Started process (PID=2627) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:25,716] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:25,717] {logging_mixin.py:112} INFO - [2022-03-22 18:26:25,717] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:25,753] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:25,758] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:25,784] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:25,993] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:26,072] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:26,092] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.383 seconds
[2022-03-22 18:26:29,701] {scheduler_job.py:153} INFO - Started process (PID=2629) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:29,707] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:29,709] {logging_mixin.py:112} INFO - [2022-03-22 18:26:29,708] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:29,742] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:29,747] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:29,761] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:30,006] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:30,094] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:30,116] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.415 seconds
[2022-03-22 18:26:33,728] {scheduler_job.py:153} INFO - Started process (PID=2631) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:33,734] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:33,735] {logging_mixin.py:112} INFO - [2022-03-22 18:26:33,735] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:33,763] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:33,767] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:33,782] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:34,048] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:34,138] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:34,160] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.432 seconds
[2022-03-22 18:26:37,717] {scheduler_job.py:153} INFO - Started process (PID=2633) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:37,722] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:37,724] {logging_mixin.py:112} INFO - [2022-03-22 18:26:37,723] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:37,750] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:37,754] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:37,767] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:37,960] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:38,039] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:38,060] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.343 seconds
[2022-03-22 18:26:41,716] {scheduler_job.py:153} INFO - Started process (PID=2635) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:41,721] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:41,722] {logging_mixin.py:112} INFO - [2022-03-22 18:26:41,722] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:41,752] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:41,758] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:41,771] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:41,978] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:42,064] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:42,087] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.372 seconds
[2022-03-22 18:26:45,742] {scheduler_job.py:153} INFO - Started process (PID=2637) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:45,747] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:45,749] {logging_mixin.py:112} INFO - [2022-03-22 18:26:45,749] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:45,788] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:45,791] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:45,805] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:46,054] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:46,139] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:46,159] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.417 seconds
[2022-03-22 18:26:49,732] {scheduler_job.py:153} INFO - Started process (PID=2639) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:49,739] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:49,740] {logging_mixin.py:112} INFO - [2022-03-22 18:26:49,740] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:49,779] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:49,786] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:49,804] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:50,022] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:50,100] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:50,122] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.390 seconds
[2022-03-22 18:26:53,743] {scheduler_job.py:153} INFO - Started process (PID=2641) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:53,750] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:53,752] {logging_mixin.py:112} INFO - [2022-03-22 18:26:53,751] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:53,780] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:53,785] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:53,799] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:54,075] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:54,163] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:54,190] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.447 seconds
[2022-03-22 18:26:57,754] {scheduler_job.py:153} INFO - Started process (PID=2643) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:57,760] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:26:57,762] {logging_mixin.py:112} INFO - [2022-03-22 18:26:57,761] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:57,808] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:26:57,813] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:26:57,832] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:26:58,080] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:26:58,173] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:26:58,196] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.442 seconds
[2022-03-22 18:27:01,744] {scheduler_job.py:153} INFO - Started process (PID=2645) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:01,750] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:01,752] {logging_mixin.py:112} INFO - [2022-03-22 18:27:01,752] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:01,787] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:01,791] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:01,808] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:02,051] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:02,140] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:02,162] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.419 seconds
[2022-03-22 18:27:06,314] {scheduler_job.py:153} INFO - Started process (PID=2647) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:06,321] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:06,323] {logging_mixin.py:112} INFO - [2022-03-22 18:27:06,323] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:06,358] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:06,363] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:06,380] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:06,708] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:06,890] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:06,946] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.633 seconds
[2022-03-22 18:27:10,306] {scheduler_job.py:153} INFO - Started process (PID=2650) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:10,312] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:10,314] {logging_mixin.py:112} INFO - [2022-03-22 18:27:10,314] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:10,344] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:10,348] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:10,363] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:10,584] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:10,672] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:10,700] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.394 seconds
[2022-03-22 18:27:14,421] {scheduler_job.py:153} INFO - Started process (PID=2652) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:14,442] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:14,444] {logging_mixin.py:112} INFO - [2022-03-22 18:27:14,444] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:14,495] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:14,500] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:14,515] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:14,785] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:14,925] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:14,958] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.537 seconds
[2022-03-22 18:27:19,265] {scheduler_job.py:153} INFO - Started process (PID=2654) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:19,273] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:19,275] {logging_mixin.py:112} INFO - [2022-03-22 18:27:19,275] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:19,377] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:19,383] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:19,408] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:19,752] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:19,857] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:19,883] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.618 seconds
[2022-03-22 18:27:22,639] {scheduler_job.py:153} INFO - Started process (PID=2656) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:22,649] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:22,653] {logging_mixin.py:112} INFO - [2022-03-22 18:27:22,653] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:22,726] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:22,731] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:22,763] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:23,173] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:23,320] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:23,358] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.719 seconds
[2022-03-22 18:27:26,337] {scheduler_job.py:153} INFO - Started process (PID=2658) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:26,346] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:26,350] {logging_mixin.py:112} INFO - [2022-03-22 18:27:26,350] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:26,392] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:26,397] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:26,419] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:26,720] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:26,845] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:26,880] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.542 seconds
[2022-03-22 18:27:30,377] {scheduler_job.py:153} INFO - Started process (PID=2660) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:30,389] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:30,391] {logging_mixin.py:112} INFO - [2022-03-22 18:27:30,391] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:30,638] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:30,644] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:30,667] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:30,953] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:31,094] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:31,130] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.753 seconds
[2022-03-22 18:27:34,349] {scheduler_job.py:153} INFO - Started process (PID=2662) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:34,359] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:34,361] {logging_mixin.py:112} INFO - [2022-03-22 18:27:34,361] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:34,420] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:34,425] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:34,460] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:34,789] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:34,913] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:34,948] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.599 seconds
[2022-03-22 18:27:38,345] {scheduler_job.py:153} INFO - Started process (PID=2664) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:38,353] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:38,355] {logging_mixin.py:112} INFO - [2022-03-22 18:27:38,355] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:38,397] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:38,405] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:38,424] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:38,706] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:38,817] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:38,854] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.509 seconds
[2022-03-22 18:27:42,380] {scheduler_job.py:153} INFO - Started process (PID=2666) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:42,390] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:42,392] {logging_mixin.py:112} INFO - [2022-03-22 18:27:42,392] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:42,442] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:42,449] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:42,467] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:42,770] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:42,898] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:42,925] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.545 seconds
[2022-03-22 18:27:46,357] {scheduler_job.py:153} INFO - Started process (PID=2668) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:46,364] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:46,366] {logging_mixin.py:112} INFO - [2022-03-22 18:27:46,366] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:46,416] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:46,421] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:46,439] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:46,726] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:46,827] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:46,853] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.496 seconds
[2022-03-22 18:27:50,348] {scheduler_job.py:153} INFO - Started process (PID=2670) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:50,353] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:50,355] {logging_mixin.py:112} INFO - [2022-03-22 18:27:50,355] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:50,383] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:50,387] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:50,405] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:50,597] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:50,663] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:50,682] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.334 seconds
[2022-03-22 18:27:54,378] {scheduler_job.py:153} INFO - Started process (PID=2672) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:54,383] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:54,385] {logging_mixin.py:112} INFO - [2022-03-22 18:27:54,385] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:54,416] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:54,424] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:54,432] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:54,625] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:54,713] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:54,736] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.358 seconds
[2022-03-22 18:27:58,565] {scheduler_job.py:153} INFO - Started process (PID=2674) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:58,576] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:27:58,578] {logging_mixin.py:112} INFO - [2022-03-22 18:27:58,578] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:58,636] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:27:58,650] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:27:58,662] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:27:59,039] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:27:59,213] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:27:59,244] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.680 seconds
[2022-03-22 18:28:02,356] {scheduler_job.py:153} INFO - Started process (PID=2676) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:02,362] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:02,369] {logging_mixin.py:112} INFO - [2022-03-22 18:28:02,369] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:02,410] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:02,424] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:02,439] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:03,040] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:03,371] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:03,401] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.045 seconds
[2022-03-22 18:28:06,451] {scheduler_job.py:153} INFO - Started process (PID=2678) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:06,457] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:06,458] {logging_mixin.py:112} INFO - [2022-03-22 18:28:06,458] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:06,500] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:06,504] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:06,512] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:06,727] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:06,809] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:06,830] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.379 seconds
[2022-03-22 18:28:10,412] {scheduler_job.py:153} INFO - Started process (PID=2680) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:10,430] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:10,439] {logging_mixin.py:112} INFO - [2022-03-22 18:28:10,438] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:10,558] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:10,570] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:10,589] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:10,825] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:10,901] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:10,919] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.507 seconds
[2022-03-22 18:28:14,364] {scheduler_job.py:153} INFO - Started process (PID=2682) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:14,371] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:14,372] {logging_mixin.py:112} INFO - [2022-03-22 18:28:14,372] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:14,404] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:14,409] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:14,417] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:14,609] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:14,687] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:14,707] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.344 seconds
[2022-03-22 18:28:18,412] {scheduler_job.py:153} INFO - Started process (PID=2684) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:18,430] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:18,433] {logging_mixin.py:112} INFO - [2022-03-22 18:28:18,433] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:18,473] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:18,478] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:18,494] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:18,764] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:18,890] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:18,923] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.511 seconds
[2022-03-22 18:28:22,402] {scheduler_job.py:153} INFO - Started process (PID=2686) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:22,419] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:22,422] {logging_mixin.py:112} INFO - [2022-03-22 18:28:22,422] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:22,481] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:22,490] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:22,511] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:22,799] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:22,897] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:22,922] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.520 seconds
[2022-03-22 18:28:26,393] {scheduler_job.py:153} INFO - Started process (PID=2688) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:26,407] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:26,409] {logging_mixin.py:112} INFO - [2022-03-22 18:28:26,408] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:26,447] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:26,453] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:26,465] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:26,755] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:26,847] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:26,875] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.482 seconds
[2022-03-22 18:28:30,427] {scheduler_job.py:153} INFO - Started process (PID=2690) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:30,439] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:30,441] {logging_mixin.py:112} INFO - [2022-03-22 18:28:30,441] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:30,477] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:30,482] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:30,492] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:30,711] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:30,796] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:30,818] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.391 seconds
[2022-03-22 18:28:34,415] {scheduler_job.py:153} INFO - Started process (PID=2692) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:34,425] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:34,430] {logging_mixin.py:112} INFO - [2022-03-22 18:28:34,430] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:34,475] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:34,482] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:34,491] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:34,743] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:34,826] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:34,850] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.435 seconds
[2022-03-22 18:28:38,408] {scheduler_job.py:153} INFO - Started process (PID=2694) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:38,421] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:38,423] {logging_mixin.py:112} INFO - [2022-03-22 18:28:38,422] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:38,470] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:38,475] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:38,486] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:38,703] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:38,795] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:38,823] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.416 seconds
[2022-03-22 18:28:42,463] {scheduler_job.py:153} INFO - Started process (PID=2696) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:42,477] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:42,478] {logging_mixin.py:112} INFO - [2022-03-22 18:28:42,478] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:42,511] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:42,517] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:42,527] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:42,792] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:42,877] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:42,897] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.434 seconds
[2022-03-22 18:28:46,438] {scheduler_job.py:153} INFO - Started process (PID=2698) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:46,448] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:46,449] {logging_mixin.py:112} INFO - [2022-03-22 18:28:46,449] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:46,493] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:46,497] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:46,507] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:46,741] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:46,824] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:46,845] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.408 seconds
[2022-03-22 18:28:50,433] {scheduler_job.py:153} INFO - Started process (PID=2700) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:50,444] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:50,445] {logging_mixin.py:112} INFO - [2022-03-22 18:28:50,445] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:50,472] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:50,476] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:50,484] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:50,684] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:50,757] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:50,778] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.346 seconds
[2022-03-22 18:28:54,753] {scheduler_job.py:153} INFO - Started process (PID=2702) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:54,796] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:54,803] {logging_mixin.py:112} INFO - [2022-03-22 18:28:54,803] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:54,931] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:54,950] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:54,979] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:55,280] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:55,356] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:55,376] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.623 seconds
[2022-03-22 18:28:58,455] {scheduler_job.py:153} INFO - Started process (PID=2704) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:58,467] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:28:58,469] {logging_mixin.py:112} INFO - [2022-03-22 18:28:58,468] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:58,531] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:28:58,536] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:28:58,545] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:28:58,809] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:28:58,929] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:28:58,983] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.529 seconds
[2022-03-22 18:29:02,463] {scheduler_job.py:153} INFO - Started process (PID=2706) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:02,476] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:02,478] {logging_mixin.py:112} INFO - [2022-03-22 18:29:02,478] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:02,560] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:02,569] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:02,587] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:03,197] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:03,348] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:03,388] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.925 seconds
[2022-03-22 18:29:06,480] {scheduler_job.py:153} INFO - Started process (PID=2708) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:06,485] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:06,486] {logging_mixin.py:112} INFO - [2022-03-22 18:29:06,486] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:06,514] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:06,518] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:06,526] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:06,749] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:06,826] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:06,853] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.373 seconds
[2022-03-22 18:29:10,491] {scheduler_job.py:153} INFO - Started process (PID=2710) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:10,510] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:10,521] {logging_mixin.py:112} INFO - [2022-03-22 18:29:10,521] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:10,587] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:10,598] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:10,630] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:10,920] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:11,074] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:11,108] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.617 seconds
[2022-03-22 18:29:14,455] {scheduler_job.py:153} INFO - Started process (PID=2712) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:14,461] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:14,462] {logging_mixin.py:112} INFO - [2022-03-22 18:29:14,462] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:14,511] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:14,522] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:14,543] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:14,897] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:14,994] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:15,072] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.617 seconds
[2022-03-22 18:29:18,508] {scheduler_job.py:153} INFO - Started process (PID=2714) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:18,516] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:18,517] {logging_mixin.py:112} INFO - [2022-03-22 18:29:18,517] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:18,551] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:18,556] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:18,568] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:18,780] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:18,878] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:18,927] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.419 seconds
[2022-03-22 18:29:22,489] {scheduler_job.py:153} INFO - Started process (PID=2716) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:22,498] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:22,504] {logging_mixin.py:112} INFO - [2022-03-22 18:29:22,504] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:22,604] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:22,610] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:22,624] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:23,294] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:23,390] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:23,415] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.926 seconds
[2022-03-22 18:29:26,480] {scheduler_job.py:153} INFO - Started process (PID=2737) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:26,487] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:26,488] {logging_mixin.py:112} INFO - [2022-03-22 18:29:26,488] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:26,516] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:26,521] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:26,533] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:26,783] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:26,852] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:26,872] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.392 seconds
[2022-03-22 18:29:30,518] {scheduler_job.py:153} INFO - Started process (PID=2741) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:30,529] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:30,531] {logging_mixin.py:112} INFO - [2022-03-22 18:29:30,530] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:30,578] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:30,588] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:30,600] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:30,882] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:31,006] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:31,036] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.518 seconds
[2022-03-22 18:29:34,509] {scheduler_job.py:153} INFO - Started process (PID=2748) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:34,524] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:34,528] {logging_mixin.py:112} INFO - [2022-03-22 18:29:34,528] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:34,585] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:34,594] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:34,610] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:34,954] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:35,070] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:35,102] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.593 seconds
[2022-03-22 18:29:38,492] {scheduler_job.py:153} INFO - Started process (PID=2753) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:38,499] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:38,502] {logging_mixin.py:112} INFO - [2022-03-22 18:29:38,501] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:38,533] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:38,538] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:38,545] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:38,744] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:38,822] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:38,846] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.355 seconds
[2022-03-22 18:29:42,504] {scheduler_job.py:153} INFO - Started process (PID=2755) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:42,509] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:42,510] {logging_mixin.py:112} INFO - [2022-03-22 18:29:42,510] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:42,539] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:42,543] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:42,552] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:42,757] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:42,829] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:42,855] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.352 seconds
[2022-03-22 18:29:46,517] {scheduler_job.py:153} INFO - Started process (PID=2757) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:46,526] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:46,527] {logging_mixin.py:112} INFO - [2022-03-22 18:29:46,527] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:46,555] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:46,559] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:46,570] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:46,851] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:46,929] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:46,953] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.436 seconds
[2022-03-22 18:29:50,764] {scheduler_job.py:153} INFO - Started process (PID=2759) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:50,780] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:50,798] {logging_mixin.py:112} INFO - [2022-03-22 18:29:50,797] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:50,977] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:50,984] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:51,018] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:51,492] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:51,690] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:51,816] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.053 seconds
[2022-03-22 18:29:54,534] {scheduler_job.py:153} INFO - Started process (PID=2761) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:54,539] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:54,541] {logging_mixin.py:112} INFO - [2022-03-22 18:29:54,540] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:54,580] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:54,588] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:54,599] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:54,840] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:54,999] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:55,103] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.569 seconds
[2022-03-22 18:29:58,520] {scheduler_job.py:153} INFO - Started process (PID=2763) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:58,528] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:29:58,530] {logging_mixin.py:112} INFO - [2022-03-22 18:29:58,529] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:58,556] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:29:58,560] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:29:58,570] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:29:58,795] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:29:58,876] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:29:58,899] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.379 seconds
[2022-03-22 18:30:02,523] {scheduler_job.py:153} INFO - Started process (PID=2765) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:02,529] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:02,530] {logging_mixin.py:112} INFO - [2022-03-22 18:30:02,530] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:02,563] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:02,568] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:02,577] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:02,818] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:02,906] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:02,928] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.405 seconds
[2022-03-22 18:30:06,549] {scheduler_job.py:153} INFO - Started process (PID=2767) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:06,554] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:06,555] {logging_mixin.py:112} INFO - [2022-03-22 18:30:06,555] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:06,582] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:06,586] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:06,595] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:06,867] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:06,992] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:07,019] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.470 seconds
[2022-03-22 18:30:10,534] {scheduler_job.py:153} INFO - Started process (PID=2769) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:10,546] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:10,549] {logging_mixin.py:112} INFO - [2022-03-22 18:30:10,549] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:10,584] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:10,589] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:10,600] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:10,890] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:10,977] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:11,002] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.467 seconds
[2022-03-22 18:30:14,544] {scheduler_job.py:153} INFO - Started process (PID=2771) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:14,550] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:14,552] {logging_mixin.py:112} INFO - [2022-03-22 18:30:14,552] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:14,583] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:14,588] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:14,600] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:14,859] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:14,938] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:14,959] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.414 seconds
[2022-03-22 18:30:18,559] {scheduler_job.py:153} INFO - Started process (PID=2773) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:18,566] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:18,568] {logging_mixin.py:112} INFO - [2022-03-22 18:30:18,567] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:18,598] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:18,602] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:18,612] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:19,016] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:19,109] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:19,133] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.573 seconds
[2022-03-22 18:30:22,659] {scheduler_job.py:153} INFO - Started process (PID=2775) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:22,670] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:22,696] {logging_mixin.py:112} INFO - [2022-03-22 18:30:22,696] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:22,962] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:22,969] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:23,004] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:23,901] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:24,360] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:24,469] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.810 seconds
[2022-03-22 18:30:27,350] {scheduler_job.py:153} INFO - Started process (PID=2777) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:27,465] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:27,490] {logging_mixin.py:112} INFO - [2022-03-22 18:30:27,490] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:28,322] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:28,350] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:28,371] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:29,173] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:29,260] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:29,285] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.935 seconds
[2022-03-22 18:30:31,989] {scheduler_job.py:153} INFO - Started process (PID=2779) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:31,994] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:31,996] {logging_mixin.py:112} INFO - [2022-03-22 18:30:31,995] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:32,034] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:32,039] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:32,050] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:32,367] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:32,461] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:32,485] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.497 seconds
[2022-03-22 18:30:36,000] {scheduler_job.py:153} INFO - Started process (PID=2781) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:36,006] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:36,007] {logging_mixin.py:112} INFO - [2022-03-22 18:30:36,007] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:36,041] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:36,046] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:36,057] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:36,393] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:36,474] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:36,497] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.497 seconds
[2022-03-22 18:30:40,020] {scheduler_job.py:153} INFO - Started process (PID=2783) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:40,036] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:40,049] {logging_mixin.py:112} INFO - [2022-03-22 18:30:40,039] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:40,084] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:40,089] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:40,101] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:40,608] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:40,742] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:40,770] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.750 seconds
[2022-03-22 18:30:43,994] {scheduler_job.py:153} INFO - Started process (PID=2785) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:44,002] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:44,005] {logging_mixin.py:112} INFO - [2022-03-22 18:30:44,005] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:44,041] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:44,046] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:44,056] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:44,310] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:44,389] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:44,410] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.416 seconds
[2022-03-22 18:30:48,005] {scheduler_job.py:153} INFO - Started process (PID=2787) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:48,012] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:48,013] {logging_mixin.py:112} INFO - [2022-03-22 18:30:48,013] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:48,044] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:48,049] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:48,058] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:48,270] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:48,369] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:48,394] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.389 seconds
[2022-03-22 18:30:52,013] {scheduler_job.py:153} INFO - Started process (PID=2789) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:52,022] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:52,024] {logging_mixin.py:112} INFO - [2022-03-22 18:30:52,024] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:52,061] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:52,067] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:52,077] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:52,470] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:52,642] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:52,675] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.661 seconds
[2022-03-22 18:30:56,016] {scheduler_job.py:153} INFO - Started process (PID=2791) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:56,023] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:30:56,024] {logging_mixin.py:112} INFO - [2022-03-22 18:30:56,024] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:56,058] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:30:56,062] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:30:56,074] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:30:56,303] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:30:56,388] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:30:56,411] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.394 seconds
[2022-03-22 18:31:00,032] {scheduler_job.py:153} INFO - Started process (PID=2793) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:00,038] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:00,041] {logging_mixin.py:112} INFO - [2022-03-22 18:31:00,040] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:00,087] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:00,092] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:00,103] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:00,341] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:00,426] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:00,444] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.412 seconds
[2022-03-22 18:31:04,037] {scheduler_job.py:153} INFO - Started process (PID=2795) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:04,043] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:04,045] {logging_mixin.py:112} INFO - [2022-03-22 18:31:04,045] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:04,082] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:04,087] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:04,099] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:04,342] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:04,423] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:04,444] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.408 seconds
[2022-03-22 18:31:08,030] {scheduler_job.py:153} INFO - Started process (PID=2797) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:08,036] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:08,043] {logging_mixin.py:112} INFO - [2022-03-22 18:31:08,042] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:08,078] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:08,083] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:08,094] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:08,320] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:08,397] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:08,421] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.391 seconds
[2022-03-22 18:31:12,044] {scheduler_job.py:153} INFO - Started process (PID=2799) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:12,053] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:12,055] {logging_mixin.py:112} INFO - [2022-03-22 18:31:12,055] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:12,093] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:12,099] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:12,110] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:12,321] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:12,397] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:12,416] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.372 seconds
[2022-03-22 18:31:16,049] {scheduler_job.py:153} INFO - Started process (PID=2801) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:16,063] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:16,064] {logging_mixin.py:112} INFO - [2022-03-22 18:31:16,064] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:16,095] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:16,101] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:16,113] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:16,343] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:16,422] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:16,446] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.398 seconds
[2022-03-22 18:31:20,041] {scheduler_job.py:153} INFO - Started process (PID=2803) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:20,049] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:20,050] {logging_mixin.py:112} INFO - [2022-03-22 18:31:20,050] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:20,086] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:20,090] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:20,101] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:20,323] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:20,410] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:20,433] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.392 seconds
[2022-03-22 18:31:24,061] {scheduler_job.py:153} INFO - Started process (PID=2805) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:24,071] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:24,073] {logging_mixin.py:112} INFO - [2022-03-22 18:31:24,072] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:24,104] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:24,109] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:24,119] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:24,329] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:24,410] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:24,432] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.370 seconds
[2022-03-22 18:31:28,062] {scheduler_job.py:153} INFO - Started process (PID=2807) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:28,068] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:28,071] {logging_mixin.py:112} INFO - [2022-03-22 18:31:28,070] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:28,107] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:28,111] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:28,122] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:28,359] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:28,441] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:28,463] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.401 seconds
[2022-03-22 18:31:32,070] {scheduler_job.py:153} INFO - Started process (PID=2809) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:32,077] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:32,079] {logging_mixin.py:112} INFO - [2022-03-22 18:31:32,079] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:32,118] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:32,124] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:32,136] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:32,334] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:32,421] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:32,442] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.373 seconds
[2022-03-22 18:31:36,068] {scheduler_job.py:153} INFO - Started process (PID=2811) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:36,074] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:36,076] {logging_mixin.py:112} INFO - [2022-03-22 18:31:36,076] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:36,105] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:36,109] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:36,119] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:36,356] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:36,473] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:36,502] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.434 seconds
[2022-03-22 18:31:40,068] {scheduler_job.py:153} INFO - Started process (PID=2813) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:40,082] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:40,088] {logging_mixin.py:112} INFO - [2022-03-22 18:31:40,087] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:40,122] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:40,127] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:40,138] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:40,707] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:40,803] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:40,827] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.759 seconds
[2022-03-22 18:31:44,071] {scheduler_job.py:153} INFO - Started process (PID=2815) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:44,077] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:44,078] {logging_mixin.py:112} INFO - [2022-03-22 18:31:44,078] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:44,114] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:44,121] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:44,134] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:44,356] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:44,423] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:44,444] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.373 seconds
[2022-03-22 18:31:48,102] {scheduler_job.py:153} INFO - Started process (PID=2817) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:48,119] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:48,130] {logging_mixin.py:112} INFO - [2022-03-22 18:31:48,130] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:48,414] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:48,425] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:48,457] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:51,083] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:51,263] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:51,309] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 3.207 seconds
[2022-03-22 18:31:52,133] {scheduler_job.py:153} INFO - Started process (PID=2819) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:52,146] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:52,147] {logging_mixin.py:112} INFO - [2022-03-22 18:31:52,147] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:52,407] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:52,415] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:52,451] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:52,895] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:53,080] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:31:53,111] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.977 seconds
[2022-03-22 18:31:56,184] {scheduler_job.py:153} INFO - Started process (PID=2821) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:56,200] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:31:56,212] {logging_mixin.py:112} INFO - [2022-03-22 18:31:56,211] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:56,364] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:31:56,372] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:31:56,403] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:31:57,573] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:31:59,157] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:00,474] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 4.290 seconds
[2022-03-22 18:32:01,488] {scheduler_job.py:153} INFO - Started process (PID=2823) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:01,497] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:01,499] {logging_mixin.py:112} INFO - [2022-03-22 18:32:01,499] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:01,539] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:01,542] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:01,551] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:01,863] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:01,947] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:01,977] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.489 seconds
[2022-03-22 18:32:05,502] {scheduler_job.py:153} INFO - Started process (PID=2825) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:05,507] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:05,508] {logging_mixin.py:112} INFO - [2022-03-22 18:32:05,508] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:05,547] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:05,551] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:05,559] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:05,780] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:05,880] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:05,907] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.406 seconds
[2022-03-22 18:32:09,484] {scheduler_job.py:153} INFO - Started process (PID=2827) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:09,489] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:09,491] {logging_mixin.py:112} INFO - [2022-03-22 18:32:09,490] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:09,522] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:09,526] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:09,535] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:09,734] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:09,810] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:09,835] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.351 seconds
[2022-03-22 18:32:13,485] {scheduler_job.py:153} INFO - Started process (PID=2829) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:13,491] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:13,493] {logging_mixin.py:112} INFO - [2022-03-22 18:32:13,493] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:13,525] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:13,530] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:13,541] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:13,755] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:13,832] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:13,858] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.374 seconds
[2022-03-22 18:32:17,542] {scheduler_job.py:153} INFO - Started process (PID=2831) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:17,548] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:17,550] {logging_mixin.py:112} INFO - [2022-03-22 18:32:17,550] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:17,591] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:17,600] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:17,619] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:17,860] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:17,979] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:18,000] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.458 seconds
[2022-03-22 18:32:21,489] {scheduler_job.py:153} INFO - Started process (PID=2833) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:21,496] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:21,498] {logging_mixin.py:112} INFO - [2022-03-22 18:32:21,497] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:21,529] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:21,534] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:21,543] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:21,752] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:21,837] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:21,860] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.370 seconds
[2022-03-22 18:32:25,498] {scheduler_job.py:153} INFO - Started process (PID=2840) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:25,505] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:25,507] {logging_mixin.py:112} INFO - [2022-03-22 18:32:25,507] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:25,556] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:25,564] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:25,578] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:25,922] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:26,063] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:26,098] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.601 seconds
[2022-03-22 18:32:29,558] {scheduler_job.py:153} INFO - Started process (PID=2847) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:29,570] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:29,572] {logging_mixin.py:112} INFO - [2022-03-22 18:32:29,572] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:29,686] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:29,707] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:29,748] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:30,073] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:30,212] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:30,232] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.674 seconds
[2022-03-22 18:32:33,509] {scheduler_job.py:153} INFO - Started process (PID=2849) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:33,516] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:33,519] {logging_mixin.py:112} INFO - [2022-03-22 18:32:33,518] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:33,557] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:33,564] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:33,579] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:33,852] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:33,940] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:33,961] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.452 seconds
[2022-03-22 18:32:37,513] {scheduler_job.py:153} INFO - Started process (PID=2851) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:37,527] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:37,529] {logging_mixin.py:112} INFO - [2022-03-22 18:32:37,529] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:37,573] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:37,579] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:37,591] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:37,973] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:38,122] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:38,180] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.667 seconds
[2022-03-22 18:32:42,892] {scheduler_job.py:153} INFO - Started process (PID=2853) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:42,908] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:42,910] {logging_mixin.py:112} INFO - [2022-03-22 18:32:42,910] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:42,967] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:42,973] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:43,006] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:44,044] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:44,285] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:44,322] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.430 seconds
[2022-03-22 18:32:45,835] {scheduler_job.py:153} INFO - Started process (PID=2855) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:45,848] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:45,850] {logging_mixin.py:112} INFO - [2022-03-22 18:32:45,850] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:45,952] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:45,958] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:45,971] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:46,192] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:46,278] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:46,300] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.465 seconds
[2022-03-22 18:32:49,814] {scheduler_job.py:153} INFO - Started process (PID=2857) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:49,823] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:49,825] {logging_mixin.py:112} INFO - [2022-03-22 18:32:49,825] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:49,892] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:49,898] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:49,932] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:50,777] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:51,057] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:51,148] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.334 seconds
[2022-03-22 18:32:56,789] {scheduler_job.py:153} INFO - Started process (PID=2859) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:56,794] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:56,796] {logging_mixin.py:112} INFO - [2022-03-22 18:32:56,796] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:56,837] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:56,842] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:56,861] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:57,192] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:57,281] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:57,302] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.513 seconds
[2022-03-22 18:32:59,606] {scheduler_job.py:153} INFO - Started process (PID=2861) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:59,611] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:32:59,612] {logging_mixin.py:112} INFO - [2022-03-22 18:32:59,612] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:59,641] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:32:59,646] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:32:59,654] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:32:59,861] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:32:59,951] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:32:59,974] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.368 seconds
[2022-03-22 18:33:03,582] {scheduler_job.py:153} INFO - Started process (PID=2863) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:03,595] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:33:03,597] {logging_mixin.py:112} INFO - [2022-03-22 18:33:03,597] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:03,713] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:33:03,722] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:33:03,735] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:03,958] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:33:04,045] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:33:04,067] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.485 seconds
[2022-03-22 18:33:07,561] {scheduler_job.py:153} INFO - Started process (PID=2865) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:07,567] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:33:07,568] {logging_mixin.py:112} INFO - [2022-03-22 18:33:07,568] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:07,609] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:33:07,614] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:33:07,626] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:07,821] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:33:07,898] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:33:07,923] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.363 seconds
[2022-03-22 18:33:11,993] {scheduler_job.py:153} INFO - Started process (PID=2867) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:12,023] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:33:12,027] {logging_mixin.py:112} INFO - [2022-03-22 18:33:12,027] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:12,129] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:33:12,137] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:33:12,333] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:13,346] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:33:13,450] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:33:13,477] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.483 seconds
[2022-03-22 18:33:15,551] {scheduler_job.py:153} INFO - Started process (PID=2869) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:15,560] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:33:15,562] {logging_mixin.py:112} INFO - [2022-03-22 18:33:15,561] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:15,611] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:33:15,617] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:33:15,626] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:15,917] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:33:16,000] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:33:16,022] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.471 seconds
[2022-03-22 18:33:19,566] {scheduler_job.py:153} INFO - Started process (PID=2871) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:19,571] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:33:19,572] {logging_mixin.py:112} INFO - [2022-03-22 18:33:19,572] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:19,608] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:33:19,614] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:33:19,621] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:19,900] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:33:19,985] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:33:20,011] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.445 seconds
[2022-03-22 18:33:23,544] {scheduler_job.py:153} INFO - Started process (PID=2873) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:23,549] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:33:23,550] {logging_mixin.py:112} INFO - [2022-03-22 18:33:23,550] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:23,579] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:33:23,584] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:33:23,591] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:23,853] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:33:23,947] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:33:23,976] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.432 seconds
[2022-03-22 18:33:27,555] {scheduler_job.py:153} INFO - Started process (PID=2875) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:27,561] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:33:27,562] {logging_mixin.py:112} INFO - [2022-03-22 18:33:27,562] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:27,588] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:33:27,593] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:33:27,601] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:27,786] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:33:27,859] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:33:27,878] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.323 seconds
[2022-03-22 18:33:31,609] {scheduler_job.py:153} INFO - Started process (PID=2877) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:31,617] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:33:31,618] {logging_mixin.py:112} INFO - [2022-03-22 18:33:31,618] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:31,655] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:33:31,665] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:33:31,684] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:31,911] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:33:31,998] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:33:32,024] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.415 seconds
[2022-03-22 18:33:35,573] {scheduler_job.py:153} INFO - Started process (PID=2879) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:35,578] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:33:35,580] {logging_mixin.py:112} INFO - [2022-03-22 18:33:35,579] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:35,614] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:33:35,621] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:33:35,630] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:35,837] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:33:35,985] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:33:36,021] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.449 seconds
[2022-03-22 18:33:39,573] {scheduler_job.py:153} INFO - Started process (PID=2881) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:39,578] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:33:39,580] {logging_mixin.py:112} INFO - [2022-03-22 18:33:39,579] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:39,608] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:33:39,612] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:33:39,622] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:33:39,824] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:33:39,905] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:33:39,937] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.364 seconds
[2022-03-22 18:37:06,680] {scheduler_job.py:153} INFO - Started process (PID=2970) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:06,692] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:06,693] {logging_mixin.py:112} INFO - [2022-03-22 18:37:06,693] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:06,739] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:06,744] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:06,762] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:07,007] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:07,086] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:07,107] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.426 seconds
[2022-03-22 18:37:10,526] {scheduler_job.py:153} INFO - Started process (PID=2972) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:10,534] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:10,537] {logging_mixin.py:112} INFO - [2022-03-22 18:37:10,537] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:10,585] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:10,590] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:10,607] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:10,917] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:11,027] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:11,055] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.529 seconds
[2022-03-22 18:37:15,367] {scheduler_job.py:153} INFO - Started process (PID=2974) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:15,381] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:15,388] {logging_mixin.py:112} INFO - [2022-03-22 18:37:15,387] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:15,467] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:15,472] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:15,487] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:16,122] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:16,278] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:16,311] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.944 seconds
[2022-03-22 18:37:18,708] {scheduler_job.py:153} INFO - Started process (PID=2976) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:18,713] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:18,716] {logging_mixin.py:112} INFO - [2022-03-22 18:37:18,716] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:18,767] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:18,772] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:18,784] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:19,053] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:19,166] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:19,189] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.481 seconds
[2022-03-22 18:37:22,718] {scheduler_job.py:153} INFO - Started process (PID=2979) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:22,744] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:22,758] {logging_mixin.py:112} INFO - [2022-03-22 18:37:22,758] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:23,448] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:23,470] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:23,505] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:24,164] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:24,286] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:24,318] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.600 seconds
[2022-03-22 18:37:26,675] {scheduler_job.py:153} INFO - Started process (PID=2981) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:26,682] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:26,684] {logging_mixin.py:112} INFO - [2022-03-22 18:37:26,684] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:26,734] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:26,740] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:26,752] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:27,990] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:28,181] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:28,221] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.547 seconds
[2022-03-22 18:37:30,715] {scheduler_job.py:153} INFO - Started process (PID=2983) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:30,721] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:30,723] {logging_mixin.py:112} INFO - [2022-03-22 18:37:30,723] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:30,762] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:30,768] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:30,779] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:31,018] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:31,087] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:31,106] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.392 seconds
[2022-03-22 18:37:34,689] {scheduler_job.py:153} INFO - Started process (PID=2985) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:34,697] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:34,705] {logging_mixin.py:112} INFO - [2022-03-22 18:37:34,705] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:34,762] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:34,774] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:34,791] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:35,061] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:35,172] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:35,213] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.524 seconds
[2022-03-22 18:37:38,709] {scheduler_job.py:153} INFO - Started process (PID=2987) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:38,726] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:38,727] {logging_mixin.py:112} INFO - [2022-03-22 18:37:38,727] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:38,778] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:38,786] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:38,798] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:39,151] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:39,240] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:39,263] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.554 seconds
[2022-03-22 18:37:42,725] {scheduler_job.py:153} INFO - Started process (PID=2989) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:42,730] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:42,732] {logging_mixin.py:112} INFO - [2022-03-22 18:37:42,732] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:42,767] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:42,772] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:42,780] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:43,065] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:43,264] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:43,303] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.578 seconds
[2022-03-22 18:37:46,706] {scheduler_job.py:153} INFO - Started process (PID=2991) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:46,712] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:46,714] {logging_mixin.py:112} INFO - [2022-03-22 18:37:46,714] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:46,755] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:46,760] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:46,772] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:47,078] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:47,177] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:47,230] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.524 seconds
[2022-03-22 18:37:51,729] {scheduler_job.py:153} INFO - Started process (PID=2993) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:51,744] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:51,746] {logging_mixin.py:112} INFO - [2022-03-22 18:37:51,746] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:51,797] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:51,805] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:51,825] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:52,209] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:52,287] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:52,310] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.581 seconds
[2022-03-22 18:37:56,075] {scheduler_job.py:153} INFO - Started process (PID=2995) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:56,141] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:37:56,144] {logging_mixin.py:112} INFO - [2022-03-22 18:37:56,144] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:56,398] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:37:56,414] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:37:56,447] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:37:58,508] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:37:58,895] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:37:58,946] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 2.871 seconds
[2022-03-22 18:38:01,180] {scheduler_job.py:153} INFO - Started process (PID=2998) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:01,196] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:01,198] {logging_mixin.py:112} INFO - [2022-03-22 18:38:01,197] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:01,281] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:01,294] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:01,339] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:01,913] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:38:02,110] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:38:02,144] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.965 seconds
[2022-03-22 18:38:05,095] {scheduler_job.py:153} INFO - Started process (PID=3000) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:05,103] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:05,104] {logging_mixin.py:112} INFO - [2022-03-22 18:38:05,104] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:05,184] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:05,190] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:05,203] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:05,378] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.283 seconds
[2022-03-22 18:38:09,071] {scheduler_job.py:153} INFO - Started process (PID=3002) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:09,083] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:09,084] {logging_mixin.py:112} INFO - [2022-03-22 18:38:09,084] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:09,135] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:09,141] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:09,152] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:09,326] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.254 seconds
[2022-03-22 18:38:13,072] {scheduler_job.py:153} INFO - Started process (PID=3004) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:13,082] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:13,084] {logging_mixin.py:112} INFO - [2022-03-22 18:38:13,084] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:13,116] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:13,121] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:13,131] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:13,455] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.384 seconds
[2022-03-22 18:38:17,108] {scheduler_job.py:153} INFO - Started process (PID=3006) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:17,116] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:17,118] {logging_mixin.py:112} INFO - [2022-03-22 18:38:17,118] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:17,155] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:17,160] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:17,172] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:17,379] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.271 seconds
[2022-03-22 18:38:21,080] {scheduler_job.py:153} INFO - Started process (PID=3008) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:21,086] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:21,087] {logging_mixin.py:112} INFO - [2022-03-22 18:38:21,087] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:21,117] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:21,121] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:21,130] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:21,344] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.264 seconds
[2022-03-22 18:38:25,106] {scheduler_job.py:153} INFO - Started process (PID=3010) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:25,118] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:25,123] {logging_mixin.py:112} INFO - [2022-03-22 18:38:25,123] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:25,176] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:25,182] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:25,197] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:25,410] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.304 seconds
[2022-03-22 18:38:29,123] {scheduler_job.py:153} INFO - Started process (PID=3012) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:29,129] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:29,130] {logging_mixin.py:112} INFO - [2022-03-22 18:38:29,130] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:29,174] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:29,179] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:29,192] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:29,378] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.255 seconds
[2022-03-22 18:38:33,440] {scheduler_job.py:153} INFO - Started process (PID=3015) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:33,476] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:33,488] {logging_mixin.py:112} INFO - [2022-03-22 18:38:33,488] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:33,769] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:33,788] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:33,821] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:34,413] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.974 seconds
[2022-03-22 18:38:37,104] {scheduler_job.py:153} INFO - Started process (PID=3017) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:37,109] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:37,110] {logging_mixin.py:112} INFO - [2022-03-22 18:38:37,110] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:37,147] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:37,153] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:37,162] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:37,384] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.281 seconds
[2022-03-22 18:38:41,147] {scheduler_job.py:153} INFO - Started process (PID=3019) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:41,157] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:41,159] {logging_mixin.py:112} INFO - [2022-03-22 18:38:41,159] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:41,205] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:41,211] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:41,227] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:41,458] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.311 seconds
[2022-03-22 18:38:45,104] {scheduler_job.py:153} INFO - Started process (PID=3021) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:45,122] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:45,124] {logging_mixin.py:112} INFO - [2022-03-22 18:38:45,124] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:45,157] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:45,164] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:45,174] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:45,691] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.587 seconds
[2022-03-22 18:38:49,119] {scheduler_job.py:153} INFO - Started process (PID=3023) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:49,129] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:49,131] {logging_mixin.py:112} INFO - [2022-03-22 18:38:49,131] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:49,172] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:49,177] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:49,190] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:49,413] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.294 seconds
[2022-03-22 18:38:53,145] {scheduler_job.py:153} INFO - Started process (PID=3025) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:53,151] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:53,153] {logging_mixin.py:112} INFO - [2022-03-22 18:38:53,152] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:53,178] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:53,183] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:53,190] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:53,351] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.206 seconds
[2022-03-22 18:38:57,145] {scheduler_job.py:153} INFO - Started process (PID=3027) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:57,151] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:38:57,154] {logging_mixin.py:112} INFO - [2022-03-22 18:38:57,154] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:57,204] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:38:57,209] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:38:57,219] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:38:57,445] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.300 seconds
[2022-03-22 18:39:01,127] {scheduler_job.py:153} INFO - Started process (PID=3029) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:01,134] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:01,136] {logging_mixin.py:112} INFO - [2022-03-22 18:39:01,136] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:01,165] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:01,169] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:01,177] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:01,351] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.223 seconds
[2022-03-22 18:39:05,193] {scheduler_job.py:153} INFO - Started process (PID=3031) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:05,199] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:05,200] {logging_mixin.py:112} INFO - [2022-03-22 18:39:05,200] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:05,229] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:05,234] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:05,242] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:06,215] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.023 seconds
[2022-03-22 18:39:09,145] {scheduler_job.py:153} INFO - Started process (PID=3033) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:09,153] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:09,155] {logging_mixin.py:112} INFO - [2022-03-22 18:39:09,154] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:09,183] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:09,187] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:09,196] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:09,391] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.246 seconds
[2022-03-22 18:39:13,152] {scheduler_job.py:153} INFO - Started process (PID=3035) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:13,161] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:13,162] {logging_mixin.py:112} INFO - [2022-03-22 18:39:13,162] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:13,192] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:13,196] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:13,205] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:13,373] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.221 seconds
[2022-03-22 18:39:17,207] {scheduler_job.py:153} INFO - Started process (PID=3038) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:17,214] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:17,217] {logging_mixin.py:112} INFO - [2022-03-22 18:39:17,216] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:17,271] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:17,277] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:17,292] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:17,495] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.288 seconds
[2022-03-22 18:39:21,168] {scheduler_job.py:153} INFO - Started process (PID=3040) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:21,173] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:21,174] {logging_mixin.py:112} INFO - [2022-03-22 18:39:21,174] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:21,210] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:21,215] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:21,223] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:21,420] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.253 seconds
[2022-03-22 18:39:25,158] {scheduler_job.py:153} INFO - Started process (PID=3042) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:25,166] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:25,168] {logging_mixin.py:112} INFO - [2022-03-22 18:39:25,168] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:25,203] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:25,208] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:25,229] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:25,509] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.351 seconds
[2022-03-22 18:39:29,390] {scheduler_job.py:153} INFO - Started process (PID=3044) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:29,406] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:29,407] {logging_mixin.py:112} INFO - [2022-03-22 18:39:29,407] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:29,465] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:29,472] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:29,514] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:29,979] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.589 seconds
[2022-03-22 18:39:33,178] {scheduler_job.py:153} INFO - Started process (PID=3046) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:33,185] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:33,187] {logging_mixin.py:112} INFO - [2022-03-22 18:39:33,187] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:33,216] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:33,220] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:33,231] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:33,412] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.235 seconds
[2022-03-22 18:39:37,217] {scheduler_job.py:153} INFO - Started process (PID=3048) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:37,236] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:37,242] {logging_mixin.py:112} INFO - [2022-03-22 18:39:37,242] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:37,314] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:37,328] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:37,346] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:37,564] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.347 seconds
[2022-03-22 18:39:41,236] {scheduler_job.py:153} INFO - Started process (PID=3050) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:41,244] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:41,245] {logging_mixin.py:112} INFO - [2022-03-22 18:39:41,245] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:41,296] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:41,304] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:41,324] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:41,524] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.288 seconds
[2022-03-22 18:39:45,206] {scheduler_job.py:153} INFO - Started process (PID=3052) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:45,216] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:45,218] {logging_mixin.py:112} INFO - [2022-03-22 18:39:45,217] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:45,249] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:45,254] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:45,269] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:45,509] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.303 seconds
[2022-03-22 18:39:49,218] {scheduler_job.py:153} INFO - Started process (PID=3054) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:49,226] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:49,227] {logging_mixin.py:112} INFO - [2022-03-22 18:39:49,227] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:49,269] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:49,274] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:49,309] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:49,540] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.322 seconds
[2022-03-22 18:39:53,339] {scheduler_job.py:153} INFO - Started process (PID=3057) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:53,345] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:53,347] {logging_mixin.py:112} INFO - [2022-03-22 18:39:53,347] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:53,394] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:53,404] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:53,424] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:53,926] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.588 seconds
[2022-03-22 18:39:57,212] {scheduler_job.py:153} INFO - Started process (PID=3059) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:57,218] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:39:57,219] {logging_mixin.py:112} INFO - [2022-03-22 18:39:57,219] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:57,254] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:39:57,259] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:39:57,280] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:39:57,625] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.413 seconds
[2022-03-22 18:40:01,297] {scheduler_job.py:153} INFO - Started process (PID=3061) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:01,311] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:01,322] {logging_mixin.py:112} INFO - [2022-03-22 18:40:01,321] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:01,487] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:01,493] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:01,541] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:03,252] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.956 seconds
[2022-03-22 18:40:07,681] {scheduler_job.py:153} INFO - Started process (PID=3063) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:07,696] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:07,698] {logging_mixin.py:112} INFO - [2022-03-22 18:40:07,698] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:07,757] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:07,763] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:07,779] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:08,098] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.417 seconds
[2022-03-22 18:40:09,233] {scheduler_job.py:153} INFO - Started process (PID=3065) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:09,239] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:09,241] {logging_mixin.py:112} INFO - [2022-03-22 18:40:09,241] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:09,278] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:09,283] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:09,297] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:09,471] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.239 seconds
[2022-03-22 18:40:13,226] {scheduler_job.py:153} INFO - Started process (PID=3067) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:13,234] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:13,235] {logging_mixin.py:112} INFO - [2022-03-22 18:40:13,235] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:13,274] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:13,278] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:13,294] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:13,490] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.264 seconds
[2022-03-22 18:40:17,232] {scheduler_job.py:153} INFO - Started process (PID=3069) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:17,237] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:17,239] {logging_mixin.py:112} INFO - [2022-03-22 18:40:17,239] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:17,278] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:17,284] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:17,299] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:17,513] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.282 seconds
[2022-03-22 18:40:21,240] {scheduler_job.py:153} INFO - Started process (PID=3071) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:21,247] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:21,248] {logging_mixin.py:112} INFO - [2022-03-22 18:40:21,248] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:21,277] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:21,282] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:21,296] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:21,487] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.247 seconds
[2022-03-22 18:40:25,242] {scheduler_job.py:153} INFO - Started process (PID=3074) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:25,249] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:25,252] {logging_mixin.py:112} INFO - [2022-03-22 18:40:25,251] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:25,282] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:25,287] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:25,302] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:26,043] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.801 seconds
[2022-03-22 18:40:29,265] {scheduler_job.py:153} INFO - Started process (PID=3076) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:29,270] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:29,271] {logging_mixin.py:112} INFO - [2022-03-22 18:40:29,271] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:29,300] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:29,305] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:29,326] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:29,543] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.278 seconds
[2022-03-22 18:40:33,264] {scheduler_job.py:153} INFO - Started process (PID=3078) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:33,279] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:33,282] {logging_mixin.py:112} INFO - [2022-03-22 18:40:33,281] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:33,339] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:33,343] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:33,371] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:33,806] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.542 seconds
[2022-03-22 18:40:37,252] {scheduler_job.py:153} INFO - Started process (PID=3080) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:37,259] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:37,261] {logging_mixin.py:112} INFO - [2022-03-22 18:40:37,260] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:37,290] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:37,294] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:37,307] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:37,488] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.236 seconds
[2022-03-22 18:40:41,269] {scheduler_job.py:153} INFO - Started process (PID=3082) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:41,274] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:41,276] {logging_mixin.py:112} INFO - [2022-03-22 18:40:41,276] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:41,304] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:41,308] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:41,320] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:41,462] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.194 seconds
[2022-03-22 18:40:45,294] {scheduler_job.py:153} INFO - Started process (PID=3084) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:45,311] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:45,323] {logging_mixin.py:112} INFO - [2022-03-22 18:40:45,323] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:45,356] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:45,361] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:45,377] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:45,551] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.257 seconds
[2022-03-22 18:40:49,282] {scheduler_job.py:153} INFO - Started process (PID=3086) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:49,294] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:49,295] {logging_mixin.py:112} INFO - [2022-03-22 18:40:49,295] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:49,327] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:49,332] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:49,346] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:49,491] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.209 seconds
[2022-03-22 18:40:53,291] {scheduler_job.py:153} INFO - Started process (PID=3088) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:53,297] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:53,299] {logging_mixin.py:112} INFO - [2022-03-22 18:40:53,299] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:53,327] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:53,332] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:53,351] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:53,508] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.217 seconds
[2022-03-22 18:40:57,344] {scheduler_job.py:153} INFO - Started process (PID=3091) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:57,360] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:40:57,362] {logging_mixin.py:112} INFO - [2022-03-22 18:40:57,361] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:57,413] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:40:57,423] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:40:57,475] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:40:58,670] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.326 seconds
[2022-03-22 18:41:03,671] {scheduler_job.py:153} INFO - Started process (PID=3093) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:03,677] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:03,678] {logging_mixin.py:112} INFO - [2022-03-22 18:41:03,678] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:03,720] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:03,733] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:03,758] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:04,135] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.465 seconds
[2022-03-22 18:41:05,301] {scheduler_job.py:153} INFO - Started process (PID=3095) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:05,317] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:05,319] {logging_mixin.py:112} INFO - [2022-03-22 18:41:05,318] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:05,362] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:05,368] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:05,384] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:05,650] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.349 seconds
[2022-03-22 18:41:09,308] {scheduler_job.py:153} INFO - Started process (PID=3097) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:09,317] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:09,319] {logging_mixin.py:112} INFO - [2022-03-22 18:41:09,319] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:09,362] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:09,368] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:09,404] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:09,614] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.306 seconds
[2022-03-22 18:41:13,316] {scheduler_job.py:153} INFO - Started process (PID=3118) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:13,322] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:13,323] {logging_mixin.py:112} INFO - [2022-03-22 18:41:13,323] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:13,357] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:13,362] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:13,374] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:13,574] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.258 seconds
[2022-03-22 18:41:17,319] {scheduler_job.py:153} INFO - Started process (PID=3123) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:17,325] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:17,327] {logging_mixin.py:112} INFO - [2022-03-22 18:41:17,326] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:17,373] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:17,378] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:17,416] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:17,686] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.367 seconds
[2022-03-22 18:41:21,330] {scheduler_job.py:153} INFO - Started process (PID=3127) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:21,338] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:21,339] {logging_mixin.py:112} INFO - [2022-03-22 18:41:21,339] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:21,383] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:21,388] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:21,407] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:21,605] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.275 seconds
[2022-03-22 18:41:25,323] {scheduler_job.py:153} INFO - Started process (PID=3129) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:25,328] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:25,330] {logging_mixin.py:112} INFO - [2022-03-22 18:41:25,330] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:25,363] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:25,368] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:25,383] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:25,548] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.225 seconds
[2022-03-22 18:41:29,330] {scheduler_job.py:153} INFO - Started process (PID=3131) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:29,336] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:29,338] {logging_mixin.py:112} INFO - [2022-03-22 18:41:29,337] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:29,363] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:29,368] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:29,379] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:29,541] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.212 seconds
[2022-03-22 18:41:33,428] {scheduler_job.py:153} INFO - Started process (PID=3134) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:33,443] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:33,444] {logging_mixin.py:112} INFO - [2022-03-22 18:41:33,444] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:33,482] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:33,490] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:33,513] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:33,776] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.349 seconds
[2022-03-22 18:41:37,327] {scheduler_job.py:153} INFO - Started process (PID=3136) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:37,334] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:37,335] {logging_mixin.py:112} INFO - [2022-03-22 18:41:37,335] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:37,377] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:37,382] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:37,397] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:37,602] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.274 seconds
[2022-03-22 18:41:41,339] {scheduler_job.py:153} INFO - Started process (PID=3138) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:41,344] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:41,346] {logging_mixin.py:112} INFO - [2022-03-22 18:41:41,346] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:41,387] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:41,392] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:41,408] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:41,594] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.255 seconds
[2022-03-22 18:41:45,349] {scheduler_job.py:153} INFO - Started process (PID=3140) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:45,357] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:45,360] {logging_mixin.py:112} INFO - [2022-03-22 18:41:45,359] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:45,393] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:45,399] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:45,415] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:45,957] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.609 seconds
[2022-03-22 18:41:49,340] {scheduler_job.py:153} INFO - Started process (PID=3142) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:49,345] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:49,346] {logging_mixin.py:112} INFO - [2022-03-22 18:41:49,346] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:49,375] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:49,379] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:49,394] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:49,619] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.279 seconds
[2022-03-22 18:41:53,362] {scheduler_job.py:153} INFO - Started process (PID=3144) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:53,371] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:53,372] {logging_mixin.py:112} INFO - [2022-03-22 18:41:53,372] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:53,405] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:53,409] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:53,428] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:53,643] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.281 seconds
[2022-03-22 18:41:57,379] {scheduler_job.py:153} INFO - Started process (PID=3146) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:57,389] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:41:57,391] {logging_mixin.py:112} INFO - [2022-03-22 18:41:57,391] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:57,422] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:41:57,427] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:41:57,443] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:41:57,678] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.299 seconds
[2022-03-22 18:42:01,367] {scheduler_job.py:153} INFO - Started process (PID=3148) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:01,374] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:01,376] {logging_mixin.py:112} INFO - [2022-03-22 18:42:01,375] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:01,409] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:01,414] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:01,428] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:01,596] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.229 seconds
[2022-03-22 18:42:06,207] {scheduler_job.py:153} INFO - Started process (PID=3151) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:06,221] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:06,222] {logging_mixin.py:112} INFO - [2022-03-22 18:42:06,222] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:06,297] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:06,312] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:06,329] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:06,651] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.445 seconds
[2022-03-22 18:42:10,213] {scheduler_job.py:153} INFO - Started process (PID=3153) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:10,225] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:10,226] {logging_mixin.py:112} INFO - [2022-03-22 18:42:10,226] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:10,273] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:10,286] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:10,301] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:10,679] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.466 seconds
[2022-03-22 18:42:14,219] {scheduler_job.py:153} INFO - Started process (PID=3155) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:14,225] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:14,227] {logging_mixin.py:112} INFO - [2022-03-22 18:42:14,227] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:14,266] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:14,271] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:14,280] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:14,458] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.239 seconds
[2022-03-22 18:42:18,192] {scheduler_job.py:153} INFO - Started process (PID=3157) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:18,201] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:18,203] {logging_mixin.py:112} INFO - [2022-03-22 18:42:18,202] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:18,242] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:18,246] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:18,256] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:18,435] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.243 seconds
[2022-03-22 18:42:22,411] {scheduler_job.py:153} INFO - Started process (PID=3159) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:22,419] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:22,422] {logging_mixin.py:112} INFO - [2022-03-22 18:42:22,422] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:22,580] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:22,590] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:22,612] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:23,111] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.700 seconds
[2022-03-22 18:42:27,911] {scheduler_job.py:153} INFO - Started process (PID=3161) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:27,942] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:27,944] {logging_mixin.py:112} INFO - [2022-03-22 18:42:27,944] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:28,241] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:28,247] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:28,280] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:28,893] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.982 seconds
[2022-03-22 18:42:30,329] {scheduler_job.py:153} INFO - Started process (PID=3163) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:30,349] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:30,358] {logging_mixin.py:112} INFO - [2022-03-22 18:42:30,358] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:30,405] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:30,410] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:30,420] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:30,584] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.255 seconds
[2022-03-22 18:42:34,232] {scheduler_job.py:153} INFO - Started process (PID=3165) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:34,244] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:34,246] {logging_mixin.py:112} INFO - [2022-03-22 18:42:34,245] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:34,274] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:34,278] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:34,288] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:34,455] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.224 seconds
[2022-03-22 18:42:38,259] {scheduler_job.py:153} INFO - Started process (PID=3167) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:38,270] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:38,271] {logging_mixin.py:112} INFO - [2022-03-22 18:42:38,271] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:38,300] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:38,304] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:38,311] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:38,472] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.213 seconds
[2022-03-22 18:42:42,225] {scheduler_job.py:153} INFO - Started process (PID=3170) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:42,240] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:42,242] {logging_mixin.py:112} INFO - [2022-03-22 18:42:42,242] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:42,290] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:42,296] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:42,311] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:42,485] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.260 seconds
[2022-03-22 18:42:46,246] {scheduler_job.py:153} INFO - Started process (PID=3172) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:46,260] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:46,261] {logging_mixin.py:112} INFO - [2022-03-22 18:42:46,261] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:46,289] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:46,293] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:46,305] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:46,494] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.248 seconds
[2022-03-22 18:42:50,270] {scheduler_job.py:153} INFO - Started process (PID=3174) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:50,280] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:50,282] {logging_mixin.py:112} INFO - [2022-03-22 18:42:50,282] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:50,310] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:50,315] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:50,322] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:50,495] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.226 seconds
[2022-03-22 18:42:54,240] {scheduler_job.py:153} INFO - Started process (PID=3176) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:54,263] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:54,266] {logging_mixin.py:112} INFO - [2022-03-22 18:42:54,266] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:54,304] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:54,309] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:54,321] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:54,544] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.304 seconds
[2022-03-22 18:42:58,295] {scheduler_job.py:153} INFO - Started process (PID=3178) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:58,313] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:42:58,314] {logging_mixin.py:112} INFO - [2022-03-22 18:42:58,314] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:58,358] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:42:58,365] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:42:58,375] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:42:58,847] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.552 seconds
[2022-03-22 18:43:02,288] {scheduler_job.py:153} INFO - Started process (PID=3180) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:02,298] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:43:02,300] {logging_mixin.py:112} INFO - [2022-03-22 18:43:02,300] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:02,333] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:43:02,338] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:43:02,347] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:02,538] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.250 seconds
[2022-03-22 18:43:06,448] {scheduler_job.py:153} INFO - Started process (PID=3182) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:06,479] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:43:06,494] {logging_mixin.py:112} INFO - [2022-03-22 18:43:06,494] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:06,553] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:43:06,559] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:43:06,574] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:06,803] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.356 seconds
[2022-03-22 18:43:10,288] {scheduler_job.py:153} INFO - Started process (PID=3184) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:10,302] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:43:10,304] {logging_mixin.py:112} INFO - [2022-03-22 18:43:10,304] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:10,340] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:43:10,345] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:43:10,354] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:10,543] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.255 seconds
[2022-03-22 18:43:14,319] {scheduler_job.py:153} INFO - Started process (PID=3187) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:14,326] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:43:14,328] {logging_mixin.py:112} INFO - [2022-03-22 18:43:14,328] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:14,387] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:43:14,393] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:43:14,406] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:14,645] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.326 seconds
[2022-03-22 18:43:18,278] {scheduler_job.py:153} INFO - Started process (PID=3189) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:18,287] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:43:18,288] {logging_mixin.py:112} INFO - [2022-03-22 18:43:18,288] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:18,317] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:43:18,321] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:43:18,329] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:18,508] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.230 seconds
[2022-03-22 18:43:22,287] {scheduler_job.py:153} INFO - Started process (PID=3191) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:22,295] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:43:22,297] {logging_mixin.py:112} INFO - [2022-03-22 18:43:22,296] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:22,325] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:43:22,329] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:43:22,339] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:22,539] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.251 seconds
[2022-03-22 18:43:26,317] {scheduler_job.py:153} INFO - Started process (PID=3193) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:26,323] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:43:26,326] {logging_mixin.py:112} INFO - [2022-03-22 18:43:26,326] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:26,372] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:43:26,378] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:43:26,390] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:26,663] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.346 seconds
[2022-03-22 18:43:30,297] {scheduler_job.py:153} INFO - Started process (PID=3195) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:30,310] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:43:30,312] {logging_mixin.py:112} INFO - [2022-03-22 18:43:30,312] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:30,411] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:43:30,417] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:43:30,430] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:30,725] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.429 seconds
[2022-03-22 18:43:34,381] {scheduler_job.py:153} INFO - Started process (PID=3197) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:34,400] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:43:34,412] {logging_mixin.py:112} INFO - [2022-03-22 18:43:34,412] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:34,641] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:43:34,647] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:43:34,678] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:35,178] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.797 seconds
[2022-03-22 18:43:38,341] {scheduler_job.py:153} INFO - Started process (PID=3199) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:38,358] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:43:38,371] {logging_mixin.py:112} INFO - [2022-03-22 18:43:38,371] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:38,423] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:43:38,428] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:43:38,440] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:38,705] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:43:38,876] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:43:39,030] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.689 seconds
[2022-03-22 18:43:42,359] {scheduler_job.py:153} INFO - Started process (PID=3201) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:42,369] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:43:42,371] {logging_mixin.py:112} INFO - [2022-03-22 18:43:42,371] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:42,472] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:43:42,478] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:43:42,498] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:43:42,807] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:43:42,894] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:43:40.796367+00:00: manual__2022-03-22T18:43:40.796367+00:00, externally triggered: True>
[2022-03-22 18:43:42,986] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:43:43,009] {scheduler_job.py:1640} INFO - Creating / updating <TaskInstance: bigquery_data_load.load_data 2022-03-22 18:43:40.796367+00:00 [scheduled]> in ORM
[2022-03-22 18:43:43,204] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.845 seconds
[2022-03-22 18:44:01,767] {scheduler_job.py:153} INFO - Started process (PID=3210) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:01,774] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:01,776] {logging_mixin.py:112} INFO - [2022-03-22 18:44:01,775] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:01,823] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:01,829] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:01,844] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:02,147] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:02,253] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:43:40.796367+00:00: manual__2022-03-22T18:43:40.796367+00:00, externally triggered: True>
[2022-03-22 18:44:02,732] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:02,768] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.002 seconds
[2022-03-22 18:44:05,742] {scheduler_job.py:153} INFO - Started process (PID=3212) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:05,748] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:05,750] {logging_mixin.py:112} INFO - [2022-03-22 18:44:05,750] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:05,778] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:05,783] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:05,792] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:06,037] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:06,116] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:43:40.796367+00:00: manual__2022-03-22T18:43:40.796367+00:00, externally triggered: True>
[2022-03-22 18:44:06,156] {logging_mixin.py:112} INFO - [2022-03-22 18:44:06,156] {dagrun.py:309} INFO - Marking run <DagRun bigquery_data_load @ 2022-03-22 18:43:40.796367+00:00: manual__2022-03-22T18:43:40.796367+00:00, externally triggered: True> failed
[2022-03-22 18:44:06,245] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:06,265] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.523 seconds
[2022-03-22 18:44:09,734] {scheduler_job.py:153} INFO - Started process (PID=3214) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:09,739] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:09,741] {logging_mixin.py:112} INFO - [2022-03-22 18:44:09,740] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:09,767] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:09,771] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:09,786] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:10,025] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:10,104] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:10,130] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.397 seconds
[2022-03-22 18:44:13,769] {scheduler_job.py:153} INFO - Started process (PID=3216) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:13,774] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:13,776] {logging_mixin.py:112} INFO - [2022-03-22 18:44:13,776] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:13,819] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:13,825] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:13,836] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:14,100] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:14,183] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:14,211] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.443 seconds
[2022-03-22 18:44:17,754] {scheduler_job.py:153} INFO - Started process (PID=3218) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:17,761] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:17,762] {logging_mixin.py:112} INFO - [2022-03-22 18:44:17,762] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:17,801] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:17,806] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:17,817] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:18,050] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:18,119] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:18,137] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.383 seconds
[2022-03-22 18:44:21,759] {scheduler_job.py:153} INFO - Started process (PID=3221) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:21,766] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:21,778] {logging_mixin.py:112} INFO - [2022-03-22 18:44:21,777] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:21,846] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:21,854] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:21,869] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:22,223] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:22,356] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:22,408] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.649 seconds
[2022-03-22 18:44:25,777] {scheduler_job.py:153} INFO - Started process (PID=3223) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:25,782] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:25,783] {logging_mixin.py:112} INFO - [2022-03-22 18:44:25,783] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:25,808] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:25,813] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:25,822] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:26,011] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:26,086] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:26,104] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.328 seconds
[2022-03-22 18:44:29,775] {scheduler_job.py:153} INFO - Started process (PID=3225) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:29,782] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:29,783] {logging_mixin.py:112} INFO - [2022-03-22 18:44:29,783] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:29,820] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:29,825] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:29,838] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:30,123] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:30,216] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:30,237] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.463 seconds
[2022-03-22 18:44:33,764] {scheduler_job.py:153} INFO - Started process (PID=3227) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:33,771] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:33,772] {logging_mixin.py:112} INFO - [2022-03-22 18:44:33,772] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:33,809] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:33,814] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:33,824] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:34,082] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:34,161] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:34,182] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.419 seconds
[2022-03-22 18:44:37,805] {scheduler_job.py:153} INFO - Started process (PID=3229) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:37,814] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:37,817] {logging_mixin.py:112} INFO - [2022-03-22 18:44:37,817] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:37,851] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:37,855] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:37,868] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:38,156] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:38,257] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:38,278] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.473 seconds
[2022-03-22 18:44:41,780] {scheduler_job.py:153} INFO - Started process (PID=3231) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:41,786] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:41,787] {logging_mixin.py:112} INFO - [2022-03-22 18:44:41,787] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:41,819] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:41,824] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:41,835] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:42,237] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:42,322] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:42,352] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.572 seconds
[2022-03-22 18:44:45,783] {scheduler_job.py:153} INFO - Started process (PID=3233) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:45,800] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:45,814] {logging_mixin.py:112} INFO - [2022-03-22 18:44:45,814] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:45,861] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:45,869] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:45,894] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:46,198] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:46,405] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:46,437] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.654 seconds
[2022-03-22 18:44:49,812] {scheduler_job.py:153} INFO - Started process (PID=3235) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:49,820] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:49,821] {logging_mixin.py:112} INFO - [2022-03-22 18:44:49,821] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:49,859] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:49,864] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:49,875] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:50,135] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:50,239] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:50,260] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.448 seconds
[2022-03-22 18:44:53,799] {scheduler_job.py:153} INFO - Started process (PID=3237) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:53,806] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:53,808] {logging_mixin.py:112} INFO - [2022-03-22 18:44:53,808] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:53,841] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:53,846] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:53,858] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:54,127] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:54,212] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:54,237] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.438 seconds
[2022-03-22 18:44:57,793] {scheduler_job.py:153} INFO - Started process (PID=3240) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:57,799] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:44:57,800] {logging_mixin.py:112} INFO - [2022-03-22 18:44:57,800] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:57,826] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:44:57,830] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:44:57,839] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:44:58,147] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:44:58,262] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:44:58,294] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.501 seconds
[2022-03-22 18:45:02,577] {scheduler_job.py:153} INFO - Started process (PID=3242) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:02,590] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:45:02,591] {logging_mixin.py:112} INFO - [2022-03-22 18:45:02,591] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:02,678] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:02,683] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:45:02,704] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:03,047] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:45:03,280] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:45:03,323] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.746 seconds
[2022-03-22 18:45:06,544] {scheduler_job.py:153} INFO - Started process (PID=3244) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:06,551] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:45:06,553] {logging_mixin.py:112} INFO - [2022-03-22 18:45:06,553] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:06,587] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:06,592] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:45:06,610] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:07,287] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:07,521] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:45:07,714] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:45:07,762] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.219 seconds
[2022-03-22 18:45:10,603] {scheduler_job.py:153} INFO - Started process (PID=3246) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:10,613] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:45:10,615] {logging_mixin.py:112} INFO - [2022-03-22 18:45:10,614] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:10,709] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:10,718] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:45:10,736] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:11,084] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:45:11,181] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:45:11,206] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.603 seconds
[2022-03-22 18:45:14,638] {scheduler_job.py:153} INFO - Started process (PID=3248) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:14,644] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:45:14,646] {logging_mixin.py:112} INFO - [2022-03-22 18:45:14,645] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:14,694] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:14,701] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:45:14,713] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:15,128] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:45:15,391] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:45:14.348276+00:00: manual__2022-03-22T18:45:14.348276+00:00, externally triggered: True>
[2022-03-22 18:45:16,062] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:45:16,086] {scheduler_job.py:1640} INFO - Creating / updating <TaskInstance: bigquery_data_load.load_data 2022-03-22 18:45:14.348276+00:00 [scheduled]> in ORM
[2022-03-22 18:45:16,291] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.653 seconds
[2022-03-22 18:45:35,848] {scheduler_job.py:153} INFO - Started process (PID=3257) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:35,860] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:45:35,862] {logging_mixin.py:112} INFO - [2022-03-22 18:45:35,862] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:35,902] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:35,907] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:45:35,922] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:36,294] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:45:36,444] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:45:14.348276+00:00: manual__2022-03-22T18:45:14.348276+00:00, externally triggered: True>
[2022-03-22 18:45:36,785] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:45:36,818] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.970 seconds
[2022-03-22 18:45:39,822] {scheduler_job.py:153} INFO - Started process (PID=3259) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:39,828] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:45:39,838] {logging_mixin.py:112} INFO - [2022-03-22 18:45:39,838] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:39,877] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:39,881] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:45:39,891] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:40,381] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:45:40,496] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:45:14.348276+00:00: manual__2022-03-22T18:45:14.348276+00:00, externally triggered: True>
[2022-03-22 18:45:40,552] {logging_mixin.py:112} INFO - [2022-03-22 18:45:40,551] {dagrun.py:309} INFO - Marking run <DagRun bigquery_data_load @ 2022-03-22 18:45:14.348276+00:00: manual__2022-03-22T18:45:14.348276+00:00, externally triggered: True> failed
[2022-03-22 18:45:40,696] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:45:40,718] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.896 seconds
[2022-03-22 18:45:43,831] {scheduler_job.py:153} INFO - Started process (PID=3261) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:43,838] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:45:43,840] {logging_mixin.py:112} INFO - [2022-03-22 18:45:43,839] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:43,873] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:43,878] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:45:43,889] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:44,111] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:45:44,203] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:45:44,225] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.395 seconds
[2022-03-22 18:45:47,853] {scheduler_job.py:153} INFO - Started process (PID=3263) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:47,861] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:45:47,864] {logging_mixin.py:112} INFO - [2022-03-22 18:45:47,863] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:47,908] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:47,914] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:45:47,929] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:48,273] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:45:48,427] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:45:48,462] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.609 seconds
[2022-03-22 18:45:51,841] {scheduler_job.py:153} INFO - Started process (PID=3265) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:51,849] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:45:51,855] {logging_mixin.py:112} INFO - [2022-03-22 18:45:51,855] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:51,890] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:51,894] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:45:51,905] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:52,178] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:45:52,264] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:45:52,286] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.445 seconds
[2022-03-22 18:45:55,835] {scheduler_job.py:153} INFO - Started process (PID=3267) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:55,840] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:45:55,842] {logging_mixin.py:112} INFO - [2022-03-22 18:45:55,842] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:55,869] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:55,874] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:45:55,883] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:56,077] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:45:56,146] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:45:56,167] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.332 seconds
[2022-03-22 18:45:59,869] {scheduler_job.py:153} INFO - Started process (PID=3269) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:59,874] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:45:59,876] {logging_mixin.py:112} INFO - [2022-03-22 18:45:59,875] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:45:59,906] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:45:59,910] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:45:59,919] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:00,167] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:00,270] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:00,292] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.423 seconds
[2022-03-22 18:46:03,855] {scheduler_job.py:153} INFO - Started process (PID=3272) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:03,861] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:03,862] {logging_mixin.py:112} INFO - [2022-03-22 18:46:03,862] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:03,896] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:03,902] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:03,913] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:04,151] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:04,250] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:04,273] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.418 seconds
[2022-03-22 18:46:07,855] {scheduler_job.py:153} INFO - Started process (PID=3274) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:07,862] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:07,864] {logging_mixin.py:112} INFO - [2022-03-22 18:46:07,864] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:07,911] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:07,917] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:07,931] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:08,353] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:09,494] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:09,585] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.730 seconds
[2022-03-22 18:46:11,893] {scheduler_job.py:153} INFO - Started process (PID=3276) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:11,904] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:11,908] {logging_mixin.py:112} INFO - [2022-03-22 18:46:11,907] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:11,972] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:11,978] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:11,991] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:12,261] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:12,374] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:12,397] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.504 seconds
[2022-03-22 18:46:15,895] {scheduler_job.py:153} INFO - Started process (PID=3278) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:15,911] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:15,914] {logging_mixin.py:112} INFO - [2022-03-22 18:46:15,913] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:15,977] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:15,983] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:16,006] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:16,291] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:16,378] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:16,400] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.506 seconds
[2022-03-22 18:46:19,893] {scheduler_job.py:153} INFO - Started process (PID=3280) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:19,905] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:19,924] {logging_mixin.py:112} INFO - [2022-03-22 18:46:19,923] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:20,007] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:20,011] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:20,023] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:20,492] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:20,674] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:20,730] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.837 seconds
[2022-03-22 18:46:23,905] {scheduler_job.py:153} INFO - Started process (PID=3282) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:23,911] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:23,912] {logging_mixin.py:112} INFO - [2022-03-22 18:46:23,912] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:23,942] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:23,947] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:23,958] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:24,196] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:24,283] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:24,313] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.408 seconds
[2022-03-22 18:46:27,915] {scheduler_job.py:153} INFO - Started process (PID=3284) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:27,932] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:27,935] {logging_mixin.py:112} INFO - [2022-03-22 18:46:27,935] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:27,996] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:28,001] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:28,043] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:28,411] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:28,520] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:28,548] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.634 seconds
[2022-03-22 18:46:31,875] {scheduler_job.py:153} INFO - Started process (PID=3286) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:31,889] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:31,890] {logging_mixin.py:112} INFO - [2022-03-22 18:46:31,890] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:31,928] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:31,933] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:31,943] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:32,166] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:32,245] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:32,267] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.393 seconds
[2022-03-22 18:46:38,806] {scheduler_job.py:153} INFO - Started process (PID=3289) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:38,825] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:38,827] {logging_mixin.py:112} INFO - [2022-03-22 18:46:38,827] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:39,612] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:39,676] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:39,819] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:40,458] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:40,554] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:40,579] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.773 seconds
[2022-03-22 18:46:41,254] {scheduler_job.py:153} INFO - Started process (PID=3291) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:41,264] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:41,266] {logging_mixin.py:112} INFO - [2022-03-22 18:46:41,266] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:41,318] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:41,324] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:41,341] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:41,613] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:41,697] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:41,720] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.465 seconds
[2022-03-22 18:46:45,230] {scheduler_job.py:153} INFO - Started process (PID=3293) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:45,236] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:45,238] {logging_mixin.py:112} INFO - [2022-03-22 18:46:45,237] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:45,267] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:45,271] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:45,280] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:45,508] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:45,595] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:45,620] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.390 seconds
[2022-03-22 18:46:49,274] {scheduler_job.py:153} INFO - Started process (PID=3295) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:49,280] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:49,284] {logging_mixin.py:112} INFO - [2022-03-22 18:46:49,284] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:49,323] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:49,328] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:49,341] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:49,630] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:49,746] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:49,774] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.500 seconds
[2022-03-22 18:46:53,256] {scheduler_job.py:153} INFO - Started process (PID=3297) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:53,267] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:53,270] {logging_mixin.py:112} INFO - [2022-03-22 18:46:53,270] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:53,312] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:53,321] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:53,337] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:53,613] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:53,699] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:53,719] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.463 seconds
[2022-03-22 18:46:57,267] {scheduler_job.py:153} INFO - Started process (PID=3299) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:57,285] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:46:57,295] {logging_mixin.py:112} INFO - [2022-03-22 18:46:57,294] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:57,592] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:46:57,597] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:46:57,611] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:46:57,835] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:46:57,922] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:46:57,944] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.676 seconds
[2022-03-22 18:47:01,294] {scheduler_job.py:153} INFO - Started process (PID=3301) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:01,299] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:01,302] {logging_mixin.py:112} INFO - [2022-03-22 18:47:01,302] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:01,336] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:01,341] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:01,351] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:01,557] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:01,638] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:01,662] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.368 seconds
[2022-03-22 18:47:05,291] {scheduler_job.py:153} INFO - Started process (PID=3303) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:05,304] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:05,306] {logging_mixin.py:112} INFO - [2022-03-22 18:47:05,306] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:05,347] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:05,353] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:05,367] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:05,625] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:05,730] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:05,756] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.465 seconds
[2022-03-22 18:47:09,534] {scheduler_job.py:153} INFO - Started process (PID=3306) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:09,544] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:09,547] {logging_mixin.py:112} INFO - [2022-03-22 18:47:09,547] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:09,609] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:09,616] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:09,636] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:10,014] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:10,153] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:10,192] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.659 seconds
[2022-03-22 18:47:13,308] {scheduler_job.py:153} INFO - Started process (PID=3308) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:13,314] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:13,317] {logging_mixin.py:112} INFO - [2022-03-22 18:47:13,316] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:13,355] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:13,361] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:13,375] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:13,623] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:13,705] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:13,732] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.424 seconds
[2022-03-22 18:47:17,474] {scheduler_job.py:153} INFO - Started process (PID=3310) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:17,492] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:17,494] {logging_mixin.py:112} INFO - [2022-03-22 18:47:17,494] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:17,557] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:17,565] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:17,598] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:18,456] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:18,808] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:18,873] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.399 seconds
[2022-03-22 18:47:21,281] {scheduler_job.py:153} INFO - Started process (PID=3312) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:21,289] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:21,290] {logging_mixin.py:112} INFO - [2022-03-22 18:47:21,290] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:21,334] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:21,340] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:21,354] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:22,068] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:22,231] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:22,263] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.983 seconds
[2022-03-22 18:47:25,317] {scheduler_job.py:153} INFO - Started process (PID=3314) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:25,323] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:25,324] {logging_mixin.py:112} INFO - [2022-03-22 18:47:25,324] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:25,372] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:25,378] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:25,393] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:25,617] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:25,694] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:25,715] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.398 seconds
[2022-03-22 18:47:29,331] {scheduler_job.py:153} INFO - Started process (PID=3316) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:29,338] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:29,340] {logging_mixin.py:112} INFO - [2022-03-22 18:47:29,339] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:29,379] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:29,386] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:29,395] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:29,644] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:29,733] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:29,755] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.425 seconds
[2022-03-22 18:47:33,308] {scheduler_job.py:153} INFO - Started process (PID=3318) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:33,313] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:33,314] {logging_mixin.py:112} INFO - [2022-03-22 18:47:33,314] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:33,361] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:33,367] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:33,378] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:33,592] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:33,680] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:33,703] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.395 seconds
[2022-03-22 18:47:37,361] {scheduler_job.py:153} INFO - Started process (PID=3320) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:37,369] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:37,371] {logging_mixin.py:112} INFO - [2022-03-22 18:47:37,371] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:37,418] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:37,423] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:37,434] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:37,691] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:37,772] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:37,794] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.433 seconds
[2022-03-22 18:47:41,311] {scheduler_job.py:153} INFO - Started process (PID=3322) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:41,322] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:41,323] {logging_mixin.py:112} INFO - [2022-03-22 18:47:41,323] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:41,357] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:41,363] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:41,375] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:41,604] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:41,679] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:41,702] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.391 seconds
[2022-03-22 18:47:45,318] {scheduler_job.py:153} INFO - Started process (PID=3325) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:45,324] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:45,325] {logging_mixin.py:112} INFO - [2022-03-22 18:47:45,325] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:45,354] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:45,359] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:45,368] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:45,574] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:45,652] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:45,673] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.355 seconds
[2022-03-22 18:47:49,356] {scheduler_job.py:153} INFO - Started process (PID=3327) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:49,363] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:49,369] {logging_mixin.py:112} INFO - [2022-03-22 18:47:49,368] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:49,428] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:49,438] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:49,454] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:49,770] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:49,907] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:49,934] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.577 seconds
[2022-03-22 18:47:53,389] {scheduler_job.py:153} INFO - Started process (PID=3329) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:53,398] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:53,400] {logging_mixin.py:112} INFO - [2022-03-22 18:47:53,400] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:53,435] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:53,442] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:53,456] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:53,752] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:53,838] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:53,863] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.474 seconds
[2022-03-22 18:47:57,325] {scheduler_job.py:153} INFO - Started process (PID=3331) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:57,331] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:47:57,333] {logging_mixin.py:112} INFO - [2022-03-22 18:47:57,333] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:57,364] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:47:57,369] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:47:57,377] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:47:57,628] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:47:57,780] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:47:57,815] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.490 seconds
[2022-03-22 18:48:01,363] {scheduler_job.py:153} INFO - Started process (PID=3333) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:01,369] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:48:01,370] {logging_mixin.py:112} INFO - [2022-03-22 18:48:01,370] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:01,404] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:48:01,409] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:48:01,419] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:01,661] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:48:01,758] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:48:01,778] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.415 seconds
[2022-03-22 18:48:05,358] {scheduler_job.py:153} INFO - Started process (PID=3335) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:05,364] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:48:05,366] {logging_mixin.py:112} INFO - [2022-03-22 18:48:05,366] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:05,393] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:48:05,399] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:48:05,407] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:05,713] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:48:05,797] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:48:05,820] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.462 seconds
[2022-03-22 18:48:09,361] {scheduler_job.py:153} INFO - Started process (PID=3337) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:09,372] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:48:09,374] {logging_mixin.py:112} INFO - [2022-03-22 18:48:09,374] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:09,431] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:48:09,442] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:48:09,461] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:09,782] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:48:09,924] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:48:09,953] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.593 seconds
[2022-03-22 18:48:13,367] {scheduler_job.py:153} INFO - Started process (PID=3339) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:13,373] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:48:13,374] {logging_mixin.py:112} INFO - [2022-03-22 18:48:13,374] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:13,413] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:48:13,420] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:48:13,430] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:13,668] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:48:13,749] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:48:13,771] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.404 seconds
[2022-03-22 18:48:17,439] {scheduler_job.py:153} INFO - Started process (PID=3342) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:17,446] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:48:17,448] {logging_mixin.py:112} INFO - [2022-03-22 18:48:17,447] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:17,596] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:48:17,604] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:48:17,625] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:18,006] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:48:18,115] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:48:18,144] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.705 seconds
[2022-03-22 18:48:21,480] {scheduler_job.py:153} INFO - Started process (PID=3344) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:21,513] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:48:21,532] {logging_mixin.py:112} INFO - [2022-03-22 18:48:21,531] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:21,758] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:48:21,769] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:48:21,790] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:22,561] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:48:22,876] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:48:22,917] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.437 seconds
[2022-03-22 18:48:25,422] {scheduler_job.py:153} INFO - Started process (PID=3346) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:25,436] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:48:25,439] {logging_mixin.py:112} INFO - [2022-03-22 18:48:25,439] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:25,509] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:48:25,517] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:48:25,531] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:25,931] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:48:26,192] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:48:26,220] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.798 seconds
[2022-03-22 18:48:29,520] {scheduler_job.py:153} INFO - Started process (PID=3348) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:29,528] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:48:29,537] {logging_mixin.py:112} INFO - [2022-03-22 18:48:29,537] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:29,610] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:48:29,615] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:48:29,628] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:30,010] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:48:30,131] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:48:30,179] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.659 seconds
[2022-03-22 18:48:33,372] {scheduler_job.py:153} INFO - Started process (PID=3350) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:33,379] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:48:33,381] {logging_mixin.py:112} INFO - [2022-03-22 18:48:33,381] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:33,453] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:48:33,461] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:48:33,473] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:33,740] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:48:33,831] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:48:33,855] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.482 seconds
[2022-03-22 18:48:37,516] {scheduler_job.py:153} INFO - Started process (PID=3352) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:37,529] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:48:37,532] {logging_mixin.py:112} INFO - [2022-03-22 18:48:37,532] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:37,597] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:48:37,607] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:48:37,620] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:38,091] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:48:38,336] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:48:36.845924+00:00: manual__2022-03-22T18:48:36.845924+00:00, externally triggered: True>
[2022-03-22 18:48:38,598] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:48:38,649] {scheduler_job.py:1640} INFO - Creating / updating <TaskInstance: bigquery_data_load.load_data 2022-03-22 18:48:36.845924+00:00 [scheduled]> in ORM
[2022-03-22 18:48:38,794] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.278 seconds
[2022-03-22 18:48:56,816] {scheduler_job.py:153} INFO - Started process (PID=3361) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:56,821] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:48:56,823] {logging_mixin.py:112} INFO - [2022-03-22 18:48:56,823] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:56,854] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:48:56,859] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:48:56,867] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:48:57,110] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:48:57,190] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:48:36.845924+00:00: manual__2022-03-22T18:48:36.845924+00:00, externally triggered: True>
[2022-03-22 18:48:57,407] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:48:57,430] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.614 seconds
[2022-03-22 18:49:00,803] {scheduler_job.py:153} INFO - Started process (PID=3363) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:00,811] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:00,812] {logging_mixin.py:112} INFO - [2022-03-22 18:49:00,812] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:00,843] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:00,848] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:00,856] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:01,132] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:01,218] {scheduler_job.py:759} INFO - Examining DAG run <DagRun bigquery_data_load @ 2022-03-22 18:48:36.845924+00:00: manual__2022-03-22T18:48:36.845924+00:00, externally triggered: True>
[2022-03-22 18:49:01,257] {logging_mixin.py:112} INFO - [2022-03-22 18:49:01,257] {dagrun.py:309} INFO - Marking run <DagRun bigquery_data_load @ 2022-03-22 18:48:36.845924+00:00: manual__2022-03-22T18:48:36.845924+00:00, externally triggered: True> failed
[2022-03-22 18:49:01,488] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:01,518] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.715 seconds
[2022-03-22 18:49:04,845] {scheduler_job.py:153} INFO - Started process (PID=3365) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:04,861] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:04,874] {logging_mixin.py:112} INFO - [2022-03-22 18:49:04,873] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:04,938] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:04,943] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:04,956] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:05,313] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:05,424] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:05,459] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.613 seconds
[2022-03-22 18:49:08,826] {scheduler_job.py:153} INFO - Started process (PID=3367) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:08,832] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:08,833] {logging_mixin.py:112} INFO - [2022-03-22 18:49:08,833] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:08,877] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:08,888] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:08,898] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:09,113] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:09,247] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:09,294] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.467 seconds
[2022-03-22 18:49:12,831] {scheduler_job.py:153} INFO - Started process (PID=3388) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:12,843] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:12,844] {logging_mixin.py:112} INFO - [2022-03-22 18:49:12,844] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:12,875] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:12,879] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:12,891] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:13,121] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:13,202] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:13,224] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.393 seconds
[2022-03-22 18:49:16,816] {scheduler_job.py:153} INFO - Started process (PID=3390) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:16,823] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:16,824] {logging_mixin.py:112} INFO - [2022-03-22 18:49:16,824] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:16,858] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:16,862] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:16,870] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:17,062] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:17,160] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:17,184] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.368 seconds
[2022-03-22 18:49:20,877] {scheduler_job.py:153} INFO - Started process (PID=3393) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:20,887] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:20,888] {logging_mixin.py:112} INFO - [2022-03-22 18:49:20,888] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:20,939] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:20,944] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:20,956] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:21,239] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:21,361] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:21,390] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.513 seconds
[2022-03-22 18:49:24,838] {scheduler_job.py:153} INFO - Started process (PID=3395) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:24,846] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:24,849] {logging_mixin.py:112} INFO - [2022-03-22 18:49:24,848] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:24,874] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:24,878] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:24,888] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:25,172] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:25,262] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:25,288] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.451 seconds
[2022-03-22 18:49:28,833] {scheduler_job.py:153} INFO - Started process (PID=3397) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:28,842] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:28,844] {logging_mixin.py:112} INFO - [2022-03-22 18:49:28,844] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:28,872] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:28,876] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:28,885] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:29,094] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:29,174] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:29,195] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.362 seconds
[2022-03-22 18:49:32,861] {scheduler_job.py:153} INFO - Started process (PID=3399) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:32,868] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:32,869] {logging_mixin.py:112} INFO - [2022-03-22 18:49:32,869] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:32,895] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:32,900] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:32,908] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:33,110] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:33,192] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:33,215] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.354 seconds
[2022-03-22 18:49:36,843] {scheduler_job.py:153} INFO - Started process (PID=3401) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:36,853] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:36,855] {logging_mixin.py:112} INFO - [2022-03-22 18:49:36,855] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:36,883] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:36,887] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:36,897] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:37,180] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:37,264] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:37,291] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.448 seconds
[2022-03-22 18:49:40,833] {scheduler_job.py:153} INFO - Started process (PID=3403) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:40,841] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:40,846] {logging_mixin.py:112} INFO - [2022-03-22 18:49:40,846] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:40,893] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:40,902] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:40,914] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:41,647] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:41,797] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:41,828] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.995 seconds
[2022-03-22 18:49:44,867] {scheduler_job.py:153} INFO - Started process (PID=3405) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:44,875] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:44,877] {logging_mixin.py:112} INFO - [2022-03-22 18:49:44,876] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:44,913] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:44,918] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:44,928] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:45,172] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:45,261] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:45,292] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.425 seconds
[2022-03-22 18:49:48,860] {scheduler_job.py:153} INFO - Started process (PID=3408) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:48,866] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:48,868] {logging_mixin.py:112} INFO - [2022-03-22 18:49:48,868] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:48,901] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:48,906] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:48,917] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:49,130] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:49,212] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:49,238] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.379 seconds
[2022-03-22 18:49:52,849] {scheduler_job.py:153} INFO - Started process (PID=3410) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:52,854] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:52,856] {logging_mixin.py:112} INFO - [2022-03-22 18:49:52,856] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:52,882] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:52,886] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:52,894] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:53,101] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:53,184] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:53,206] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.357 seconds
[2022-03-22 18:49:56,894] {scheduler_job.py:153} INFO - Started process (PID=3413) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:56,900] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:49:56,901] {logging_mixin.py:112} INFO - [2022-03-22 18:49:56,901] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:56,935] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:49:56,940] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:49:56,950] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:49:57,213] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:49:57,311] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:49:57,334] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.440 seconds
[2022-03-22 18:50:00,880] {scheduler_job.py:153} INFO - Started process (PID=3415) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:00,888] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:00,889] {logging_mixin.py:112} INFO - [2022-03-22 18:50:00,889] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:00,920] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:00,925] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:00,934] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:01,185] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:01,263] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:01,283] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.404 seconds
[2022-03-22 18:50:04,869] {scheduler_job.py:153} INFO - Started process (PID=3417) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:04,875] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:04,876] {logging_mixin.py:112} INFO - [2022-03-22 18:50:04,876] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:04,903] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:04,907] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:04,915] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:05,121] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:05,197] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:05,221] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.352 seconds
[2022-03-22 18:50:08,925] {scheduler_job.py:153} INFO - Started process (PID=3419) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:08,932] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:08,934] {logging_mixin.py:112} INFO - [2022-03-22 18:50:08,934] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:08,967] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:08,972] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:08,982] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:09,191] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:09,263] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:09,284] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.359 seconds
[2022-03-22 18:50:12,892] {scheduler_job.py:153} INFO - Started process (PID=3421) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:12,898] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:12,901] {logging_mixin.py:112} INFO - [2022-03-22 18:50:12,901] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:12,939] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:12,943] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:12,952] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:13,184] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:13,265] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:13,292] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.401 seconds
[2022-03-22 18:50:16,889] {scheduler_job.py:153} INFO - Started process (PID=3423) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:16,895] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:16,897] {logging_mixin.py:112} INFO - [2022-03-22 18:50:16,896] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:16,929] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:16,934] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:16,942] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:17,140] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:17,220] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:17,242] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.354 seconds
[2022-03-22 18:50:20,913] {scheduler_job.py:153} INFO - Started process (PID=3425) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:20,918] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:20,920] {logging_mixin.py:112} INFO - [2022-03-22 18:50:20,920] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:20,957] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:20,961] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:20,972] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:21,207] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:21,282] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:21,305] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.392 seconds
[2022-03-22 18:50:24,908] {scheduler_job.py:153} INFO - Started process (PID=3428) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:24,914] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:24,916] {logging_mixin.py:112} INFO - [2022-03-22 18:50:24,915] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:24,952] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:24,957] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:24,969] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:25,224] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:25,309] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:25,341] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.433 seconds
[2022-03-22 18:50:28,901] {scheduler_job.py:153} INFO - Started process (PID=3431) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:28,908] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:28,910] {logging_mixin.py:112} INFO - [2022-03-22 18:50:28,910] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:28,940] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:28,944] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:28,952] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:29,157] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:29,247] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:29,268] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.367 seconds
[2022-03-22 18:50:32,923] {scheduler_job.py:153} INFO - Started process (PID=3433) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:32,928] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:32,929] {logging_mixin.py:112} INFO - [2022-03-22 18:50:32,929] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:32,956] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:32,960] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:32,969] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:33,174] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:33,254] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:33,274] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.352 seconds
[2022-03-22 18:50:36,928] {scheduler_job.py:153} INFO - Started process (PID=3435) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:36,936] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:36,938] {logging_mixin.py:112} INFO - [2022-03-22 18:50:36,938] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:36,973] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:36,977] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:36,987] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:37,248] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:37,331] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:37,354] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.426 seconds
[2022-03-22 18:50:40,919] {scheduler_job.py:153} INFO - Started process (PID=3437) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:40,925] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:40,926] {logging_mixin.py:112} INFO - [2022-03-22 18:50:40,926] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:40,956] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:40,960] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:40,969] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:41,180] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:41,278] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:41,303] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.384 seconds
[2022-03-22 18:50:44,951] {scheduler_job.py:153} INFO - Started process (PID=3440) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:44,958] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:44,959] {logging_mixin.py:112} INFO - [2022-03-22 18:50:44,959] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:44,991] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:44,995] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:45,006] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:45,276] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:45,356] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:45,376] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.424 seconds
[2022-03-22 18:50:48,939] {scheduler_job.py:153} INFO - Started process (PID=3442) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:48,944] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:48,945] {logging_mixin.py:112} INFO - [2022-03-22 18:50:48,945] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:48,989] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:48,994] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:49,003] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:49,244] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:49,334] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:49,356] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.417 seconds
[2022-03-22 18:50:53,056] {scheduler_job.py:153} INFO - Started process (PID=3444) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:53,082] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:53,089] {logging_mixin.py:112} INFO - [2022-03-22 18:50:53,088] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:53,186] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:53,193] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:53,223] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:53,974] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:54,131] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:54,153] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.097 seconds
[2022-03-22 18:50:56,964] {scheduler_job.py:153} INFO - Started process (PID=3446) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:56,977] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:50:56,979] {logging_mixin.py:112} INFO - [2022-03-22 18:50:56,978] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:57,034] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:50:57,039] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:50:57,051] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:50:57,320] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:50:57,453] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:50:57,481] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 0.517 seconds
[2022-03-22 18:51:00,952] {scheduler_job.py:153} INFO - Started process (PID=3449) to work on /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:51:00,959] {scheduler_job.py:1562} INFO - Processing file /c/users/dan/airflow/dags/big_query_data_load.py for tasks to queue
[2022-03-22 18:51:00,963] {logging_mixin.py:112} INFO - [2022-03-22 18:51:00,963] {dagbag.py:396} INFO - Filling up the DagBag from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:51:01,059] {logging_mixin.py:112} INFO - /c/Users/Dan/airflow/secrets/keyfile.json
[2022-03-22 18:51:01,070] {logging_mixin.py:112} WARNING - /home/collid/.local/lib/python3.8/site-packages/airflow/contrib/operators/gcs_to_bq.py:183: PendingDeprecationWarning: Invalid arguments were passed to GoogleCloudStorageToBigQueryOperator (task_id: load_data). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'field_delimeter': ',', 'biqquery_conn_id': 'google_cloud_default'}
  super(GoogleCloudStorageToBigQueryOperator, self).__init__(*args, **kwargs)
[2022-03-22 18:51:01,105] {scheduler_job.py:1574} INFO - DAG(s) dict_keys(['bigquery_data_load']) retrieved from /c/users/dan/airflow/dags/big_query_data_load.py
[2022-03-22 18:51:02,111] {scheduler_job.py:1284} INFO - Processing bigquery_data_load
[2022-03-22 18:51:02,222] {scheduler_job.py:447} INFO - Skipping SLA check for <DAG: bigquery_data_load> because no tasks in DAG have SLAs
[2022-03-22 18:51:02,256] {scheduler_job.py:161} INFO - Processing /c/users/dan/airflow/dags/big_query_data_load.py took 1.304 seconds
